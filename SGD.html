<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>SGD in Nature: Close Analogues to AI Algorithms</title>
<style>
html,body{margin:0;padding:0;height:100%;width:100%;font-family:"Helvetica Neue",Arial,sans-serif;}
#poster{
  width:100%;
  max-width:12in;
  margin:0 auto;
  padding:0.35in;
  box-sizing:border-box;
  display:flex;
  flex-direction:column;
  gap:0.25in;
}
.columns {
  display:flex;
  gap:0.75in;
  justify-content:space-between;
  flex-wrap:wrap;
}
.column {
  display:flex;
  flex-direction:column;
  gap:0.3in;
  flex:1;
}
.left-content-column {
  display:flex;
  flex-direction:column;
  gap:0.3in;
  flex: 1;
  min-width: 7in;
}
.content-image-row {
  display: flex;
  flex-direction: row;
  gap: 0.75in;
  align-items: flex-start;
}
.content-image-row > .text-content {
  flex: 1;
}
.center-img {
  width:2.4in;
  height:2.4in;
  border:1.8px dashed #888;
  display:flex;
  align-items:center;
  justify-content:center;
  font-size:9pt;
  color:#777;
}
h1{font-size:40pt;line-height:44pt;margin:0;text-align:center;}
.big{font-size:26pt;font-weight:700;line-height:30pt;}
.med{font-size:18pt;font-weight:600;line-height:22pt;margin:0;}
.small{font-size:11pt;line-height:13pt;margin:0;}
ul.small{margin:0;padding-left:1em;}
#conclusion { margin-top: 0.5in; clear:both; }
#refs {
  display: flex;
  justify-content: space-between;
  flex-wrap: wrap;
  font-size: 8.5pt;
  line-height: 10.5pt;
  gap: 1in;
}
.ref-col {
  flex: 1;
  min-width: 300px;
}
@media(max-width:900px){
  .columns{flex-direction:column;gap:0.5in;}
  .left-content-column {
    min-width: auto;
  }
  .content-image-row {
    flex-direction: column;
    align-items: center;
    gap: 0.4in;
  }
  .content-image-row > .text-content {
    width: 100%;
  }
  h1{font-size:28pt;line-height:32pt;}
  .big{font-size:20pt;line-height:24pt;}
  .med{font-size:15pt;line-height:19pt;}
  .small{font-size:10pt;line-height:12pt;}
}
</style>
</head>
<body>
  <div id="poster">
    <h1>SGD in Nature:<br/>Close Analogues to AI Algorithms</h1>
    <p class="med" style="text-align:center;">Evidence that biological &amp; physical processes already exploit SGD‑like dynamics</p>
    <p class="big">Thesis ▶ Stochastic Gradient Descent obeys the same stochastic‑drift laws that underlie <em>evolution</em>, <em>neural adaptation</em>, and <em>thermodynamic relaxation</em>.</p>
    <p class="small" style="text-align:center;">These natural algorithms showcase how noisy, local updates can reliably discover globally robust solutions in high‑dimensional landscapes.</p>

    <div class="columns">
      <div class="left-content-column">
        <div class="content-image-row">
          <div class="text-content">
            <p class="med">1 ◆ Evolutionary Search ≈ "Fitness‑SGD"</p>
            <ul class="small">
              <li><strong>Gradient Source –</strong> Relative fitness selects allele changes that ascend the fitness gradient [5,6].</li>
              <li><strong>Noise –</strong> Mutation & drift maintain exploration; large pop. ↔ small batch size trade‑off.</li>
              <li><strong>Flat Peaks –</strong> Broad adaptive plateaus dominate rugged genotypic spaces, mirroring flat minima that generalize in DNNs [7].</li>
              <li><strong>Formal link –</strong> Adaptive walk dynamics converge to SDE: dθ = +∇F·dt + √Σ·dWt (same form as SGD) [8].</li>
            </ul>
          </div>
          <div class="center-img">Evolution Image</div>
        </div>
        <div class="content-image-row">
          <div class="text-content">
            <p class="med">2 ◆ Neural Plasticity ≈ Local Gradient Descent</p>
            <ul class="small">
              <li><strong>Gradient Source –</strong> Hebbian + STDP rules derived as exact gradients of free‑energy under predictive coding [3].</li>
              <li><strong>Experimental Evidence –</strong> In‑vitro cortical cultures follow predicted gradient flow when exposed to structured stimuli [9].</li>
              <li><strong>Global Signal –</strong> Dopamine reward prediction error ≈ global scalar loss informing weight updates (RL‑style SGD) [10].</li>
              <li><strong>Credit Assignment –</strong> Approximated through dendritic segregation & feedback pathways – biological "backprop" surrogates [11].</li>
            </ul>
          </div>
          <div class="center-img">Synapse Image</div>
        </div>
        <div class="content-image-row">
          <div class="text-content">
            <p class="med">3 ◆ Protein Folding & Over‑Damped Langevin Dynamics</p>
            <ul class="small">
              <li><strong>Gradient Source –</strong> Conformational search obeys dX = −∇U·dt + √(2β⁻¹)·dWt – identical SDE to SGD with η↔β⁻¹ [12].</li>
              <li><strong>Landscape –</strong> Funnel‑shaped landscapes explain fast convergence despite astronomical state space – same empirical observation for over‑param nets [13].</li>
              <li><strong>Temperature Scheduling –</strong> Simulated annealing & cyclical learning‑rates both exploit temperature schedules to cross barriers.</li>
              <li><strong>Experiment –</strong> Single‑molecule folding trajectories exhibit barrier hopping rates matching SGD escape statistics [14].</li>
            </ul>
          </div>
          <div class="center-img">Protein Funnel Image</div>
        </div>
      </div>
      <div class="column">
        <div>
          <p class="med">◆ Nonequilibrium Thermodynamics of SGD</p>
          <ul class="small">
            <li><strong>Entropy Flow –</strong> SGD maintains a steady‑state entropy production: ⟨ΔS⟩ ≈ η·Var[g] – obeys fluctuation theorems [15].</li>
            <li><strong>Effective Temperature –</strong> Teff ∝ η·(Bfull/Bmini) links batch size to exploration radius.</li>
            <li><strong>Flatness –</strong> Flatter minima correspond to lower stationary entropy – predicting better generalization [1,2].</li>
          </ul>
        </div>
        <div>
          <p class="med">✖ Non‑Analogues (Evidence Against)</p>
          <ul class="small">
            <li><strong>Hamiltonian Mechanics –</strong> Conservative, time‑reversible ⇒ no descent.</li>
            <li><strong>Closed Quantum Evolution –</strong> Unitary, preserves entropy; requires decoherence or baths to approximate SGD.</li>
            <li><strong>Pure Random Walk –</strong> Lacks drift term ⇒ polynomially slower search vs. SGD's biased walk.</li>
          </ul>
        </div>
        <div>
          <p class="med">◆ Design Lessons Transferred to Machine Learning</p>
          <ul class="small">
            <li><strong>Temperature Control –</strong> Adapt η & batch size like annealing schedules.</li>
            <li><strong>Population Diversity –</strong> Ensembles & PBT mimic evolutionary breadth.</li>
            <li><strong>Local+Global Signals –</strong> Merge Hebbian locality with global error modulators (e.g. feedback alignment).</li>
            <li><strong>Barrier Hopping –</strong> Inject noise bursts / sharpness‑aware steps to exit narrow valleys.</li>
            <li><strong>Energy‑Based Regularization –</strong> Entropy‑SGD, SAM explicitly penalize sharp minima → echoes natural robustness.</li>
          </ul>
        </div>
      </div>
    </div>

    <section id="conclusion">
      <p class="big">Conclusion ▶</p>
      <p class="small">The core mathematical structure of SGD—a biased stochastic process with drift and diffusion—has natural analogues across biology, neuroscience, and statistical physics. These domains exploit noise to improve search, ensure robustness, and generalize across uncertain landscapes—mirroring why SGD works so well in machine learning. This convergence offers a framework for new algorithm design based on nature's time-tested principles.</p>
    </section>

    <section id="refs">
      <div class="ref-col">
        <p>[1] Cohen T. et al., Edge of Stability, NeurIPS 2021.</p>
        <p>[2] Chaudhari P. et al., Entropy‑SGD, ICLR 2017.</p>
        <p>[3] Friston K., Free‑Energy Principle, Nat. Rev. Neurosci. 2010.</p>
        <p>[4] Lillicrap T. et al., Backprop in the Brain, Nat. Rev. Neurosci. 2020.</p>
        <p>[5] Orr H.A., Adaptation & Fitness Landscapes, Nat. Rev. Genet. 2005.</p>
        <p>[6] Kryazhimskiy S., Global Epistasis Makes Adaptation Predictable, Science 2014.</p>
        <p>[7] Izmailov P. et al., SGD & Flat Minima, NeurIPS 2018.</p>
      </div>
      <div class="ref-col">
        <p>[8] Fisher R.A., Fundamental Theorem of Natural Selection, 1930.</p>
        <p>[9] Isomura T., Friston K., Neural Cultures & Free‑Energy Minimization, eLife 2022.</p>
        <p>[10] Schultz W., Dopamine & Reward Prediction Error, Annu. Rev. Neurosci. 2016.</p>
        <p>[11] Guerguiev J. et al., Dendritic Credit Assignment, PNAS 2017.</p>
        <p>[12] Zwanzig R., Diffusion in a Rough Potential, PNAS 1988.</p>
        <p>[13] Wolynes P., Protein Folding Funnels, Brookhaven Symp. 1995.</p>
        <p>[14] Neupane K. et al., Protein Folding Barriers, Nat. Phys. 2016.</p>
        <p>[15] Sohl‑Dickstein J. et al., Nonequilibrium Thermodynamics for DL, ICML 2015.</p>
      </div>
    </section>
  </div>
</body>
</html>
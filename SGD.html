<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>SGD in Nature: Close Analogues to AI Algorithms</title>
<style>
html,body{margin:0;padding:0;height:100%;width:100%;font-family:"Helvetica Neue",Arial,sans-serif;}
#poster{
  width:100%;
  max-width:12in;
  margin:0 auto;
  padding:0.35in;
  box-sizing:border-box;
  display:flex;
  flex-direction:column;
  gap:0.25in;
}
.columns {
  display:flex;
  gap:1in;
  justify-content:space-between;
  position:relative;
}
.column {
  width:48%;
  display:flex;
  flex-direction:column;
  gap:0.3in;
}
.center-img {
  position:absolute;
  left:50%;
  transform:translateX(-50%);
  width:2.4in;
  height:3.4in;
  border:1.8px dashed #888;
  display:flex;
  align-items:center;
  justify-content:center;
  font-size:9pt;
  color:#777;
}
#img1 { top:1.5in; }
#img2 { top:5.3in; }
#img3 { top:9in; }

h1{font-size:40pt;line-height:44pt;margin:0;text-align:center;}
.big{font-size:26pt;font-weight:700;line-height:30pt;}
.med{font-size:18pt;font-weight:600;line-height:22pt;margin:0;}
.small{font-size:11pt;line-height:13pt;margin:0;}
ul.small{margin:0;padding-left:1em;}
#conclusion { margin-top: 0.5in; }
#refs {
  display: flex;
  justify-content: space-between;
  flex-wrap: wrap;
  font-size: 8.5pt;
  line-height: 10.5pt;
  gap: 1in;
}
.ref-col {
  flex: 1;
  min-width: 300px;
}
@media(max-width:900px){
  .columns{flex-direction:column;gap:0.5in;}
  .column{width:100%;}
  .center-img{position:static;transform:none;width:1.6in;height:2.2in;margin:0 auto;}
  h1{font-size:28pt;line-height:32pt;}
  .big{font-size:20pt;line-height:24pt;}
  .med{font-size:15pt;line-height:19pt;}
  .small{font-size:10pt;line-height:12pt;}
}
</style>
</head>
<body>
  <div id="poster">
    <h1>SGD in Nature:<br/>Close Analogues to AI Algorithms</h1>
    <p class="med" style="text-align:center;">Evidence that biological &amp; physical processes already exploit SGD‑like dynamics</p>
    <p class="big">Thesis ▶ Stochastic Gradient Descent obeys the same stochastic‑drift laws that underlie <em>evolution</em>, <em>neural adaptation</em>, and <em>thermodynamic relaxation</em>.</p>
    <p class="small" style="text-align:center;">These natural algorithms showcase how noisy, local updates can reliably discover globally robust solutions in high‑dimensional landscapes.</p>

    <div class="columns">
      <div class="center-img" id="img1">Evolution Image</div>
      <div class="center-img" id="img2">Synapse Image</div>
      <div class="center-img" id="img3">Protein Funnel Image</div>

      <div class="column">
        <div>
          <p class="med">1 ◆ Evolutionary Search ≈ "Fitness‑SGD"</p>
          <ul class="small">
            <li><strong>Gradient Source –</strong> Relative fitness selects allele changes that ascend the fitness gradient [5,6].</li>
            <li><strong>Noise –</strong> Mutation &amp; drift maintain exploration; large pop. ↔ small batch size trade‑off.</li>
            <li><strong>Flat Peaks –</strong> Broad adaptive plateaus dominate rugged genotypic spaces, mirroring flat minima that generalize in DNNs [7].</li>
            <li><strong>Formal link –</strong> Adaptive walk dynamics converge to SDE: dθ = +∇F·dt + √Σ·dW<sub>t</sub> (same form as SGD) [8].</li>
          </ul>
        </div>
        <div>
          <p class="med">2 ◆ Neural Plasticity ≈ Local Gradient Descent</p>
          <ul class="small">
            <li>Hebbian + STDP rules derived as <em>exact</em> gradients of free‑energy under predictive coding [3].</li>
            <li>In‑vitro cortical cultures follow predicted gradient flow when exposed to structured stimuli [9].</li>
            <li>Dopamine reward prediction error ≈ global scalar loss informing weight updates (RL‑style SGD) [10].</li>
            <li>Credit assignment approximated through dendritic segregation &amp; feedback pathways – biological "backprop" surrogates [11].</li>
          </ul>
        </div>
        <div>
          <p class="med">3 ◆ Protein Folding &amp; Over‑Damped Langevin Dynamics</p>
          <ul class="small">
            <li>Conformational search obeys dX = −∇U·dt + √(2β<sup>−1</sup>)·dW<sub>t</sub> – identical SDE to SGD with η↔β<sup>−1</sup> [12].</li>
            <li>Funnel‑shaped landscapes explain fast convergence despite astronomical state space – same empirical observation for over‑param nets [13].</li>
            <li>Simulated annealing &amp; cyclical learning‑rates both exploit temperature schedules to cross barriers.</li>
            <li>Single‑molecule folding trajectories exhibit barrier hopping rates matching SGD escape statistics [14].</li>
          </ul>
        </div>
      </div>
      <div class="column">
        <div>
          <p class="med">4 ◆ Nonequilibrium Thermodynamics of SGD</p>
          <ul class="small">
            <li>SGD maintains steady‑state entropy production: ⟨ΔS⟩ ≈ η·Var[g] – obeys fluctuation theorems [15].</li>
            <li>Effective temperature T<sub>eff</sub> ∝ η·(B<sub>full</sub>/B<sub>mini</sub>) links batch size to exploration radius.</li>
            <li>Flatter minima correspond to lower stationary entropy – predicting better generalization [1,2].</li>
          </ul>
        </div>
        <div>
          <p class="med">5 ◆ Non‑Analogues (Evidence Against)</p>
          <ul class="small">
            <li><strong>Hamiltonian Mechanics</strong>: conservative, time‑reversible ⇒ no descent.</li>
            <li><strong>Closed Quantum Evolution</strong>: unitary, preserves entropy; requires decoherence or baths to approximate SGD.</li>
            <li><strong>Pure Random Walk</strong>: lacks drift ⇒ polynomially slower search vs. SGD’s biased walk.</li>
          </ul>
        </div>
        <div>
          <p class="med">6 ◆ Design Lessons Transferred to ML</p>
          <ul class="small">
            <li><strong>Temperature Control:</strong> adapt η &amp; batch size like annealing schedules.</li>
            <li><strong>Population Diversity:</strong> ensembles &amp; PBT mimic evolutionary breadth.</li>
            <li><strong>Local+Global Signals:</strong> combine Hebbian locality with global modulators (e.g. feedback alignment).</li>
            <li><strong>Barrier Hopping:</strong> inject noise bursts / sharpness‑aware steps to exit valleys.</li>
            <li><strong>Energy Regularization:</strong> entropy‑SGD, SAM penalize sharp minima → natural robustness.</li>
          </ul>
        </div>
      </div>
    </div>

    <section id="conclusion">
      <p class="big">Conclusion ▶</p>
      <p class="small">The core mathematical structure of SGD—a biased stochastic process with drift and diffusion—has natural analogues across biology, neuroscience, and statistical physics. These domains exploit noise to improve search, ensure robustness, and generalize across uncertain landscapes—mirroring why SGD works so well in machine learning. This convergence offers a framework for new algorithm design based on nature’s time-tested principles.</p>
    </section>

    <section id="refs">
      <div class="ref-col">
        <p>[1] Cohen T. et al., <em>Edge of Stability</em>, NeurIPS 2021.</p>
        <p>[2] Chaudhari P. et al., <em>Entropy‑SGD</em>, ICLR 2017.</p>
        <p>[3] Friston K., <em>Free‑Energy Principle</em>, Nat. Rev. Neurosci. 2010.</p>
        <p>[4] Lillicrap T. et al., <em>Backprop in the Brain</em>, Nat. Rev. Neurosci. 2020.</p>
        <p>[5] Orr H.A., <em>Adaptation &amp; Fitness Landscapes</em>, Nat. Rev. Genet. 2005.</p>
        <p>[6] Kryazhimskiy S., <em>Global Epistasis</em>, Science 2014.</p>
        <p>[7] Izmailov P. et al., <em>SGD &amp; Flat Minima</em>, NeurIPS 2018.</p>
      </div>
      <div class="ref-col">
        <p>[8] Fisher R.A., <em>Fundamental Theorem</em>, 1930.</p>
        <p>[9] Isomura T., Friston K., <em>Free-Energy in Cultures</em>, eLife 2022.</p>
        <p>[10] Schultz W., <em>Reward Prediction</em>, Annu. Rev. Neurosci. 2016.</p>
        <p>[11] Guerguiev J. et al., <em>Dendritic Credit Assignment</em>, PNAS 2017.</p>
        <p>[12] Zwanzig R., <em>Diffusion in Rough Potentials</em>, PNAS 1988.</p>
        <p>[13] Wolynes P., <em>Protein Folding Funnels</em>, Brookhaven 1995.</p>
        <p>[14] Neupane K. et al., <em>Folding Barriers</em>, Nat. Phys. 2016.</p>
        <p>[15] Sohl‑Dickstein J. et al., <em>Thermodynamics of Learning</em>, ICML 2015.</p>
      </div>
    </section>
  </div>
</body>
</html>

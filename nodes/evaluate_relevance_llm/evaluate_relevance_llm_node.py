# No requests import needed here directly
import json
from typing import Dict
# Import the node containing the static method
from ..llm_query_api.llm_query_api_node import LLMQueryAPINode, DEFAULT_LLM_API_URL, DEFAULT_LLM_MODEL, DEFAULT_MAX_RETRIES, DEFAULT_TIMEOUT

# Default system prompt specific to this node's purpose
SYSTEM_PROMPT_FILTER = """You are an expert at analyzing ComfyUI nodes and determining their relevance to specific workflow tasks. Your job is to evaluate each node's description and determine how useful it would be for a given workflow goal. Please analyze the node based on: 1. Direct relevance to the workflow goal 2. Utility as a supporting node for the workflow 3. Specific features that would help achieve the goal. Rate the node's applicability from 0-100 where: 0-20: Not relevant, 21-40: Marginally relevant, 41-60: Moderately useful, 61-80: Very useful, 81-100: Essential for this workflow. Return only a JSON response in this format: {"score": <0-100>, "reason": "<brief 1-2 sentence explanation>"}"""


class EvaluateRelevanceLLMNode:
    """
    Takes a specific relevance evaluation prompt, queries the LLM API
    using the LLMQueryAPINode's static method, and returns the raw JSON
    response string containing score and reason.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_prompt": ("STRING", {"multiline": True}), # The prompt generated by GenerateFilterPromptNode
            },
             "optional": {
                 # Allow overriding the system prompt
                 "system_prompt": ("STRING", {"multiline": True, "default": SYSTEM_PROMPT_FILTER}),
                 "api_url": ("STRING", {"default": DEFAULT_LLM_API_URL}),
                 "model": ("STRING", {"default": DEFAULT_LLM_MODEL}),
                 "temperature": ("FLOAT", {"default": 0.3, "min": 0.0, "max": 2.0, "step": 0.1}),
                 "max_retries": ("INT", {"default": DEFAULT_MAX_RETRIES, "min": 0}),
                 "timeout": ("INT", {"default": DEFAULT_TIMEOUT, "min": 5}),
                 "trigger": ("*", {}),
             }
        }

    RETURN_TYPES = ("STRING",) # Output is the raw JSON string from the LLM
    RETURN_NAMES = ("llm_response_json",)
    FUNCTION = "evaluate_relevance"
    CATEGORY = "utils/llm"
    OUTPUT_NODE = False

    def evaluate_relevance(self, llm_prompt: str, system_prompt: str = SYSTEM_PROMPT_FILTER, api_url: str = DEFAULT_LLM_API_URL, model: str = DEFAULT_LLM_MODEL, temperature: float = 0.3, max_retries: int = DEFAULT_MAX_RETRIES, timeout: int = DEFAULT_TIMEOUT, trigger=None) -> tuple[str]:
        """Queries the LLM using the imported static method."""
        if not llm_prompt:
             print("Error: LLM prompt is empty.")
             error_response = json.dumps({"score": 0, "reason": "Input LLM prompt was empty."})
             return (error_response,)

        print(f"Sending prompt to LLM for relevance evaluation via LLMQueryAPINode...")
        # Call the static method from the imported node class
        llm_response_content = LLMQueryAPINode._static_query_llm_single(
            user_prompt=llm_prompt,
            system_prompt=system_prompt,
            api_url=api_url,
            model=model,
            temperature=temperature,
            max_retries=max_retries,
            timeout=timeout
        )

        # --- Post-processing (optional but good practice) ---
        # Try to parse the content to validate it's the expected JSON format
        try:
             parsed = json.loads(llm_response_content)
             # Optional: Add more validation, e.g., check for 'score' and 'reason' keys
             print(f"LLM Response received and parsed: {llm_response_content}")
             # Return the original content string as required by the node's RETURN_TYPES
             return (llm_response_content,)
        except json.JSONDecodeError:
             print(f"Warning: LLM response content is not valid JSON: {llm_response_content[:100]}...")
             # Return the raw content anyway, or an error JSON
             error_response = json.dumps({"error": "LLM response was not valid JSON", "raw_content": llm_response_content})
             return (error_response,) # Return error JSON
        except Exception as e:
             print(f"Error processing LLM response: {e}")
             error_response = json.dumps({"error": f"Failed to process LLM response: {e}", "raw_content": llm_response_content})
             return (error_response,) 
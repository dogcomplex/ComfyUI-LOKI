Here is the file tree and contents of all files in the project:
target_folder: .//
+-- scribe
    +-- __init__.py
    +-- scribe_node.py
    +-- scribe.md
    +-- STATUS.md
    +-- scribe_name.md
    +-- scribe.json
+-- build_workflow_tool
    +-- build_workflows.py
+-- evaluate_relevance_llm
    +-- __init__.py
    +-- evaluate_relevance_llm_node.py
+-- filter_nodes
    +-- __init__.py
    +-- filter_nodes_node.py
+-- glamour
    +-- glamour_utils.py
    +-- __init__.py
    +-- glamour_node.py
    +-- glamour.md
    +-- STATUS.md
    +-- glamour_name.md
    +-- glamour.json
    +-- glamour.py
    +-- glamour
        +-- card
+-- list_available_nodes
    +-- __init__.py
    +-- list_available_nodes_node.py
    +-- list_available_nodes.py
    +-- list_available_nodes.md
    +-- STATUS.md
    +-- list_available_nodes_name.md
    +-- list_available_nodes.json
+-- list_installed_nodes
    +-- __init__.py
    +-- list_installed_nodes_node.py
    +-- list_installed_nodes.json
    +-- list_installed_nodes.md
    +-- STATUS.md
    +-- list_installed_nodes_name.md
    +-- list_installed_nodes.py
+-- llm_query_api
    +-- __init__.py
    +-- llm_query_api_node.py
+-- llm_query_api_batch
    +-- __init__.py
    +-- llm_query_api_batch_node.py
+-- llm_utils
    +-- __init__.py
    +-- client.py
+-- twitter_scraper
    +-- context
        +-- how_to.txt
    +-- twitter_scraper.md
    +-- twitter_scraper_name.md
    +-- twitter_scraper.py
    +-- __init__.py
    +-- requirements.txt
    +-- twitter_scraper_node.py
    +-- tweet_output.json
+-- twitter_user_scraper
    +-- twitter_user_scraper.md
    +-- twitter_user_scraper_name.md
    +-- twitter_user_scraper.py
    +-- twitter_user_scraper_node.py
    +-- __init__.py
    +-- requirements.txt
    +-- STATUS.md
    +-- CONTEXT.md
+-- twitter_thread_scraper
    +-- twitter_thread_scraper.md
    +-- twitter_thread_scraper_name.md
    +-- twitter_thread_scraper.py
    +-- twitter_thread_scraper_node.py
    +-- __init__.py
    +-- requirements.txt
    +-- thread_output.json
./scribe\__init__.py
from .scribe_node import Scribe

# Using information from scribe_name.md and scribe.json
NODE_CLASS_MAPPINGS = {
    "✍️Scribe (LOKI)": Scribe
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "✍️Scribe (LOKI)": "✍️ Scribe Workflow Documenter (LOKI)"
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']

# Print a message to the console when the node is loaded
print("\n----------------------------------------")
print("🦊 LOKI: Loaded Scribe node")
print("    [✓] Class: Scribe")
print("    [✓] Display Name: ✍️ Scribe Workflow Documenter (LOKI)")
print("----------------------------------------\n") 

./scribe\scribe_node.py
import os
import json
import requests
from urllib.parse import quote
from pathlib import Path # Use pathlib
from typing import Dict, List, Any, Optional
from datetime import datetime # Need to import datetime
import server # Need server instance to access graph
import re
import folder_paths # Import folder_paths

# --- DEBUG: Print server module path ---
print(f"DEBUG: Imported 'server' module from: {getattr(server, '__file__', 'N/A')}")
# --- End DEBUG ---

class Scribe:
    """
    Scribe node that documents the current workflow state and node information
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "include_docs": ("BOOLEAN", {"default": True}),
                "output_format": (["json", "markdown"], {"default": "markdown"}),
            },
            "optional": {
                "trigger": ("*", {}), # Optional input to delay execution
                "docs_base_url": ("STRING", {
                    "default": "https://docs.getsalt.ai/md/Comfy/Nodes/",
                    "multiline": False
                }),
                "filename_prefix": ("STRING", {"default": "workflow_doc"}),
            },
            "hidden": { # Still need to declare we need these
                "prompt": {"type": "PROMPT"},
                "extra_pnginfo": {"type": "EXTRA_PNGINFO"},
                "unique_id": {"type": "UNIQUE_ID"} 
            }
        }

    RETURN_TYPES = ("STRING",)
    FUNCTION = "scribe"
    CATEGORY = "LOKI 🦊/Documentation" # Updated category
    OUTPUT_NODE = True # Output is saved to file and returned as string

    def __init__(self):
        # Get the main ComfyUI output directory
        output_dir = Path(folder_paths.get_output_directory())
        # Define the LOKI-specific output path
        self.output_dir = output_dir / "LOKI" / "Scribe" / "output"
        try:
            # Ensure the LOKI Scribe output directory exists
            self.output_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            print(f"Warning: Could not create Scribe output directory at {self.output_dir}: {e}")
            pass # Or handle fallback

    def fetch_node_docs(self, node_type: str, base_url: str) -> Optional[str]:
        """Fetch documentation for a node type from the docs site"""
        if not base_url or not node_type:
            return None
        try:
            # Ensure base_url ends with a slash
            if not base_url.endswith('/'):
                base_url += '/'
            # URL encode the node type
            url = f"{base_url}{quote(node_type)}.md" # Assuming .md extension
            print(f"Fetching docs from: {url}")
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                # Maybe strip frontmatter if present? For now, return raw text.
                 return response.text
            else:
                print(f"Docs fetch failed for {node_type} (Status: {response.status_code})")
                return f"Failed to fetch documentation (Status: {response.status_code})"
        except requests.exceptions.RequestException as e:
            print(f"Error fetching docs for {node_type}: {e}")
            return f"Error fetching documentation: {e}"
        except Exception as e:
            print(f"Unexpected error fetching docs for {node_type}: {e}")
            return f"Unexpected error fetching documentation: {e}"


    def get_node_info(self, node_data: Dict[str, Any], all_nodes_data: Dict[str, Any]) -> Dict[str, Any]:
        """Gather comprehensive information about a node from its graph data"""
        node_id = str(node_data.get("id", "unknown"))
        node_type = node_data.get("type", "unknown")

        info = {
            "id": node_id,
            "type": node_type,
            "title": node_data.get("title", node_type), # Use title if available
            "properties": node_data.get("properties", {}),
            "widgets_values": node_data.get("widgets_values", []),
            "inputs": {},
            "outputs": {},
            "connections": {"inputs": {}, "outputs": {}},
            "position": node_data.get("pos", [0, 0]),
            "size": node_data.get("size", [0, 0]),
            # Placeholder for fetched docs
            "description": f"Node Type: {node_type}", # Basic description
            "docs": None
        }

        # --- Inputs ---
        if "inputs" in node_data:
            for input_info in node_data["inputs"]:
                input_name = input_info.get("name", "unknown")
                input_type = input_info.get("type", "*")
                link_id = input_info.get("link", None)

                info["inputs"][input_name] = {
                    "type": input_type,
                    "link": link_id,
                    "origin_node": None, # Will be filled later if linked
                    "origin_slot": None,
                }

                # Find connection source
                if link_id is not None:
                    for other_node_id, other_node_data in all_nodes_data.items():
                        if "outputs" in other_node_data:
                            for output_idx, output_info in enumerate(other_node_data["outputs"]):
                                if "links" in output_info and link_id in output_info["links"]:
                                    info["inputs"][input_name]["origin_node"] = str(other_node_id)
                                    info["inputs"][input_name]["origin_slot"] = output_idx
                                    # Add to connections structure
                                    if node_id not in info["connections"]["inputs"]:
                                        info["connections"]["inputs"][node_id] = []
                                    info["connections"]["inputs"][node_id].append({
                                        "input_name": input_name,
                                        "origin_node_id": str(other_node_id),
                                        "origin_slot_index": output_idx,
                                        # Optionally add origin node title here if needed later
                                    })
                                    break # Found the source link
                            if info["inputs"][input_name]["origin_node"]:
                                break # Found the source node

        # --- Outputs ---
        if "outputs" in node_data:
            for output_idx, output_info in enumerate(node_data["outputs"]):
                output_name = output_info.get("name", f"output_{output_idx}")
                output_type = output_info.get("type", "*")
                links = output_info.get("links", [])
                slot_index = output_info.get("slot_index", output_idx) # Use provided index if available

                info["outputs"][output_name] = {
                    "type": output_type,
                    "links": links,
                    "slot_index": slot_index,
                    "targets": [] # Will be filled later
                }

                # Find connection targets
                if links:
                     if node_id not in info["connections"]["outputs"]:
                          info["connections"]["outputs"][node_id] = []

                     for link_id in links:
                          for target_node_id, target_node_data in all_nodes_data.items():
                               if "inputs" in target_node_data:
                                    for input_idx, input_info in enumerate(target_node_data["inputs"]):
                                         if input_info.get("link") == link_id:
                                             target_input_name = input_info.get("name", f"input_{input_idx}")
                                             info["outputs"][output_name]["targets"].append({
                                                 "target_node_id": str(target_node_id),
                                                 "target_input_name": target_input_name,
                                             })
                                             # Add to connections structure
                                             info["connections"]["outputs"][node_id].append({
                                                 "output_name": output_name,
                                                 "output_slot_index": slot_index,
                                                 "target_node_id": str(target_node_id),
                                                 "target_input_name": target_input_name,
                                                  # Optionally add target node title here if needed later
                                             })
                                             # Note: A single output can link to multiple inputs
                                             # Don't break here, find all targets for the link_id

        # --- Widget Values ---
        # Try to map widget values back to inputs if possible (requires node definition knowledge)
        # This is complex without loading the actual node class definition.
        # For now, just list them.
        # TODO: Enhance this by potentially looking up NODE_CLASS_MAPPINGS if feasible

        return info

    def format_output(self, workflow_info: Dict[str, Any], output_format: str) -> str:
        """Format the workflow information in the specified format"""
        if output_format == "json":
            # Use default=str to handle potential non-serializable types like datetime
            return json.dumps(workflow_info, indent=2, default=str)

        # Markdown format
        md_output = ["# Workflow Documentation\n"]
        md_output.append(f"Generated: {workflow_info['metadata']['timestamp']}\n")
        md_output.append(f"Includes External Docs: {workflow_info['metadata']['include_docs']}\n")
        md_output.append("---\n")

        # Sort nodes by ID for consistent output
        sorted_node_ids = sorted(workflow_info["nodes"].keys(), key=lambda x: int(x))

        for node_id in sorted_node_ids:
            node_info = workflow_info["nodes"][node_id]
            node_title = node_info.get('title', node_info['type'])
            md_output.append(f"## Node {node_id}: {node_title} (`{node_info['type']}`)\n")

            md_output.append(f"- Position: `{node_info.get('position')}`")
            md_output.append(f"- Size: `{node_info.get('size')}`\n")

            if node_info.get("properties"):
                md_output.append("### Properties\n")
                for key, value in node_info["properties"].items():
                     md_output.append(f"- `{key}`: `{value}`")
                md_output.append("\n")

            if node_info.get("widgets_values"):
                md_output.append("### Widget Values\n")
                # Try to provide more context if widget names exist (not guaranteed in basic graph data)
                widget_values = node_info["widgets_values"]
                if isinstance(widget_values, list):
                    for idx, value in enumerate(widget_values):
                        md_output.append(f"- Widget {idx}: `{value}`")
                elif isinstance(widget_values, dict): # Some nodes might store widgets differently
                    for name, value in widget_values.items():
                        md_output.append(f"- {name}: `{value}`")
                md_output.append("\n")


            md_output.append("### Inputs\n")
            if node_info["inputs"]:
                for input_name, input_data in node_info["inputs"].items():
                    connection_info = ""
                    if input_data["link"] is not None and input_data["origin_node"]:
                        # Find the title of the origin node
                        origin_node_title = workflow_info["nodes"].get(input_data["origin_node"], {}).get('title', f"Node {input_data['origin_node']}")
                        origin_node_type = workflow_info["nodes"].get(input_data["origin_node"], {}).get('type', 'unknown')
                         # Find the name of the origin output slot
                        origin_output_name = f"output_{input_data['origin_slot']}" # Default name
                        origin_node_outputs = workflow_info["nodes"].get(input_data["origin_node"], {}).get("outputs", {})
                        for name, out_data in origin_node_outputs.items():
                            if out_data.get("slot_index") == input_data['origin_slot']:
                                origin_output_name = name
                                break

                        connection_info = f" <- `{origin_node_title}`.`{origin_output_name}` (Node {input_data['origin_node']}, Slot {input_data['origin_slot']})"
                    md_output.append(f"- **{input_name}** (`{input_data['type']}`){connection_info}")
            else:
                 md_output.append("*No inputs defined.*\n")
            md_output.append("\n")


            md_output.append("### Outputs\n")
            if node_info["outputs"]:
                 for output_name, output_data in node_info["outputs"].items():
                     md_output.append(f"- **{output_name}** (`{output_data['type']}`, Slot {output_data['slot_index']})")
                     if output_data["targets"]:
                         for target in output_data["targets"]:
                             target_node_title = workflow_info["nodes"].get(target['target_node_id'], {}).get('title', f"Node {target['target_node_id']}")
                             md_output.append(f"  - -> `{target_node_title}`.`{target['target_input_name']}` (Node {target['target_node_id']})")
            else:
                md_output.append("*No outputs defined.*\n")
            md_output.append("\n")

            # Include fetched documentation
            if node_info.get("docs"):
                md_output.append("### Documentation\n")
                md_output.append("```markdown") # Use code block for potentially raw markdown docs
                md_output.append(node_info["docs"])
                md_output.append("```\n")

            md_output.append("\n---\n")

        return "\n".join(md_output)

    # Signature accepts all defined inputs (req, opt, hidden) as keyword args
    def scribe(self, 
               # Required
               include_docs: bool = True, 
               output_format: str = "markdown", 
               # Optional
               trigger=None, 
               docs_base_url: str = None, 
               filename_prefix: str = "workflow_doc",
               # Hidden (expected to be passed by name)
               prompt=None, 
               extra_pnginfo=None, 
               unique_id=None
               ) -> tuple[str]:
        """Main function to document the workflow, using prompt data passed via hidden input keyword argument."""

        # --- DEBUG: Log the received prompt argument --- 
        print(f"DEBUG: scribe received prompt argument - Type: {type(prompt)}, Value: {prompt}")
        # --- End DEBUG ---

        # --- Validate Prompt Data --- 
        if not isinstance(prompt, dict) or 'nodes' not in prompt:
            error_msg = f"Error: Invalid or missing prompt data received via hidden input keyword argument. Type: {type(prompt)}"
            print(error_msg)
            return ("Error: Invalid prompt data received by node.",)
        
        # Use prompt_id from hidden unique_id if available (often execution ID?)
        current_prompt_id = str(unique_id) if unique_id is not None else 'unknown' 
        print(f"INFO: Scribe using prompt data passed via hidden input. Execution ID: {current_prompt_id}")
        prompt_data = prompt

        # --- Proceed with processing --- 
        raw_nodes_data = {str(n['id']): n for n in prompt_data['nodes']} # Use dict indexed by ID

        workflow_info = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "include_docs": include_docs,
                "output_format": output_format,
                "docs_base_url": docs_base_url,
                "prompt_id": current_prompt_id # Use best available ID
            },
            "nodes": {}
        }


        # Process each node found in the prompt data
        for node_id, node_data in raw_nodes_data.items():
            node_info = self.get_node_info(node_data, raw_nodes_data) # Pass all nodes for connection lookup

            # Fetch documentation if requested
            if include_docs and docs_base_url:
                node_info["docs"] = self.fetch_node_docs(node_info["type"], docs_base_url)

            workflow_info["nodes"][node_id] = node_info # Store by ID


        # Format output
        output_string = self.format_output(workflow_info, output_format)

        # Save to file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_prefix = re.sub(r'[^\w\-_\. ]', '_', filename_prefix) # Sanitize prefix
        filename = f"{safe_prefix}_{timestamp}.{output_format}"
        filepath = self.output_dir / filename # Use the new output_dir path

        try:
            filepath.write_text(output_string, encoding="utf-8") # Use write_text
            print(f"Workflow documentation saved to: {filepath}")
        except Exception as e:
            print(f"Error saving workflow documentation: {e}")
            return (f"Error saving file: {e}",) # Return error message

        # Return the formatted string as output
        return (output_string,)

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        # Make it run every time its inputs change or it's triggered
        return float("nan") 

./scribe\scribe.md
# REQUIREMENTS: Scribe Node

## DESCRIPTION

A node that generates documentation for the current ComfyUI workflow, including node details, connections, and optionally fetching external documentation for individual node types.

## INPUTS

-   **`include_docs`** (BOOLEAN, default: `True`): If enabled, the node attempts to fetch external documentation for each node type from the specified `docs_base_url`.
-   **`output_format`** (STRING: "json" | "markdown", default: "markdown"): Specifies the format of the generated documentation file and the output string.
-   **`trigger`** (*, optional): Connect an output from a later node here to delay the Scribe node's execution until that node finishes. This helps ensure the workflow data is available when Scribe runs.
-   **`docs_base_url`** (STRING, optional, default: "https://docs.getsalt.ai/md/Comfy/Nodes/"): The base URL used for fetching external node documentation.
-   **`filename_prefix`** (STRING, optional, default: "workflow_doc"): Prefix for the output documentation file name.

*(Hidden Input)*: The node implicitly receives the current workflow definition (prompt data) via a hidden `PROMPT` input provided by the ComfyUI execution system.

## OUTPUTS

-   **`documentation`** (STRING): The generated workflow documentation as a single string, formatted according to the `output_format` input. The content includes workflow structure, node details (ID, type, title, properties, widgets, connections), and fetched external documentation if `include_docs` is enabled.

## NODE_STRATEGY

This node is implemented as a standalone ComfyUI Python node (`scribe_node.py`). It relies on the ComfyUI execution system identifying the need for workflow data via the hidden `PROMPT` input and passing this data as a keyword argument (`prompt=...`) to the node's execution function. To ensure this data is available when the function is called, connect the optional `trigger` input to a node that executes late in the workflow. It parses the node and connection data, optionally makes external HTTP requests to fetch documentation, formats the information, saves it to a file in the main ComfyUI output directory (`<output>/LOKI/Scribe/output/`), and returns the documentation as a string. It uses standard Python libraries (`requests`, `json`, `pathlib`). 

./scribe\STATUS.md
# STATUS: Scribe Node

## STATUS

-   Initial Implementation / Under Development

## PRIORITY

-   Medium (Core utility node for workflow understanding and sharing)

## CURRENT LIMITATIONS

-   Requires network connectivity to fetch external documentation.
-   External documentation fetching assumes a specific URL structure (`<base_url>/<NodeType>.md`) and retrieves raw Markdown content without parsing.
-   Error handling for network requests is basic.
-   Mapping of `widgets_values` back to named widgets is not implemented; values are listed by index or raw dictionary structure.
-   Documentation output is saved to a node-specific `./output/` subfolder, which might not be easily discoverable by users.
-   Does not fully adhere to LOKI `NODE_SPECIFICATIONS.md` yet (e.g., config files not fully utilized).

## TODO / FUTURE WORK

-   [x] Refactor class name from `ScribeNode` to `Scribe` in `scribe_node.py`.
-   [x] Update node `CATEGORY` to `LOKI 🦊/Documentation` in `scribe_node.py`.
-   [ ] Refactor `__init__.py` to load mappings from `scribe.json` / `scribe_name.md`.
-   [x] Implement `NODE_DISPLAY_NAME_MAPPINGS` using `scribe_name.md` conventions.
-   [x] Moved output file saving to main ComfyUI output dir (`<comfy_output>/LOKI/Scribe/output/`).
-   [x] Resolved workflow data access issue by using hidden `PROMPT` input.
-   [ ] Enhance `get_node_info` to map `widgets_values` to widget names if feasible.
-   [ ] Improve robustness of `fetch_node_docs` (handle 404s, timeouts, parse Markdown).
-   [ ] Add `scribe_test.py` for basic tests.
-   [ ] Add `scribe_workflow.json` example usage.
-   [ ] Develop Glamour layer (`glamour.json`, `icon.png`).
-   [ ] Utilize `scribe.json` config file more effectively (e.g., load defaults). 

./scribe\scribe_name.md
# NAME: Scribe

## FULL_NAME

Scribe Workflow Documenter

## TOKEN

✍️Scribe

## EMOJI_ICON

✍️

## RECIPE

-   `C => T+` (Trigger generates Text document)
-   `C => D+` (Trigger generates JSON document)
-   (Alternate conceptual view: `D+ => T+` / `D+ => D+` where D+ is the Graph/Workflow Data)

## X2Y_FORMAT

-   `C2T_WorkflowDoc`
-   `C2D_WorkflowDoc`

## ALIASES

-   Workflow Documenter
-   Graph Exporter
-   Doc Gen
-   Workflow Info

## TAGS

-   documentation
-   workflow
-   graph
-   export
-   utils
-   scribe
-   markdown
-   json
-   LOKI
-   metadata
-   explain

## METAPHORS

-   A diligent chronicler mapping the flow of creation.
-   A cartographer for your node graph.
-   An automated technical writer for your ComfyUI setup.
-   A workflow's self-reflection. 

./scribe\scribe.json
{
  "node_name": "scribe",
  "version": "0.1.0",
  "category": "LOKI 🦊/Documentation",
  "output_node": true,
  "defaults": {
    "docs_base_url": "https://docs.getsalt.ai/md/Comfy/Nodes/",
    "filename_prefix": "workflow_doc",
    "output_format": "markdown",
    "include_docs": true
  }
} 

./build_workflow_tool\build_workflows.py
import os
import json
import subprocess
from pathlib import Path
from datetime import datetime

# Define base directory relative to this script's location
TOOL_BASE_DIR = Path(__file__).parent
# LOKI_ROOT_DIR should be the parent of the 'nodes' directory
LOKI_ROOT_DIR = TOOL_BASE_DIR.parent.parent

# Define expected location of the filter_nodes script
FILTER_NODES_SCRIPT_PATH = LOKI_ROOT_DIR / "nodes" / "filter_nodes" / "filter_nodes_node.py"

# Define where the list nodes output their files (NEW LOCATIONS)
LIST_INSTALLED_NODE_OUTPUT_DIR = LOKI_ROOT_DIR / "nodes" / "list_installed_nodes" / "output"
LIST_AVAILABLE_NODE_OUTPUT_DIR = LOKI_ROOT_DIR / "nodes" / "list_available_nodes" / "output"


def ensure_workflow_dir(workflow_name):
    """Create workflow directory if it doesn't exist"""
    workflow_dir = LOKI_ROOT_DIR / "workflows" / workflow_name
    workflow_dir.mkdir(parents=True, exist_ok=True)
    return workflow_dir

def load_workflow_requests():
    """Load workflow requests from JSON file"""
    requests_file = TOOL_BASE_DIR / "workflow_requests.json"
    if not requests_file.exists():
         print(f"Error: workflow_requests.json not found at {requests_file}")
         return {}
    with open(requests_file, "r") as f:
        return json.load(f)

def load_workflow_info(workflow_dir):
    """Load workflow.json if it exists"""
    workflow_file = workflow_dir / "workflow.json"
    if workflow_file.exists():
        with open(workflow_file, "r") as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                 print(f"Warning: Corrupted workflow.json in {workflow_dir}. Initializing.")
                 return {"status": "error", "error": {"message": "Corrupted workflow.json"}}
    return {"status": "pending"}

def save_workflow_info(workflow_dir, info):
    """Save workflow.json"""
    workflow_file = workflow_dir / "workflow.json"
    with open(workflow_file, "w") as f:
        json.dump(info, f, indent=2, default=str) # Use default=str for datetime


def process_workflow(workflow_name, workflow_data):
    """Process a single workflow by calling the standalone filter script"""
    workflow_dir = ensure_workflow_dir(workflow_name)

    # Load or initialize workflow info
    workflow_info = load_workflow_info(workflow_dir)

    # Check status
    if workflow_info.get("status") == "completed":
        print(f"[SKIP] {workflow_name} (already completed)")
        return
    elif workflow_info.get("status") == "processing":
         print(f"[WARN] {workflow_name} was in processing state. Retrying.")
         # Optionally reset state or handle differently

    print(f"\n[START] Processing {workflow_name}")
    print(f"Description: {workflow_data.get('description', '')}")

    # Define input files (UPDATED PATHS if documentation dir is also moved, but assuming it stays at root for now)
    # Assuming documentation generated by list_* nodes still goes to root documentation folder
    documentation_dir = LOKI_ROOT_DIR / "documentation"
    installed_nodes_md = LIST_INSTALLED_NODE_OUTPUT_DIR / "installed_nodes_detailed.md"
    available_nodes_md = LIST_AVAILABLE_NODE_OUTPUT_DIR / "available_nodes.md"

    # Check if input files exist (optional but recommended)
    if not installed_nodes_md.exists():
         print(f"[ERROR] Input file not found: {installed_nodes_md}")
         workflow_info.update({"status": "error", "error": {"message": f"Input file missing: {installed_nodes_md.name}"}})
         save_workflow_info(workflow_dir, workflow_info)
         return
    if not available_nodes_md.exists():
        print(f"[ERROR] Input file not found: {available_nodes_md}")
        workflow_info.update({"status": "error", "error": {"message": f"Input file missing: {available_nodes_md.name}"}})
        save_workflow_info(workflow_dir, workflow_info)
        return

    try:
        # Update workflow status
        workflow_info.update({
            "name": workflow_name,
            "data": workflow_data, # Store original request data
            "description": workflow_data.get("description", ""),
            "requirements": workflow_data.get("requirements", []),
            "status": "processing",
            "timestamp": datetime.now().isoformat(),
            "processed_inputs": {}, # Track which inputs were processed
            "output_files": {} # Track output files
        })
        save_workflow_info(workflow_dir, workflow_info)

        # --- Run filter_nodes.py for installed nodes ---
        print(f"[RUN] Filtering INSTALLED nodes for {workflow_name}...")
        output_installed_filtered = workflow_dir / "installed_nodes_filtered.md"
        cmd_installed = [
            "python", str(FILTER_NODES_SCRIPT_PATH),
            "--input", str(installed_nodes_md),
            "--output", str(output_installed_filtered),
            "--prompt", workflow_data["description"]
            # Add --threshold or --batch-size if needed from workflow_data
        ]
        # Use subprocess.run for simpler execution if real-time output isn't critical
        result_installed = subprocess.run(cmd_installed, capture_output=True, text=True, check=False) # check=False to handle errors manually

        print(result_installed.stdout) # Print stdout from the script
        if result_installed.returncode != 0:
             print(f"[ERROR] Filtering installed nodes failed (Code: {result_installed.returncode})\n{result_installed.stderr}")
             raise subprocess.CalledProcessError(result_installed.returncode, cmd_installed, stderr=result_installed.stderr)
        else:
             workflow_info["processed_inputs"]["installed_nodes"] = str(installed_nodes_md.relative_to(LIST_INSTALLED_NODE_OUTPUT_DIR.parent))
             workflow_info["output_files"]["installed_nodes_filtered"] = str(output_installed_filtered.relative_to(LOKI_ROOT_DIR)) # Store relative path
             print("[DONE] Filtering installed nodes.")


        # --- Run filter_nodes.py for available nodes ---
        print(f"[RUN] Filtering AVAILABLE nodes for {workflow_name}...")
        output_available_filtered = workflow_dir / "available_nodes_filtered.md"
        cmd_available = [
            "python", str(FILTER_NODES_SCRIPT_PATH),
            "--input", str(available_nodes_md),
            "--output", str(output_available_filtered),
            "--prompt", workflow_data["description"]
             # Add --threshold or --batch-size if needed
        ]
        result_available = subprocess.run(cmd_available, capture_output=True, text=True, check=False)

        print(result_available.stdout)
        if result_available.returncode != 0:
             print(f"[ERROR] Filtering available nodes failed (Code: {result_available.returncode})\n{result_available.stderr}")
             raise subprocess.CalledProcessError(result_available.returncode, cmd_available, stderr=result_available.stderr)
        else:
            workflow_info["processed_inputs"]["available_nodes"] = str(available_nodes_md.relative_to(LIST_AVAILABLE_NODE_OUTPUT_DIR.parent))
            workflow_info["output_files"]["available_nodes_filtered"] = str(output_available_filtered.relative_to(LOKI_ROOT_DIR))
            print("[DONE] Filtering available nodes.")

        # Update status to completed
        workflow_info["status"] = "completed"
        print(f"[SUCCESS] {workflow_name} completed successfully.")
        save_workflow_info(workflow_dir, workflow_info)

    except subprocess.CalledProcessError as e:
        print(f"[FATAL ERROR] {workflow_name} failed during subprocess execution:\n  Command: {' '.join(e.cmd)}\n  Stderr: {e.stderr}")
        workflow_info.update({"status": "error", "error": {"message": f"Subprocess failed code {e.returncode}", "command": ' '.join(e.cmd), "stderr": e.stderr}})
        save_workflow_info(workflow_dir, workflow_info)
    except Exception as e:
         print(f"[FATAL ERROR] Unexpected error processing {workflow_name}: {e}")
         workflow_info.update({"status": "error", "error": {"message": f"Unexpected error: {str(e)}", "type": type(e).__name__}})
         save_workflow_info(workflow_dir, workflow_info)


def main():
    """Main execution function for the build tool"""
    print("--- Starting Workflow Build Process ---")
    # Ensure base workflows directory exists
    (LOKI_ROOT_DIR / "workflows").mkdir(exist_ok=True)
    # Ensure documentation directory exists (where inputs are expected)
    (LOKI_ROOT_DIR / "documentation").mkdir(exist_ok=True)

    # Also ensure the *node* output dirs exist before the tool runs
    LIST_INSTALLED_NODE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    LIST_AVAILABLE_NODE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # Load workflow requests
    workflow_requests = load_workflow_requests()
    if not workflow_requests:
         print("No workflow requests found. Exiting.")
         return

    # Process each workflow
    print(f"Found {len(workflow_requests)} workflow requests.")
    for workflow_name, workflow_data in workflow_requests.items():
        process_workflow(workflow_name, workflow_data)

    print("\n--- Workflow Build Process Finished ---")


if __name__ == "__main__":
    main() 

./evaluate_relevance_llm\__init__.py
from .evaluate_relevance_llm_node import EvaluateRelevanceLLMNode

NODE_CLASS_MAPPINGS = {
    "EvaluateRelevanceLLM": EvaluateRelevanceLLMNode
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "EvaluateRelevanceLLM": "Evaluate Node Relevance (LLM)"
}
__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS'] 

./evaluate_relevance_llm\evaluate_relevance_llm_node.py
# No requests import needed here directly
import json
from typing import Dict
# Import the node containing the static method
from ..llm_query_api.llm_query_api_node import LLMQueryAPINode, DEFAULT_LLM_API_URL, DEFAULT_LLM_MODEL, DEFAULT_MAX_RETRIES, DEFAULT_TIMEOUT

# Default system prompt specific to this node's purpose
SYSTEM_PROMPT_FILTER = """You are an expert at analyzing ComfyUI nodes and determining their relevance to specific workflow tasks. Your job is to evaluate each node's description and determine how useful it would be for a given workflow goal. Please analyze the node based on: 1. Direct relevance to the workflow goal 2. Utility as a supporting node for the workflow 3. Specific features that would help achieve the goal. Rate the node's applicability from 0-100 where: 0-20: Not relevant, 21-40: Marginally relevant, 41-60: Moderately useful, 61-80: Very useful, 81-100: Essential for this workflow. Return only a JSON response in this format: {"score": <0-100>, "reason": "<brief 1-2 sentence explanation>"}"""


class EvaluateRelevanceLLMNode:
    """
    Takes a specific relevance evaluation prompt, queries the LLM API
    using the LLMQueryAPINode's static method, and returns the raw JSON
    response string containing score and reason.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_prompt": ("STRING", {"multiline": True}), # The prompt generated by GenerateFilterPromptNode
            },
             "optional": {
                 # Allow overriding the system prompt
                 "system_prompt": ("STRING", {"multiline": True, "default": SYSTEM_PROMPT_FILTER}),
                 "api_url": ("STRING", {"default": DEFAULT_LLM_API_URL}),
                 "model": ("STRING", {"default": DEFAULT_LLM_MODEL}),
                 "temperature": ("FLOAT", {"default": 0.3, "min": 0.0, "max": 2.0, "step": 0.1}),
                 "max_retries": ("INT", {"default": DEFAULT_MAX_RETRIES, "min": 0}),
                 "timeout": ("INT", {"default": DEFAULT_TIMEOUT, "min": 5}),
                 "trigger": ("*", {}),
             }
        }

    RETURN_TYPES = ("STRING",) # Output is the raw JSON string from the LLM
    RETURN_NAMES = ("llm_response_json",)
    FUNCTION = "evaluate_relevance"
    CATEGORY = "utils/llm"
    OUTPUT_NODE = False

    def evaluate_relevance(self, llm_prompt: str, system_prompt: str = SYSTEM_PROMPT_FILTER, api_url: str = DEFAULT_LLM_API_URL, model: str = DEFAULT_LLM_MODEL, temperature: float = 0.3, max_retries: int = DEFAULT_MAX_RETRIES, timeout: int = DEFAULT_TIMEOUT, trigger=None) -> tuple[str]:
        """Queries the LLM using the imported static method."""
        if not llm_prompt:
             print("Error: LLM prompt is empty.")
             error_response = json.dumps({"score": 0, "reason": "Input LLM prompt was empty."})
             return (error_response,)

        print(f"Sending prompt to LLM for relevance evaluation via LLMQueryAPINode...")
        # Call the static method from the imported node class
        llm_response_content = LLMQueryAPINode._static_query_llm_single(
            user_prompt=llm_prompt,
            system_prompt=system_prompt,
            api_url=api_url,
            model=model,
            temperature=temperature,
            max_retries=max_retries,
            timeout=timeout
        )

        # --- Post-processing (optional but good practice) ---
        # Try to parse the content to validate it's the expected JSON format
        try:
             parsed = json.loads(llm_response_content)
             # Optional: Add more validation, e.g., check for 'score' and 'reason' keys
             print(f"LLM Response received and parsed: {llm_response_content}")
             # Return the original content string as required by the node's RETURN_TYPES
             return (llm_response_content,)
        except json.JSONDecodeError:
             print(f"Warning: LLM response content is not valid JSON: {llm_response_content[:100]}...")
             # Return the raw content anyway, or an error JSON
             error_response = json.dumps({"error": "LLM response was not valid JSON", "raw_content": llm_response_content})
             return (error_response,) # Return error JSON
        except Exception as e:
             print(f"Error processing LLM response: {e}")
             error_response = json.dumps({"error": f"Failed to process LLM response: {e}", "raw_content": llm_response_content})
             return (error_response,) 

./filter_nodes\__init__.py
from .filter_nodes_node import FilterNodesNode

NODE_CLASS_MAPPINGS = {
    "FilterNodesLLM": FilterNodesNode
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "FilterNodesLLM": "Filter Nodes (LLM)"
}
__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS'] 

./filter_nodes\filter_nodes_node.py
import argparse
import json
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import os
# No requests import needed here directly

# Import the batch query node for its static method
from ..llm_query_api_batch.llm_query_api_batch_node import LLMQueryAPIBatchNode, DEFAULT_LLM_API_URL, DEFAULT_LLM_MODEL, DEFAULT_MAX_RETRIES, DEFAULT_TIMEOUT, DEFAULT_BATCH_SIZE, DEFAULT_TEMPERATURE

# Default system prompt specific to this node's batch filtering purpose
SYSTEM_PROMPT_FILTER_BATCH = """You are an expert at analyzing ComfyUI nodes and determining their relevance to specific workflow tasks. Your job is to evaluate each node's description and determine how useful it would be for a given workflow goal. Please analyze the node based on: 1. Direct relevance to the workflow goal 2. Utility as a supporting node for the workflow 3. Specific features that would help achieve the goal. Rate the node's applicability from 0-100 where: 0-20: Not relevant, 21-40: Marginally relevant, 41-60: Moderately useful, 61-80: Very useful, 81-100: Essential for this workflow.

You will be given multiple nodes to analyze in one request. You must respond with only a valid JSON array containing one JSON object (with 'score' and 'reason') for each node provided in the prompt, in the same order. Format: [{'score': number, 'reason': 'string'}, ...]"""


class FilterNodesNode:
    """
    Parses node documentation, evaluates relevance using an LLM
    (via LLMQueryAPIBatchNode's static method), filters based on score,
    and formats the output markdown.
    """
    NODE_OUTPUT_DIR = Path(__file__).parent / "output"

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "trigger": ("*", {}),
                "node_markdown_content": ("STRING", {"multiline": True}),
                "workflow_prompt": ("STRING", {"multiline": True, "default": "Describe the desired workflow..."}),
                "relevance_threshold": ("INT", {"default": 40, "min": 0, "max": 100}),
                "output_filename_prefix": ("STRING", {"default": "filtered_nodes"}),
            },
            "optional": {
                 "llm_batch_size": ("INT", {"default": DEFAULT_BATCH_SIZE, "min": 1, "max": 16}),
                 "batch_system_prompt": ("STRING", {"multiline": True, "default": SYSTEM_PROMPT_FILTER_BATCH}),
                 "api_url": ("STRING", {"default": DEFAULT_LLM_API_URL}),
                 "model": ("STRING", {"default": DEFAULT_LLM_MODEL}),
                 "temperature": ("FLOAT", {"default": DEFAULT_TEMPERATURE, "min": 0.0, "max": 2.0, "step": 0.1}),
                 "max_retries": ("INT", {"default": DEFAULT_MAX_RETRIES, "min": 0}),
                 "timeout": ("INT", {"default": DEFAULT_TIMEOUT, "min": 10}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("filtered_markdown", "output_filepath")
    FUNCTION = "filter_nodes_batch"
    CATEGORY = "utils/docs"
    OUTPUT_NODE = True

    def __init__(self):
        self.output_dir = FilterNodesNode.NODE_OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)

    # --- Helper Functions (Adapted from original script) ---

    def parse_node_metadata(self, text: str) -> dict:
        """Extract metadata from node description section"""
        metadata = {}
        lines = text.split('\n')
        # Look for specific patterns used in list_installed_nodes markdown
        for line in lines:
             if line.startswith('**Source:**'):
                  match = re.search(r'`(.*?)`', line)
                  if match: metadata['module'] = match.group(1)
             elif line.startswith('**Author:**'): # If available from manager list markdown
                  metadata['author'] = line.split('**Author:**')[1].strip()
             elif line.startswith('**Repository/Reference:**'):
                  match = re.search(r'\[(.*?)\]\(.*?\)', line) # Link format
                  if match: metadata['repository'] = match.group(1)
                  else: metadata['repository'] = line.split('**Repository/Reference:**')[1].strip() # Plain text format
             elif line.startswith('**Install Type:**'): # If available
                  metadata['install_type'] = line.split('**Install Type:**')[1].strip()
        return metadata


    def parse_node_blocks(self, text: str) -> List[Tuple[str, str, dict]]:
        """Parse markdown text into list of (node_name, full_block_text, metadata) tuples"""
        nodes = []
        current_node_name = None
        current_node_lines = []
        node_delimiter = "\n---\n" # Assuming this separates nodes in input markdown

        # Split by the delimiter, handling potential starting/ending delimiters
        blocks = [block.strip() for block in text.split(node_delimiter) if block.strip()]

        for block in blocks:
             lines = block.split('\n')
             node_name = "Unknown Node"
             metadata_text = block # Assume whole block might contain metadata initially
             description = block # Default description is the whole block

             # Find the node name (usually in H3)
             for line in lines:
                  if line.startswith('### '):
                       # Extract name, potentially including the backticked name
                       match = re.match(r'###\s*(.*?)(?:\s*`\((.*?)\)`)?', line)
                       if match:
                            node_name = match.group(1).strip()
                            # Prefer backticked name if present
                            if match.group(2):
                                node_name = match.group(2).strip()
                       break # Found the header

             # Simple approach: metadata is everything after the first H4 or before it if no H4
             desc_lines = []
             metadata_lines = []
             found_h4 = False
             for line in lines:
                  if line.startswith("####"):
                      found_h4 = True
                  if found_h4:
                       metadata_lines.append(line)
                  else:
                       desc_lines.append(line)

             description = "\n".join(desc_lines).strip()
             metadata_text = "\n".join(metadata_lines).strip()

             # Re-parse metadata from the identified section or whole block
             metadata = self.parse_node_metadata(metadata_text if metadata_lines else block)
             nodes.append((node_name, block, metadata)) # Store the original full block text

        return nodes

    def format_filtered_markdown(self, scored_nodes: List[Dict], original_prompt: str, threshold: int) -> str:
        """Formats the filtered nodes into a markdown string."""
        output_lines = [
            f"# Filtered Nodes for Workflow Prompt:\n",
            f"> {original_prompt}\n",
            f"(Threshold: {threshold}+)\n",
            "---", "" # Separator
        ]

        if not scored_nodes:
            output_lines.append("*No nodes met the relevance threshold.*")
            return "\n".join(output_lines)

        # Sort by score descending
        sorted_nodes = sorted(scored_nodes, key=lambda x: x['score'], reverse=True)

        for node_info in sorted_nodes:
            output_lines.append(f"## Score: {node_info['score']}/100\n")
            output_lines.append(f"**Reason:** {node_info['reason']}\n")
            # Append the original node block
            output_lines.append(node_info['original_block'])
            output_lines.append("\n---\n") # Separator

        return "\n".join(output_lines)


    # --- Node Execution Method (Batch Processing) ---
    def filter_nodes_batch(self, trigger=None, node_markdown_content: str = "", workflow_prompt: str = "", relevance_threshold: int = 40, output_filename_prefix: str = "filtered_nodes", llm_batch_size: int = DEFAULT_BATCH_SIZE, batch_system_prompt: str = SYSTEM_PROMPT_FILTER_BATCH, api_url: str = DEFAULT_LLM_API_URL, model: str = DEFAULT_LLM_MODEL, temperature: float = DEFAULT_TEMPERATURE, max_retries: int = DEFAULT_MAX_RETRIES, timeout: int = DEFAULT_TIMEOUT):
        """Parses markdown, generates prompts, calls LLM batch static method, filters, and formats."""
        if not node_markdown_content:
            return ("Error: Input markdown content is empty.", "Error: No input content")
        if not workflow_prompt:
             return ("Error: Workflow prompt is empty.", "Error: No prompt")

        print(f"Filtering nodes based on prompt: '{workflow_prompt[:50]}...'")
        nodes_to_evaluate = self.parse_node_blocks(node_markdown_content)
        total_nodes = len(nodes_to_evaluate)
        print(f"Parsed {total_nodes} node blocks to evaluate.")

        if total_nodes == 0:
            return ("*No node blocks found in the input markdown.*", "Error: No nodes parsed")

        # Generate Prompts
        llm_prompts = []
        for node_name, node_block_text, _ in nodes_to_evaluate:
            # Inline the prompt generation logic here
            llm_prompt = f"""Workflow Goal: {workflow_prompt}

Node Name: {node_name}

Node Information Block:
--- START BLOCK ---
{node_block_text}
--- END BLOCK ---

Evaluate this node's relevance and utility for the specified workflow goal based *only* on the information provided in the block."""
            llm_prompts.append(llm_prompt)

        # Query LLM using imported static batch logic
        print(f"Querying LLM via LLMQueryAPIBatchNode static method...")
        llm_results = LLMQueryAPIBatchNode._static_query_llm_batch(
            prompts_list=llm_prompts,
            system_prompt=batch_system_prompt,
            api_url=api_url, model=model, temperature=temperature,
            batch_size=llm_batch_size, max_retries=max_retries, timeout=timeout
        )

        # Process results
        filtered_nodes_info = []
        if len(llm_results) != total_nodes:
             print(f"Warning: Mismatch between prompts ({total_nodes}) and results ({len(llm_results)}). Padding.")
             while len(llm_results) < total_nodes: llm_results.append({"score": 0, "reason": "Missing LLM response"})

        for i, (node_name, block, metadata) in enumerate(nodes_to_evaluate):
            result = llm_results[i]
            # Check for error key from internal batch logic first
            if result.get("error"):
                 print(f"Skipping node '{node_name}' due to processing error: {result['error']}")
                 continue
            score = result.get("score", 0)
            reason = result.get("reason", "No reason provided.")

            # Ensure score is an integer
            try:
                 score = int(score)
            except (ValueError, TypeError):
                 print(f"Warning: Invalid score '{score}' for node '{node_name}'. Setting to 0.")
                 score = 0


            if score >= relevance_threshold:
                filtered_nodes_info.append({
                    "name": node_name,
                    "original_block": block, # Store original block text
                    "score": score,
                    "reason": reason,
                    **metadata # Include parsed metadata
                })

        print(f"Found {len(filtered_nodes_info)} nodes meeting threshold {relevance_threshold}.")

        # Format output markdown
        filtered_markdown_output = self.format_filtered_markdown(
             filtered_nodes_info, workflow_prompt, relevance_threshold
        )

        # Save to file
        safe_prefix = re.sub(r'[^\w\-_\. ]', '_', output_filename_prefix)
        # Create a more descriptive filename if possible
        prompt_slug = re.sub(r'[^\w\-]+', '_', workflow_prompt[:25]).strip('_')
        output_filename = f"{safe_prefix}_filtered_{prompt_slug}.md"
        output_filepath = self.output_dir / output_filename

        try:
            output_filepath.write_text(filtered_markdown_output, encoding="utf-8")
            print(f"Filtered nodes saved to: {output_filepath}")
        except Exception as e:
            print(f"Error saving filtered nodes file: {e}")
            return (f"Error saving file: {e}", f"Error: {e}")

        return (filtered_markdown_output, str(output_filepath.resolve()))


    # --- Standalone Execution Capability (Optional) ---
    # Keep if you still want to run this script independently

    @classmethod
    def execute_standalone(cls, input_file: Path, output_file: Path, prompt: str, threshold: int = 40, batch_size: int = 4):
        """Run the filtering process from the command line."""
        print(f"\n--- Standalone Execution ---")
        print(f"Input: {input_file}, Output: {output_file}, Prompt: '{prompt[:60]}...', Threshold: {threshold}, Batch: {batch_size}")

        if not input_file.exists():
            print(f"Error: Input file not found: {input_file}")
            return

        try:
            markdown_content = input_file.read_text(encoding='utf-8')
        except Exception as e:
            print(f"Error reading input file: {e}")
            return

        # Parse nodes directly using instance method (needs instantiation now)
        instance = cls() # Instantiate to use parse_node_blocks
        nodes_to_evaluate = instance.parse_node_blocks(markdown_content)
        total_nodes = len(nodes_to_evaluate)
        print(f"Parsed {total_nodes} node blocks.")
        if total_nodes == 0:
            print("No nodes found in input.")
            return

        # Generate Prompts (Inlined)
        llm_prompts = []
        for node_name, node_block_text, _ in nodes_to_evaluate:
            llm_prompt = f"""Workflow Goal: {prompt}

Node Name: {node_name}

Node Information Block:
--- START BLOCK ---
{node_block_text}
--- END BLOCK ---

Evaluate this node's relevance and utility for the specified workflow goal based *only* on the information provided in the block."""
            llm_prompts.append(llm_prompt)

        # Query LLM (using default params for simplicity in standalone)
        print(f"Querying LLM with batch size {batch_size}...")
        llm_results = LLMQueryAPIBatchNode._static_query_llm_batch(
            prompts_list=llm_prompts,
            system_prompt=SYSTEM_PROMPT_FILTER_BATCH,
            api_url=DEFAULT_LLM_API_URL,
            model=DEFAULT_LLM_MODEL,
            temperature=DEFAULT_TEMPERATURE,
            batch_size=batch_size,
            max_retries=DEFAULT_MAX_RETRIES,
            timeout=DEFAULT_TIMEOUT
        )

        # Process results
        filtered_nodes_info = []
        if len(llm_results) != total_nodes:
             print(f"Warning: Mismatch between prompts ({total_nodes}) and results ({len(llm_results)}). Padding.")
             while len(llm_results) < total_nodes: llm_results.append({"score": 0, "reason": "Missing LLM response"})

        for i, (node_name, block, metadata) in enumerate(nodes_to_evaluate):
            result = llm_results[i]
            # Check for error key from internal batch logic first
            if result.get("error"):
                 print(f"Skipping node '{node_name}' due to processing error: {result['error']}")
                 continue
            score = result.get("score", 0)
            reason = result.get("reason", "No reason provided.")

            # Ensure score is an integer
            try:
                 score = int(score)
            except (ValueError, TypeError):
                 print(f"Warning: Invalid score '{score}' for node '{node_name}'. Setting to 0.")
                 score = 0


            if score >= threshold:
                filtered_nodes_info.append({
                    "name": node_name,
                    "original_block": block, # Store original block text
                    "score": score,
                    "reason": reason,
                    **metadata # Include parsed metadata
                })

        print(f"Found {len(filtered_nodes_info)} nodes meeting threshold {threshold}.")

        # Format output markdown
        filtered_markdown_output = self.format_filtered_markdown(
             filtered_nodes_info, prompt, threshold
        )

        # Save to file
        safe_prefix = re.sub(r'[^\w\-_\. ]', '_', output_file.stem)
        # Create a more descriptive filename if possible
        prompt_slug = re.sub(r'[^\w\-]+', '_', prompt[:25]).strip('_')
        output_filename = f"{safe_prefix}_filtered_{prompt_slug}.md"
        output_filepath = output_file

        try:
            output_filepath.write_text(filtered_markdown_output, encoding="utf-8")
            print(f"Standalone output written to: {output_filepath}")
        except Exception as e:
            print(f"Error writing standalone output file: {e}")


# --- Main block for standalone execution ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='(Standalone) Filter ComfyUI node markdown using LLM.')
    parser.add_argument('--input', required=True, help='Input markdown file path.')
    parser.add_argument('--output', required=True, help='Output markdown file path.')
    parser.add_argument('--prompt', required=True, help='Workflow goal prompt.')
    parser.add_argument('--threshold', type=int, default=40, help='Relevance score threshold.')
    parser.add_argument('--batch-size', type=int, default=4, help='LLM batch size.')
    args = parser.parse_args()

    input_path = Path(args.input)
    output_path = Path(args.output)

    FilterNodesNode.execute_standalone(
        input_file=input_path,
        output_file=output_path,
        prompt=args.prompt,
        threshold=args.threshold,
        batch_size=args.batch_size
    ) 

./glamour\glamour_utils.py
import os
import hashlib
import json
import re
import folder_paths # Need this for BASE_OUTPUT_DIR resolution

class GlamourImageManager:
    # Resolve path relative to ComfyUI's output directory
    BASE_OUTPUT_DIR = os.path.abspath(os.path.join(folder_paths.get_output_directory(), "Glamour"))
    os.makedirs(BASE_OUTPUT_DIR, exist_ok=True) # Ensure directory exists on module load

    @staticmethod
    def to_snake_case(text: str) -> str:
        if not isinstance(text, str):
             text = str(text) # Attempt to convert non-strings
        return re.sub(r'[-\s]+', '_', re.sub(r'[^\w\s-]', '', text)).strip().lower()

    @staticmethod
    def generate_image_id(node_id: str, node_type: str, node_hash: str) -> str:
         """Generates an image ID using node type, ID, and a hash."""
         # Using the hash passed from JS ensures consistency with frontend state
         return f"{GlamourImageManager.to_snake_case(node_type)}_{node_id}_{node_hash}"

    @staticmethod
    def get_image_path(image_id: str, node_type: str = None) -> str:
        """
        Finds the most specific existing image path.
        Checks for specific ID (webp/png), then default type (webp/png).
        Returns the expected path for the specific ID png if none exist.
        """
        base_name_specific = os.path.join(GlamourImageManager.BASE_OUTPUT_DIR, image_id)
        paths_to_check = [
            f"{base_name_specific}.webp",
            f"{base_name_specific}.png",
        ]

        if node_type:
            base_name_default = os.path.join(
                GlamourImageManager.BASE_OUTPUT_DIR,
                GlamourImageManager.to_snake_case(node_type)
            )
            paths_to_check.extend([
                f"{base_name_default}.webp",
                f"{base_name_default}.png",
            ])

        for path in paths_to_check:
            if os.path.exists(path):
                return path

        # If none exist, return the path where the specific PNG *should* be
        # (this is the path JS will try first if generating an image)
        return f"{base_name_specific}.png" 

./glamour\__init__.py
import json
import os

from .glamour_node import GlamourNode

# Load config to get names
config_path = os.path.join(os.path.dirname(__file__), 'glamour.json')
try:
    with open(config_path, 'r') as f:
        config = json.load(f)
    NODE_NAME_INTERNAL = config.get('node_name_internal', 'GlamourNode_Fallback')
    NODE_DISPLAY_NAME = config.get('node_display_name', 'Glamour_Fallback (LOKI)')
except Exception as e:
    print(f"[LOKI Glamour Init Error] Could not load or parse glamour.json: {e}")
    NODE_NAME_INTERNAL = 'GlamourNode_Error'
    NODE_DISPLAY_NAME = 'Glamour_Error (LOKI)'

NODE_CLASS_MAPPINGS = {
    NODE_NAME_INTERNAL: GlamourNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    NODE_NAME_INTERNAL: NODE_DISPLAY_NAME
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS'] 

./glamour\glamour_node.py
import os
import folder_paths
# Updated import relative to the current file's directory
from .glamour_utils import GlamourImageManager

class GlamourNode:
    @classmethod
    def INPUT_TYPES(cls):
        # Add a boolean widget to control the overall feature
        return {
            "required": {
                "enable_controls": ("BOOLEAN", {"default": True}),
                "glamour_state": (["All Glamoured", "Mixed", "All Veiled"],),
                "transparency": ("BOOLEAN", {"default": True, "label_on": "Transparent Overlay", "label_off": "Opaque Overlay"})
            }
        }

    RETURN_TYPES = () # This node primarily affects the UI, doesn't return workflow data
    FUNCTION = "do_nothing" # Needs a function, even if it does nothing in the backend flow
    CATEGORY = "LOKI 🦊/UI"
    OUTPUT_NODE = True # Crucial: Allows it to interact with the UI framework
    NODE_NAME = "Glamour 🌘" # Keep the original internal name for JS matching if needed

    def __init__(self):
        # Utils might handle path creation now, but keep reference if needed
        self.glamour_dir = GlamourImageManager.BASE_OUTPUT_DIR
        os.makedirs(self.glamour_dir, exist_ok=True)

    def do_nothing(self, **kwargs):
        # This node's primary function is handled by its JavaScript counterpart
        # and the class methods registered as routes.
        return ()

    # --- Server Routes ---
    # These methods are called via HTTP requests from the JS frontend

    @classmethod
    def get_glamour_path(cls, image_id=None, node_type=None):
        """API endpoint to get the path for a glamour image."""
        if not image_id:
            # Use Flask or similar for proper HTTP responses if available,
            # otherwise return dicts which ComfyUI might handle.
            # For now, stick to dicts for simplicity.
            return {"success": False, "error": "No image_id provided"}

        try:
            path = GlamourImageManager.get_image_path(image_id, node_type)
            # Ensure path is absolute before making it relative
            abs_path = os.path.abspath(path)
            output_dir_abs = os.path.abspath(folder_paths.get_output_directory())

            if abs_path.startswith(output_dir_abs):
                 relative_path = os.path.relpath(abs_path, output_dir_abs)
                 # Normalize path separators for web
                 relative_path = relative_path.replace(os.sep, '/')
            else:
                 # Handle cases where the image might be outside the standard output dir (if applicable)
                 # This might indicate an issue or require different handling.
                 print(f"Warning: Glamour image path {abs_path} is outside output directory {output_dir_abs}")
                 # Fallback or error as appropriate for your setup
                 # For now, let's report success but maybe an empty path or flag this?
                 # Returning the absolute might be a security risk. Let's signal non-standard location.
                 return {
                     "success": True,
                     "path": None, # Indicate it's not relative to output
                     "absolute_path_warning": True, # Flag for frontend awareness
                     "exists": os.path.exists(abs_path)
                 }


            return {
                "success": True,
                "path": relative_path,
                "exists": os.path.exists(abs_path)
            }
        except Exception as e:
            print(f"Error in get_glamour_path: {e}")
            return {"success": False, "error": str(e)}


    @classmethod
    def check_glamour_timestamp(cls, image_id=None, node_type=None):
        """API endpoint to check the timestamp of a glamour image."""
        if not image_id:
            return {"success": False, "error": "No image_id provided"}

        try:
            path = GlamourImageManager.get_image_path(image_id, node_type)
            abs_path = os.path.abspath(path)
            output_dir_abs = os.path.abspath(folder_paths.get_output_directory())

            if os.path.exists(abs_path):
                timestamp = os.path.getmtime(abs_path)
                relative_path = None
                abs_warning = False
                if abs_path.startswith(output_dir_abs):
                    relative_path = os.path.relpath(abs_path, output_dir_abs).replace(os.sep, '/')
                else:
                    print(f"Warning: Timestamp check for non-output path: {abs_path}")
                    abs_warning = True


                return {
                    "success": True,
                    "timestamp": timestamp,
                    "path": relative_path, # Can be None if outside output dir
                    "absolute_path_warning": abs_warning,
                }
            else:
                return {"success": False, "error": "File not found"}
        except Exception as e:
            print(f"Error in check_glamour_timestamp: {e}")
            return {"success": False, "error": str(e)} 

./glamour\glamour.md
# Glamour Node Requirements (`glamour.md`)

## DESCRIPTION

Provides a dynamic, visually customizable UI overlay framework for other ComfyUI nodes within the LOKI ecosystem.

## PURPOSE

To allow nodes to define custom visual appearances and interactive behaviors directly within the ComfyUI graph interface, overriding the standard node presentation. This enhances user experience by providing richer visual feedback and node-specific controls integrated into their visual representation.

## INPUTS

*   **Implicit (Configuration):** Relies on target nodes having a `glamour/` subfolder containing:
    *   `glamour.json`: Defines UI states, images, widget mappings, and interactions.
    *   Image Assets (e.g., `glamour.png`, `glamour_alt1.png`): Visuals for different states.
*   **Implicit (API Calls):** Receives requests from its JavaScript counterpart (`js/glamour-*.js`) via internal ComfyUI server routes to:
    *   Get glamour image paths (`get_glamour_path`).
    *   Check image timestamps (`check_glamour_timestamp`).

## OUTPUTS

*   **Implicit (UI Modification):** Modifies the visual appearance and behavior of target ComfyUI nodes in the frontend graph. Does not output data directly into the workflow graph connections.

## NODE_STRATEGY

This is a core UI infrastructure node for the LOKI project. Its implementation is split:
*   **Backend (Python):** `glamour_node.py` defines the ComfyUI node structure and registers server API endpoints. `glamour_utils.py` handles file path logic and image management.
*   **Frontend (JavaScript):** Files in the `js/` directory (`glamour-ui.js`, `glamour.js`, `glamour-images.js`) handle the rendering of overlays, interaction logic (clicks, hovers), state management, and communication with the Python backend via API calls.

It acts as a central service that other LOKI nodes utilize by providing the necessary configuration files in their respective `glamour/` subdirectories. 

./glamour\STATUS.md
# Glamour Node Status (`STATUS.md`)

## Current Status

*   **Implemented:**
    *   Basic ComfyUI Node structure (`glamour_node.py`).
    *   Core Python utility for image path management (`glamour_utils.py`).
    *   Backend API endpoints (`get_glamour_path`, `check_glamour_timestamp`) for JS communication.
    *   JavaScript logic (`js/glamour-*.js`) for fetching configurations, rendering image overlays, and basic state handling (loading images based on config).
    *   Initial support for different glamour states (`All Glamoured`, `Mixed`, `All Veiled`) via node widget.
    *   Transparency toggle widget.
*   **Partially Implemented / In Progress:**
    *   Handling interactions defined in `glamour.json` (basic structure exists, needs full implementation).
    *   Widget overlay rendering and mapping (needs implementation based on `glamour.json`).

## Known Limitations / Issues

*   Full interaction model (`on_click`, `on_hover`, etc.) from `glamour.json` spec is not yet implemented in JS.
*   Overlaying and interacting with standard ComfyUI widgets based on `glamour.json` is not yet implemented.
*   Error handling for missing configurations or assets could be improved.
*   Performance implications for large numbers of glamoured nodes haven't been tested.
*   No visual icon or card representation defined yet.

## TODOs / Future Work

*   [ ] Implement full `interactions` handling in `js/glamour.js`.
*   [ ] Implement widget overlay rendering and linking in `js/glamour-ui.js`.
*   [ ] Refine state management (default, cycling, setting states via interactions).
*   [ ] Add support for animated image formats (`.webp`, `.gif`) with `play_mode`.
*   [ ] Develop robust error handling and user feedback (e.g., placeholder visuals for missing assets).
*   [ ] Define and create default Icon and Card assets (`nodes/glamour/glamour/` folders).
*   [ ] Create standard `glamour.json` schema documentation/validation.
*   [ ] Investigate performance and optimization strategies.
*   [ ] Add caching mechanisms for fetched configurations/assets if needed.
*   [ ] Create basic tests (`glamour_test.py`).

## PRIORITY

**High.** This node is fundamental infrastructure for the visual and interactive aspects of other LOKI nodes. 

./glamour\glamour_name.md
# Glamour Node Naming (`glamour_name.md`)

## NAME

Glamour

## TOKEN

`🌘GlamourUI`

## EMOJI_ICON

`🌘`

## RECIPE

`🧩 => 🧩🌘`

_(Conceptual recipe: Takes another node's glamour configuration and produces a visual UI override for that node)._

## X2Y_FORMAT

`C+2G+_Glamour`

## ALIASES

*   UI Override
*   Node Skin
*   Visualizer
*   Node Glamour
*   LOKI UI Engine

## TAGS

`UI`, `Visual`, `Interface`, `LOKI`, `Core`, `Dynamic UI`, `Frontend`, `UX`, `Node Appearance`

## METAPHORS

*   A chameleon cloak nodes can wear.
*   A stage director for the ComfyUI node graph.
*   A visual skinning engine for nodes.
*   A customizable dashboard overlay for individual nodes.
*   The 'look and feel' controller for LOKI nodes. 

./glamour\glamour.json
{
  "node_name_internal": "GlamourNode",
  "node_display_name": "🌘 Glamour (LOKI)",
  "category": "LOKI 🦊/UI",
  "output_node": true,
  "version": "0.1.0",
  "description": "Applies dynamic visual overlays and interactions to other nodes."
} 

./glamour\glamour.py
# glamour.py - LOKI Script layer for Glamour node functionality

import argparse
import os
import sys

# Determine the parent directory of the current script's directory
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
# Add the parent directory (ComfyUI-LOKI/nodes) to sys.path
# This allows importing from sibling directories like 'glamour'
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Try importing the utility function
try:
    from glamour.glamour_utils import GlamourImageManager
except ImportError as e:
    print(f"Error importing GlamourImageManager: {e}")
    print("Please ensure the script is run from within the ComfyUI-LOKI directory structure,")
    print("or that ComfyUI-LOKI/nodes is in your PYTHONPATH.")
    GlamourImageManager = None # Indicate failure

def check_image(image_id, node_type):
    """Checks the status of a specific glamour image."""
    if not GlamourImageManager:
        print("GlamourImageManager could not be loaded. Cannot check image.")
        return

    print(f"--- Checking Glamour Image ---")
    print(f"  Image ID: {image_id}")
    print(f"  Node Type: {node_type if node_type else 'N/A'}")
    try:
        path = GlamourImageManager.get_image_path(image_id, node_type)
        abs_path = os.path.abspath(path)
        exists = os.path.exists(abs_path)
        print(f"  Resolved Path: {abs_path}")
        print(f"  Exists: {exists}")
        if exists:
            timestamp = os.path.getmtime(abs_path)
            print(f"  Last Modified Timestamp: {timestamp}")
            size = os.path.getsize(abs_path)
            print(f"  Size (bytes): {size}")
        else:
             print(f"  Reason: File not found at the expected location.")

    except Exception as e:
        print(f"  Error during check: {e}")
    print(f"-----------------------------")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LOKI Glamour Script Utilities. Provides command-line access to check Glamour image assets.",
        epilog="Example: python glamour.py --check my_node_output_001 --type SaveImageNode"
    )
    parser.add_argument(
        "--check",
        metavar="IMAGE_ID",
        required=True,
        help="Check status of a Glamour image by its base ID (e.g., the output filename without extension)."
    )
    parser.add_argument(
        "--type",
        metavar="NODE_TYPE",
        default=None,
        help="Optional: Node type associated with the image ID (used for potential type-specific subdirectories)."
    )

    args = parser.parse_args()

    if GlamourImageManager: # Proceed only if import was successful
        check_image(args.check, args.type)
    else:
        # Error message already printed during import attempt
        sys.exit(1) # Exit with error code 

./list_available_nodes\__init__.py
from .list_available_nodes_node import ListAvailableNodes

NODE_CLASS_MAPPINGS = {
    "LokiListAvailableNodes": ListAvailableNodes
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LokiListAvailableNodes": "🌐🔍 List Available Nodes (LOKI)"
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS'] 

./list_available_nodes\list_available_nodes_node.py
import json
import os
import sys
from pathlib import Path
import requests
from collections import defaultdict
import re
from typing import List, Dict, Optional
import uuid
from io import StringIO

# Import the core logic
from .list_available_nodes import load_extension_list

# --- Node Definition ---

class ListAvailableNodes:
    """
    Lists available ComfyUI custom node packages from ComfyUI-Manager's list or a fallback URL.
    Outputs the data in various formats (JSON string, Markdown string, list of JSON, list of Markdown).
    """
    _class_last_node_info_hash = None # Class variable for IS_CHANGED

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {},
            "optional": {
                 "manager_cache_path_override": ("STRING", {"default": "", "multiline": False}),
                 "fallback_github_url": ("STRING", {"default": "https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json"})
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING") # Match list_installed_nodes
    RETURN_NAMES = ("all_nodes_json", "summary_markdown", "node_json_list", "node_markdown_list")
    FUNCTION = "list_available"
    CATEGORY = "LOKI 🦊/System"
    OUTPUT_NODE = False # Not writing files

    def __init__(self):
        pass # No output dir needed

    # --- Helper: Generate Markdown for a single available node --- (Adapted from original write_extension_info)
    @staticmethod
    def generate_single_node_markdown(ext: Dict) -> str:
        """Generates detailed markdown documentation string for a single available node/extension."""
        md_stream = StringIO()

        # Normalize keys: 'reference' might be 'files'[0]
        if 'reference' not in ext and 'files' in ext and ext.get('files'):
             ext['reference'] = ext['files'][0]

        title = ext.get('title', ext.get('name', 'Unknown Extension'))
        md_stream.write(f"### {title}\n\n")

        if ext.get('description'):
            md_stream.write(f"{ext['description']}\n\n")

        md_stream.write(f"- **Author:** {ext.get('author', 'Unknown')}\n")

        reference = ext.get('reference')
        if reference:
             if reference.startswith('http://') or reference.startswith('https://'):
                 md_stream.write(f"- **Repository/Reference:** [{reference}]({reference})\n")
             else:
                 md_stream.write(f"- **Repository/Reference:** {reference}\n")

        if ext.get('install_type'):
            md_stream.write(f"- **Install Type:** {ext['install_type']}\n")

        node_types = ext.get('nodeTypes', ext.get('nodes')) # Handle alias
        if node_types and isinstance(node_types, list):
            md_stream.write("\n**Included Node Types:**\n")
            for node_type in sorted(node_types):
                md_stream.write(f"- `{node_type}`\n")

        md_stream.write("\n---\n") # Separator
        return md_stream.getvalue()

    # --- Helper: Generate Summary Markdown String --- (Adapted from original write_markdown)
    @staticmethod
    def generate_summary_markdown(extensions: List[Dict]) -> str:
        """Generates a condensed markdown summary string of available nodes/extensions."""
        md_stream = StringIO()
        md_stream.write("# Available ComfyUI Custom Nodes (Summary)\n\n")

        # Group by category if available
        by_category = defaultdict(list)
        uncategorized = []
        for ext in extensions:
            category = ext.get('category')
            if category and isinstance(category, str):
                by_category[category].append(ext)
            else:
                uncategorized.append(ext)

        # Write categorized extensions
        for category in sorted(by_category.keys()):
            md_stream.write(f"## {category}\n\n")
            sorted_extensions = sorted(by_category[category], key=lambda x: x.get('title', x.get('name', '')).lower())
            for ext in sorted_extensions:
                 title = ext.get('title', ext.get('name', 'Unknown Extension'))
                 desc = ext.get('description', '').split('\n')[0]
                 if len(desc) > 120: desc = desc[:117] + '...'
                 author = ext.get('author', '?')
                 md_stream.write(f"- **{title}** (by {author}): {desc}\n")
            md_stream.write("\n")

        # Write uncategorized extensions
        if uncategorized:
            md_stream.write("## Uncategorized\n\n")
            sorted_extensions = sorted(uncategorized, key=lambda x: x.get('title', x.get('name', '')).lower())
            for ext in sorted_extensions:
                 title = ext.get('title', ext.get('name', 'Unknown Extension'))
                 desc = ext.get('description', '').split('\n')[0]
                 if len(desc) > 120: desc = desc[:117] + '...'
                 author = ext.get('author', '?')
                 md_stream.write(f"- **{title}** (by {author}): {desc}\n")
            md_stream.write("\n")

        return md_stream.getvalue()

    # --- JSON Serialization Helper ---
    @staticmethod
    def _serialize_item(item):
        # Basic serialization, extend if needed for complex types
        try:
            json.dumps(item)
            return item
        except TypeError:
            return repr(item)

    # --- Main Execution Logic ---
    def list_available(self, manager_cache_path_override=None, fallback_github_url=None):
        print("LOKI List Available Nodes: Executing...")
        extensions = []
        error_str = None
        try:
            extensions = load_extension_list(manager_cache_path_override, fallback_github_url)
        except Exception as e:
            print(f"Error loading available node list: {e}")
            error_str = f"Failed to load available nodes: {e}"

        # Calculate hash of current results (even if errored, hash the error state)
        current_hash = None
        try:
            data_to_hash = extensions if error_str is None else {"error": error_str}
            serialized_info = json.dumps(data_to_hash, sort_keys=True, default=repr)
            current_hash = hash(serialized_info)
            print(f"Calculated current available nodes hash: {current_hash}")
        except Exception as e:
            print(f"Warning: Could not calculate hash for available nodes: {e}")
            current_hash = hash(str(data_to_hash)) # Fallback hash

        # Update the class hash *after* successful execution/hashing
        ListAvailableNodes._class_last_node_info_hash = current_hash

        # --- Output Generation ---
        all_nodes_json_string = "{}"
        summary_markdown_string = ""
        node_json_list = []
        node_markdown_list = []

        if error_str:
             all_nodes_json_string = json.dumps({"error": error_str})
             summary_markdown_string = f"# Error\n\n{error_str}"
             # node_json_list and node_markdown_list remain empty
        else:
            # 1. Generate Full JSON String
            try:
                 all_nodes_json_string = json.dumps(extensions, indent=4, default=self._serialize_item)
            except Exception as json_err:
                 print(f"Error serializing available node info to JSON: {json_err}")
                 all_nodes_json_string = json.dumps({"error": "Failed to serialize available node info", "details": str(json_err)})

            # 2. Generate Summary Markdown String
            try:
                summary_markdown_string = self.generate_summary_markdown(extensions)
            except Exception as e:
                print(f"Error generating summary markdown: {e}")
                summary_markdown_string = f"# Error Generating Summary\n\n{e}"

            # 3. & 4. Generate Lists
            for ext_info in extensions:
                ext_key = ext_info.get('name', ext_info.get('reference', str(uuid.uuid4())))
                # JSON List Item
                try:
                    node_json_str = json.dumps({ext_key: ext_info}, indent=4, default=self._serialize_item)
                    node_json_list.append(node_json_str)
                except Exception as e:
                    print(f"Error serializing single available node JSON ({ext_key}): {e}")
                    node_json_list.append(json.dumps({"error": f"Failed to serialize node {ext_key}", "details": str(e)}))
                # Markdown List Item
                try:
                    node_md_str = self.generate_single_node_markdown(ext_info)
                    node_markdown_list.append(node_md_str)
                except Exception as e:
                     print(f"Error generating single available node markdown ({ext_key}): {e}")
                     node_markdown_list.append(f"### Error: Node {ext_key}\n\n{e}\n---")

        # Return the four outputs
        return (all_nodes_json_string, summary_markdown_string, node_json_list, node_markdown_list)

    @classmethod
    def IS_CHANGED(cls, manager_cache_path_override=None, fallback_github_url=None):
        """Check if the available node list has actually changed since the last run."""
        print("LOKI List Available Nodes: IS_CHANGED check")
        current_extensions = []
        error_str = None
        try:
            current_extensions = load_extension_list(manager_cache_path_override, fallback_github_url)
        except Exception as e:
            print(f"  IS_CHANGED: Error loading list: {e}")
            error_str = f"Failed to load list: {e}"

        current_hash = None
        try:
            data_to_hash = current_extensions if error_str is None else {"error": error_str}
            serialized_info = json.dumps(data_to_hash, sort_keys=True, default=repr)
            current_hash = hash(serialized_info)
            print(f"  IS_CHANGED current hash: {current_hash}")
            print(f"  IS_CHANGED last class hash: {cls._class_last_node_info_hash}")
        except Exception as e:
            print(f"  Warning: Could not calculate hash for IS_CHANGED check: {e}")
            return str(uuid.uuid4()) # Assume change if hashing fails

        if current_hash != cls._class_last_node_info_hash:
            print("  IS_CHANGED detected changes.")
            return str(uuid.uuid4())
        else:
            print("  IS_CHANGED detected no changes.")
            return cls._class_last_node_info_hash 

./list_available_nodes\list_available_nodes.py
import json
import os
import sys
from pathlib import Path
import requests
from typing import List, Dict, Optional

# --- Core Logic for Fetching Available Nodes ---

def get_manager_path(override_path: str = None) -> Optional[str]:
    """Find ComfyUI-Manager directory"""
    if override_path and os.path.isdir(override_path):
         print(f"Using provided manager path: {override_path}")
         return override_path

    # Try to find it relative to a potential ComfyUI root
    # Heuristic: Go up several levels from this script's location
    # Assumes standard ComfyUI/custom_nodes/ComfyUI-LOKI structure
    try:
        current_dir = Path(__file__).resolve().parent # nodes/list_available_nodes
        loki_dir = current_dir.parent # nodes
        custom_nodes_dir = loki_dir.parent # custom_nodes
        comfy_root_dir = custom_nodes_dir.parent # Potential ComfyUI root

        # Search known possible locations for ComfyUI-Manager
        possible_paths = [
            custom_nodes_dir / "ComfyUI-Manager",
            comfy_root_dir / "custom_nodes" / "ComfyUI-Manager"
        ]

        for manager_path in possible_paths:
            if manager_path.is_dir():
                 print(f"Found manager path: {manager_path}")
                 return str(manager_path)

    except Exception as e:
         print(f"Warning: Error determining relative paths: {e}")

    print(f"Warning: Could not automatically find ComfyUI-Manager directory.")
    return None # Indicate not found

def load_extension_list(manager_path_override: str = None, fallback_url: str = None) -> List[Dict]:
    """Load extension list from ComfyUI Manager's cache or fetch from GitHub"""
    manager_dir = get_manager_path(manager_path_override)
    cache_file = None
    data = None

    if manager_dir:
        cache_file = Path(manager_dir) / "cache" / "custom-node-list.json"
        if cache_file.exists():
            print(f"Loading extensions from cache: {cache_file}")
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    data = json.loads(content)
                    # Check structure (can be list or dict with 'custom_nodes')
                    if isinstance(data, dict) and 'custom_nodes' in data:
                        return data['custom_nodes']
                    elif isinstance(data, list):
                        return data # Already a list
                    else:
                         print(f"Warning: Unexpected JSON structure in cache file: {type(data)}")
                         data = None # Reset data to trigger fallback
            except json.JSONDecodeError as e:
                print(f"JSON Parse Error in cache file {cache_file}: {e}")
                # Fall through to GitHub fetch
            except Exception as e:
                print(f"Error reading cache file {cache_file}: {e}")
                # Fall through to GitHub fetch
        else:
            print(f"Manager cache file not found at: {cache_file}")

    # Fallback to direct GitHub fetch if cache failed or not found
    if data is None and fallback_url:
        print(f"Attempting to fetch extension list from: {fallback_url}")
        try:
            response = requests.get(fallback_url, timeout=15)
            response.raise_for_status()
            data = response.json()
            # Check structure again
            if isinstance(data, dict) and 'custom_nodes' in data:
                return data['custom_nodes']
            elif isinstance(data, list):
                return data
            else:
                 print(f"Error: Unexpected JSON structure from GitHub URL: {type(data)}")
                 raise ValueError("Invalid data structure from fallback URL")

        except requests.exceptions.RequestException as e:
            print(f"Error fetching extension list from {fallback_url}: {e}")
            raise # Re-raise network errors
        except json.JSONDecodeError as e:
             print(f"JSON Parse Error from {fallback_url}: {e}")
             raise # Re-raise parse errors
        except Exception as e:
             print(f"Unexpected error fetching from {fallback_url}: {e}")
             raise # Re-raise other errors
    elif data is None:
         raise FileNotFoundError("Could not load extension list from cache or fallback URL.")

    # This line should theoretically not be reached if logic is correct,
    # but added for safety. It implies data was loaded but wasn't the right type.
    if data is None:
        raise ValueError("Failed to load or parse extension list data.")
    # If data was loaded from cache but wasn't list/dict, return empty or error?
    # Returning empty list for now if structure was wrong but file existed.
    return []


# --- Standalone Execution Example --- (Optional)
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Fetch available ComfyUI nodes list.")
    parser.add_argument("--manager-path", help="Optional path to override ComfyUI-Manager directory.")
    parser.add_argument("--url", default="https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json", help="Fallback URL to fetch the list.")
    parser.add_argument("-o", "--output", default="standalone_available_nodes.json", help="Output JSON file name.")

    args = parser.parse_args()

    try:
        print("Running available node lister standalone...")
        extensions = load_extension_list(args.manager_path, args.url)

        output_file = Path(args.output)
        # Ensure output directory exists (create if necessary)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        print(f"Writing results to {output_file.resolve()}...")
        with output_file.open('w', encoding='utf-8') as f:
            json.dump(extensions, f, indent=4)
        print("Done.")

    except Exception as e:
        print(f"An error occurred: {e}")
        sys.exit(1) 

./list_available_nodes\list_available_nodes.md
# Requirements: list_available_nodes

## DESCRIPTION

Lists available ComfyUI custom node packages, primarily by fetching the list used by ComfyUI-Manager.

## INTENT

To provide a dynamic list of known available custom nodes for discovery, installation planning, or analysis.

## NODE_STRATEGY

This node attempts to load the `custom-node-list.json` file cached by an existing ComfyUI-Manager installation. If the manager is not found or the cache is missing/invalid, it falls back to fetching the list directly from a specified GitHub URL (defaulting to the ComfyUI-Manager repository's list).

## INPUTS

- `manager_cache_path_override` (optional, STRING): Manually specify the path to the ComfyUI-Manager directory if automatic detection fails.
- `fallback_github_url` (optional, STRING): The URL to fetch the `custom-node-list.json` from if the local cache cannot be used.

## OUTPUTS

- `all_nodes_json` (STRING): A JSON string representing the entire list of available extensions/nodes.
- `summary_markdown` (STRING): A Markdown string summarizing the available extensions, grouped by category.
- `node_json_list` (LIST): A list where each item is a JSON string representing a single available extension.
- `node_markdown_list` (LIST): A list where each item is a Markdown string detailing a single available extension. 

./list_available_nodes\STATUS.md
# Node Status: list_available_nodes

## Current Status

- **Refactored:** Core logic separated, node wrapper updated to match `list_installed_nodes` format.
- **Documentation:** Basic spec files generated.

## Priority

- **Medium:** Useful utility for node discovery and ecosystem analysis.

## TODO

- [ ] Add more robust error handling for network issues or invalid JSON.
- [ ] Consider adding options to filter the list based on keywords, author, etc. (would likely happen in a separate filter node).
- [ ] Improve automatic detection of ComfyUI-Manager path if possible.
- [ ] Add test cases (`list_available_nodes_test.py`) - needs mocking for network requests/cache files.
- [ ] Add visual/glamour layers. 

./list_available_nodes\list_available_nodes_name.md
# Node Name: list_available_nodes

## Official Name

List Available Nodes

## TOKEN

🌐🔍ListAvailable

## EMOJI_ICON

🌐

## RECIPE

`_ => 🌐🔍` (Outputs various list formats)

## X2Y_FORMAT

`C2D+_available_nodes` (Fetches external data, outputs structured data)

## ALIASES

- List Available Custom Nodes
- ComfyUI Manager List
- Node Browser

## TAGS

- utility
- system
- info
- nodes
- list
- available
- custom_nodes
- manager
- remote
- discovery

## METAPHORS

- A scout reporting back on potential tools found elsewhere.
- Browsing an online catalog of extensions.
- Checking the ComfyUI-Manager index. 

./list_available_nodes\list_available_nodes.json
{
  "node_name": "list_available_nodes",
  "version": "0.1.0",
  "inputs": {
    "manager_cache_path_override": {
      "type": "STRING",
      "description": "Optional path to manually specify the ComfyUI-Manager directory.",
      "required": false,
      "default": ""
    },
    "fallback_github_url": {
      "type": "STRING",
      "description": "URL to fetch the custom-node-list.json if local cache fails.",
      "required": false,
      "default": "https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json"
    }
  },
  "outputs": {
    "all_nodes_json": {
        "type": "STRING",
        "description": "A JSON string containing the entire list of available nodes/extensions."
    },
    "summary_markdown": {
        "type": "STRING",
        "description": "A Markdown string summarizing the available extensions."
    },
    "node_json_list": {
        "type": "LIST",
        "subtype": "STRING",
        "description": "A list of JSON strings, each representing one available extension."
    },
     "node_markdown_list": {
        "type": "LIST",
        "subtype": "STRING",
        "description": "A list of Markdown strings, each detailing one available extension."
    }
  },
  "constants": {}
} 

./list_installed_nodes\__init__.py
from .list_installed_nodes_node import ListInstalledNodes

NODE_CLASS_MAPPINGS = {
    "LokiListInstalledNodes": ListInstalledNodes
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "LokiListInstalledNodes": "📜🔍 List Installed Nodes (LOKI)"
}
__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS'] 

./list_installed_nodes\list_installed_nodes_node.py
import os
import sys
from pathlib import Path
import json
from collections import defaultdict
import importlib
import inspect
from typing import Dict, Any, List, Tuple
import re
import uuid
from io import StringIO # For generating strings

# Import the core logic from the separate file
from .list_installed_nodes import get_all_node_info

# Add ComfyUI root directory to Python path if running standalone,
# but usually not needed when run as a node.
# COMFY_ROOT = str(Path(__file__).parent.parent.parent.parent) # Adjust path depth
# sys.path.append(COMFY_ROOT)
# Instead, rely on ComfyUI's environment providing access to 'nodes'

# --- Node Definition ---

class ListInstalledNodes:
    """
    Lists installed ComfyUI nodes (built-in and custom) and outputs
    their information in various formats (JSON string, Markdown string, list of JSON, list of Markdown).
    """
    _class_last_node_info_hash = None # Class variable to store the hash

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {},
             "optional": {
                  "include_custom_nodes": ("BOOLEAN", {"default": True}),
             }
        }

    # Updated RETURN types and names
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("all_nodes_json", "summary_markdown", "node_json_list", "node_markdown_list")
    FUNCTION = "list_nodes"
    CATEGORY = "LOKI 🦊/System"
    OUTPUT_NODE = False # Not primarily writing files anymore

    # NODE_OUTPUT_DIR = Path(__file__).parent / "output" # Removed

    def __init__(self):
        # self.output_dir = ListInstalledNodes.NODE_OUTPUT_DIR # Removed
        # self.output_dir.mkdir(parents=True, exist_ok=True) # Removed
        pass # No instance setup needed now

    # --- Helper: Generate Markdown for a single node --- (Adapted from write_markdown_to_file)
    @staticmethod
    def generate_single_node_markdown(class_key: str, info: Dict[str, Any]) -> str:
        """Generates detailed markdown documentation string for a single node."""
        # Use StringIO to capture the markdown string
        md_stream = StringIO()

        node_name = info.get('node_name', class_key)
        display_name = info.get('display_name', node_name)

        if 'error' in info:
             md_stream.write(f"### {display_name} (`{node_name}` / `{class_key}`)\n\n")
             md_stream.write(f"**Error processing this node:** {info['error']}\n\n")
             md_stream.write("---\n")
             return md_stream.getvalue()

        # Node header
        md_stream.write(f"### {display_name} (`{node_name}`)\n\n")

        # Class Name and Module
        md_stream.write(f"**Class:** `{info.get('class_name', 'N/A')}`\n")
        md_stream.write(f"**Source Module:** `{info.get('module', 'unknown')}`")
        if info.get('is_custom'):
             md_stream.write(" (Custom)")
        if info.get('is_output_node'):
             md_stream.write(" (Output Node)")
        md_stream.write("\n")
        md_stream.write(f"**File Path:** `{info.get('file_path', 'unknown')}`\n\n")

        # Description
        if info.get('description'):
            md_stream.write(f"{info['description']}\n\n")
        else:
            md_stream.write("*No description provided.*\n\n")

        # Inputs
        inputs = info.get('inputs', {})
        if inputs:
            md_stream.write("**Inputs:**\n\n")
            md_stream.write("| Name | Type | Required | Tooltip |\n")
            md_stream.write("|---|---|---|---|\n")
            for input_name, input_details in inputs.items():
                req = 'Yes' if input_details.get('required') else 'No'
                tooltip = info.get('input_tooltips', {}).get(input_name, '')
                # Truncate long tooltips
                if len(tooltip) > 100:
                    tooltip = tooltip[:97] + '...'
                # Escape pipes in tooltips
                tooltip = tooltip.replace('|', '\\|')
                md_stream.write(f"| `{input_name}` | `{input_details.get('type', '')}` | {req} | {tooltip} |\n")
            md_stream.write("\n")
        else:
            md_stream.write("**Inputs:** *None*\n\n")

        # Outputs
        outputs = info.get('outputs', [])
        if outputs:
            md_stream.write("**Outputs:**\n\n")
            md_stream.write("| Index | Name | Type | Tooltip |\n")
            md_stream.write("|---|---|---|---|\n")
            output_names = info.get('output_names', [])
            output_tooltips = info.get('output_tooltips', [])
            # Ensure lists have correct length for zip
            if len(output_names) < len(outputs):
                 output_names.extend([f'output_{i}' for i in range(len(output_names), len(outputs))])
            if len(output_tooltips) < len(outputs):
                 output_tooltips.extend(['' for _ in range(len(output_tooltips), len(outputs))])

            for i, (out_type, out_name, out_tooltip) in enumerate(zip(outputs, output_names, output_tooltips)):
                tooltip = out_tooltip or ''
                if len(tooltip) > 100:
                    tooltip = tooltip[:97] + '...'
                tooltip = tooltip.replace('|', '\\|')
                md_stream.write(f"| {i} | `{out_name}` | `{out_type}` | {tooltip} |\n")
            md_stream.write("\n")
        else:
            md_stream.write("**Outputs:** *None*\n\n")

        md_stream.write("---\n") # Separator between nodes
        return md_stream.getvalue()


    # --- Helper: Generate Summary Markdown String --- (Adapted from write_short_markdown)
    @staticmethod
    def generate_summary_markdown(node_info: Dict[str, Dict[str, Any]]) -> str:
        """Generates a condensed markdown summary string of all nodes."""
        md_stream = StringIO()
        md_stream.write("# Installed ComfyUI Nodes (Summary)\n\n")

        # Group by category
        by_category = defaultdict(list)
        for class_key, info in node_info.items():
            category = info.get('category', 'uncategorized') if 'error' not in info else 'Errored Nodes'
            by_category[category].append((class_key, info))

        # Write each category
        for category in sorted(by_category.keys()):
            md_stream.write(f"## {category.upper()}\n\n")
            sorted_nodes = sorted(by_category[category], key=lambda item: item[1].get('display_name', item[0]))
            for class_key, info in sorted_nodes:
                node_name = info.get('node_name', class_key)
                display_name = info.get('display_name', node_name)
                if 'error' in info:
                    md_stream.write(f"- **{display_name} (`{node_name}`)** - ERROR: {info['error']}\n")
                else:
                    desc = info.get('description', '').split('\n')[0] # First line of description
                    if len(desc) > 120:
                        desc = desc[:117] + '...'
                    module_info = f" (Source: `{info.get('module', '?')}`)" if info.get('is_custom') else ''
                    md_stream.write(f"- **{display_name} (`{node_name}`)**{module_info}: {desc}\n")
            md_stream.write("\n")

        return md_stream.getvalue()

    # --- JSON Serialization Helper ---
    @staticmethod
    def _serialize_item(item):
        if isinstance(item, Path):
            return str(item)
        try:
            json.dumps(item)
            return item
        except TypeError:
            return repr(item)

    # --- Main Node Execution Logic ---
    def list_nodes(self, include_custom_nodes=True):
        print(f"LOKI List Installed Nodes: Executing (Include Custom: {include_custom_nodes})")
        node_info = get_all_node_info(include_custom=include_custom_nodes)

        # Calculate hash of current results
        current_hash = None
        try:
             serialized_info = json.dumps(node_info, sort_keys=True, default=repr)
             current_hash = hash(serialized_info)
             print(f"Calculated current node info hash: {current_hash}")
        except Exception as e:
             print(f"Warning: Could not calculate hash for node info: {e}")
             current_hash = hash(str(node_info)) # Fallback hash

        # Update the class hash *after* successful execution and hashing
        ListInstalledNodes._class_last_node_info_hash = current_hash

        # --- Output Generation ---
        all_nodes_json_string = "{}"
        summary_markdown_string = ""
        node_json_list = []
        node_markdown_list = []

        # 1. Generate Full JSON String
        try:
             all_nodes_json_string = json.dumps(node_info, indent=4, default=self._serialize_item)
        except Exception as json_err:
             print(f"Error serializing node info to JSON: {json_err}")
             all_nodes_json_string = json.dumps({"error": "Failed to serialize node info", "details": str(json_err)})

        # 2. Generate Summary Markdown String
        try:
            summary_markdown_string = self.generate_summary_markdown(node_info)
        except Exception as e:
            print(f"Error generating summary markdown: {e}")
            summary_markdown_string = f"# Error Generating Summary\n\n{e}"

        # 3. Generate List of Node JSON Strings
        # 4. Generate List of Node Markdown Strings
        for class_key, info in node_info.items():
            # JSON List Item
            try:
                node_json_str = json.dumps({class_key: info}, indent=4, default=self._serialize_item)
                node_json_list.append(node_json_str)
            except Exception as e:
                print(f"Error serializing single node JSON ({class_key}): {e}")
                node_json_list.append(json.dumps({"error": f"Failed to serialize node {class_key}", "details": str(e)}))

            # Markdown List Item
            try:
                node_md_str = self.generate_single_node_markdown(class_key, info)
                node_markdown_list.append(node_md_str)
            except Exception as e:
                 print(f"Error generating single node markdown ({class_key}): {e}")
                 node_markdown_list.append(f"### Error: Node {class_key}\n\n{e}\n---")


        # Return the four outputs
        return (all_nodes_json_string, summary_markdown_string, node_json_list, node_markdown_list)

    @classmethod
    def IS_CHANGED(cls, include_custom_nodes=True):
        """Check if the node list has actually changed since the last run."""
        print("LOKI List Installed Nodes: IS_CHANGED check")
        # Get current node info
        # Note: This incurs the cost of scanning nodes during the check phase.
        # Consider optimizations if this becomes a bottleneck (e.g., caching results briefly).
        current_node_info = get_all_node_info(include_custom=include_custom_nodes)
        current_hash = None
        try:
            serialized_info = json.dumps(current_node_info, sort_keys=True, default=repr)
            current_hash = hash(serialized_info)
            print(f"  IS_CHANGED current hash: {current_hash}")
            print(f"  IS_CHANGED last class hash: {cls._class_last_node_info_hash}")
        except Exception as e:
            print(f"  Warning: Could not calculate hash for IS_CHANGED check: {e}")
            # If hashing fails, assume it changed to be safe
            return str(uuid.uuid4())

        # Compare with the stored hash from the last successful execution
        if current_hash != cls._class_last_node_info_hash:
            print("  IS_CHANGED detected changes.")
            # Return a new unique ID to signal change
            # (Don't update the class hash here, only in list_nodes after success)
            return str(uuid.uuid4())
        else:
            print("  IS_CHANGED detected no changes.")
            # Return the *same* hash to signal no change
            return cls._class_last_node_info_hash

# --- ComfyUI Registration --- (Moved to __init__.py in the parent nodes directory)
# NODE_CLASS_MAPPINGS = {
#     "ListInstalledNodes_Loki": ListInstalledNodesNode
# }
# NODE_DISPLAY_NAME_MAPPINGS = {
#     "ListInstalledNodes_Loki": "List Installed Nodes 📜 (LOKI)"
# } 

./list_installed_nodes\list_installed_nodes.json
{
  "node_name": "list_installed_nodes",
  "version": "0.1.0",
  "inputs": {
    "trigger": {
      "type": "*",
      "description": "Optional trigger signal to re-run the node.",
      "required": false
    }
  },
  "outputs": {
    "NODE_LIST": {
      "type": "LIST",
      "description": "A list of strings representing installed custom nodes."
    }
  },
  "constants": {}
} 

./list_installed_nodes\list_installed_nodes.md
# Requirements: list_installed_nodes

## DESCRIPTION

Lists all detected custom nodes installed in the ComfyUI environment.

## INTENT

To provide a dynamic list of installed custom nodes for inspection or use in other nodes.

## NODE_STRATEGY

This node scans the ComfyUI `custom_nodes` directory (and potentially other configured node directories) to identify installed nodes. It likely relies on ComfyUI's internal node loading mechanisms or directory scanning.

## INPUTS

- `trigger` (optional, any type `*`): An optional input that can be used to trigger re-execution of the node when the input changes.

## OUTPUTS

- `NODE_LIST` (LIST): A list of strings, where each string represents an identified installed custom node (e.g., by class name, display name, or module path). 

./list_installed_nodes\STATUS.md
# Node Status: list_installed_nodes

## Current Status

- **Implemented:** Basic functionality exists in `list_installed_nodes_node.py`.
- **Documentation:** Basic spec files generated.

## Priority

- **Medium:** Useful utility node, but not critical path.

## TODO

- [ ] Refine output format (e.g., provide more structured data than just strings?).
- [ ] Add error handling for cases where node directories cannot be scanned.
- [ ] Consider adding options to filter the list (e.g., by keyword, author, directory).
- [ ] Create test cases (`list_installed_nodes_test.py`).
- [ ] Evaluate if core logic should be split into `list_installed_nodes.py`.
- [ ] Add visual/glamour layers. 

./list_installed_nodes\list_installed_nodes_name.md
# Node Name: list_installed_nodes

## Official Name

List Installed Nodes

## TOKEN

📜🔍ListInstalled

## EMOJI_ICON

📜

## RECIPE

`_ => 📜🔍`

## X2Y_FORMAT

`C2D+_installed_nodes` (Takes no specific input, outputs structured data - a list)

## ALIASES

- List Nodes
- Installed Nodes
- Custom Node Lister

## TAGS

- utility
- system
- info
- nodes
- list
- installed
- custom_nodes

## METAPHORS

- A librarian checking the catalog of available tools.
- A system inventory scanner.
- A directory listing for custom nodes. 

./list_installed_nodes\list_installed_nodes.py
import os
import sys
from pathlib import Path
import importlib
import inspect
from typing import Dict, Any, List, Tuple
import argparse
import traceback
import json

# Global variables to hold the loaded mappings
# We populate these once during standalone loading
LOADED_NODE_CLASS_MAPPINGS = {}
LOADED_NODE_DISPLAY_NAME_MAPPINGS = {}
COMFY_ROOT_PATH = None

def get_node_io_info(node_class) -> Dict[str, Any]:
    """Extract comprehensive input/output information from a node class"""
    info = {
        'inputs': {}, 'input_tooltips': {},
        'outputs': [], 'output_tooltips': [], 'output_is_list': [], 'output_names': [],
        'description': getattr(node_class, 'DESCRIPTION', getattr(node_class, '__doc__', '') or '').strip(),
        'category': getattr(node_class, 'CATEGORY', 'unknown')
    }
    try:
        if hasattr(node_class, 'INPUT_TYPES') and callable(node_class.INPUT_TYPES):
            input_types = node_class.INPUT_TYPES()
            for section in ['required', 'optional']:
                if section in input_types:
                    for input_name, input_config in input_types[section].items():
                        input_type_info = input_config[0] if isinstance(input_config, (list, tuple)) else input_config
                        options = input_config[1] if isinstance(input_config, (list, tuple)) and len(input_config) > 1 else None
                        tooltip = options.get('tooltip') if isinstance(options, dict) else None
                        info['inputs'][input_name] = {
                            'type': input_type_info,
                            'required': section == 'required',
                            'options': options
                        }
                        if tooltip:
                            info['input_tooltips'][input_name] = tooltip

        if hasattr(node_class, 'RETURN_TYPES'):
            ret_types = node_class.RETURN_TYPES
            info['outputs'] = list(ret_types) if isinstance(ret_types, (list, tuple)) else [ret_types]

        if hasattr(node_class, 'OUTPUT_TOOLTIPS'):
            info['output_tooltips'] = list(node_class.OUTPUT_TOOLTIPS)

        if hasattr(node_class, 'OUTPUT_IS_LIST'):
            info['output_is_list'] = list(node_class.OUTPUT_IS_LIST)
        else:
            info['output_is_list'] = [False] * len(info.get('outputs', []))

        if hasattr(node_class, 'RETURN_NAMES'):
            ret_names = node_class.RETURN_NAMES
            info['output_names'] = list(ret_names) if isinstance(ret_names, (list, tuple)) else [ret_names]

        if len(info.get('output_names', [])) != len(info.get('outputs', [])):
            info['output_names'] = [str(t) for t in info.get('outputs', [])]

    except Exception as e:
        print(f"Warning: Error processing IO for {node_class.__name__}: {e}")
        # Ensure lists are initialized even on error
        info['outputs'] = info.get('outputs', [])
        info['output_names'] = info.get('output_names', [])
        info['output_is_list'] = info.get('output_is_list', [False] * len(info.get('outputs', [])))
        if len(info.get('output_names', [])) != len(info.get('outputs', [])):
            info['output_names'] = [f"output_{i}" for i in range(len(info.get('outputs', [])))]

    return info

def get_all_node_info(include_custom=True, class_mappings=None, display_mappings=None) -> Dict[str, Dict[str, Any]]:
    """Gather comprehensive information about all nodes.

    Args:
        include_custom: Whether to include custom nodes.
        class_mappings: Pre-loaded NODE_CLASS_MAPPINGS (used in standalone mode).
        display_mappings: Pre-loaded NODE_DISPLAY_NAME_MAPPINGS (used in standalone mode).

    Returns:
        A dictionary containing information about each discovered node.
    """
    # Determine which mappings to use
    active_class_mappings = class_mappings
    active_display_mappings = display_mappings

    # If mappings weren't provided (i.e., running inside ComfyUI),
    # try to import and use the live `nodes` module.
    if active_class_mappings is None or active_display_mappings is None:
        print("Mapping arguments not provided, attempting to import live ComfyUI 'nodes' module...")
        try:
            import nodes # Try importing the live module
            active_class_mappings = nodes.NODE_CLASS_MAPPINGS
            active_display_mappings = nodes.NODE_DISPLAY_NAME_MAPPINGS
            print("Successfully imported live 'nodes' module.")
        except ImportError:
            print("Error: Could not import live 'nodes' module. Node mappings are unavailable.")
            return {}
        except Exception as e:
             print(f"Error importing or accessing live 'nodes' module mappings: {e}")
             return {}

    if not active_class_mappings:
        print("Error: Node class mappings are empty or unavailable.")
        return {}

    node_info_dict = {}
    processed_classes = set()

    print("Processing nodes...")
    # Iterate using the active mappings
    for node_name, node_class in active_class_mappings.items():
        if node_class in processed_classes:
            continue
        try:
            display_name = active_display_mappings.get(node_name, node_name)
            io_info = get_node_io_info(node_class)

            module_name = getattr(node_class, '__module__', 'unknown')
            file_path = 'unknown'
            is_custom = False

            if module_name != 'unknown':
                try:
                    file_path = inspect.getfile(node_class)
                    normalized_file_path = os.path.normpath(file_path)
                    # Assume COMFY_ROOT_PATH is set correctly by load_comfyui_nodes *if running standalone*
                    # In ComfyUI mode, COMFY_ROOT_PATH might be None, rely on path structure.
                    custom_node_dir_part = os.path.normpath('custom_nodes')
                    comfy_nodes_dir_part = os.path.normpath('comfy') # Check within comfy too

                    # More robust check
                    path_parts = Path(normalized_file_path).parts
                    if custom_node_dir_part in path_parts:
                        is_custom = True
                        # Find the custom node root directory name
                        try:
                             custom_nodes_index = path_parts.index(custom_node_dir_part)
                             if custom_nodes_index + 1 < len(path_parts):
                                 module_name = path_parts[custom_nodes_index + 1]
                             else:
                                 module_name = "custom_nodes_root"
                        except ValueError: # Should not happen if check passed, but safeguard
                              module_name = "custom_node_unknown_structure"

                    elif comfy_nodes_dir_part in path_parts and 'nodes.py' not in path_parts[-1]: # Check if it's inside comfy/, but not the base nodes.py
                        # Could be a built-in node within a subfolder of comfy/
                        module_name = 'ComfyUI'
                    elif 'nodes.py' in path_parts[-1]: # Base comfy/nodes.py
                         module_name = 'ComfyUI'
                    else:
                        # Could be a built-in type or from another library added to path
                        if not COMFY_ROOT_PATH or not normalized_file_path.startswith(os.path.normpath(COMFY_ROOT_PATH)):
                            # Likely not part of comfy install path, treat as external/built-in
                            pass # Keep original module_name
                        else: # Inside comfy root, but not nodes/ or custom_nodes/
                             module_name = module_name # Keep original module name

                except TypeError:
                    file_path = 'built-in or unknown source'
                    module_name = 'built-in'
                except Exception as inspect_e:
                    print(f"Warning: Could not inspect file for {node_name} ({node_class.__name__}): {inspect_e}")

            if not include_custom and is_custom:
                continue

            class_key = f"{module_name}.{node_class.__name__}"
            node_info_dict[class_key] = {
                'class_name': node_class.__name__,
                'node_name': node_name,
                'display_name': display_name,
                'is_output_node': getattr(node_class, 'OUTPUT_NODE', False),
                'module': module_name,
                'file_path': file_path,
                'is_custom': is_custom,
                **io_info
            }
            processed_classes.add(node_class)

        except Exception as e:
            print(f"Error processing node {node_name} ({getattr(node_class, '__name__', 'UnknownClass')}): {e}")
            error_key = f"error.{node_name}.{getattr(node_class, '__name__', 'UnknownClass')}"
            node_info_dict[error_key] = {'error': str(e), 'display_name': node_name, 'node_name': node_name, 'module': 'error'}
            if 'node_class' in locals() and isinstance(node_class, type):
                 processed_classes.add(node_class)

    print(f"Processed {len(node_info_dict)} total nodes ({len(processed_classes)} unique classes).")
    return node_info_dict

def load_comfyui_nodes(comfyui_path: str):
    """Loads node mappings by simulating ComfyUI's loading process."""
    global LOADED_NODE_CLASS_MAPPINGS, LOADED_NODE_DISPLAY_NAME_MAPPINGS, COMFY_ROOT_PATH

    comfy_path = Path(comfyui_path).resolve()
    COMFY_ROOT_PATH = str(comfy_path)

    if not comfy_path.is_dir():
        print(f"Error: ComfyUI path not found or not a directory: {comfyui_path}")
        return False

    # Add ComfyUI path to sys.path to allow imports
    if str(comfy_path) not in sys.path:
        sys.path.insert(0, str(comfy_path))
        print(f"Added {comfy_path} to sys.path")

    # 1. Import base nodes
    try:
        import nodes # Import ComfyUI's base nodes.py
        LOADED_NODE_CLASS_MAPPINGS = nodes.NODE_CLASS_MAPPINGS.copy()
        LOADED_NODE_DISPLAY_NAME_MAPPINGS = nodes.NODE_DISPLAY_NAME_MAPPINGS.copy()
        print(f"Loaded {len(LOADED_NODE_CLASS_MAPPINGS)} built-in nodes.")
    except ImportError as e:
        print(f"Error: Could not import base 'nodes' module from {comfy_path}: {e}")
        return False
    except Exception as e:
         print(f"Error loading base nodes: {e}")
         traceback.print_exc()
         return False

    # 2. Scan and import custom nodes
    custom_nodes_path = comfy_path / "custom_nodes"
    if not custom_nodes_path.is_dir():
        print("No custom_nodes directory found. Skipping custom nodes.")
        return True

    print(f"Scanning custom nodes in: {custom_nodes_path}")
    for item in custom_nodes_path.iterdir():
        if item.is_dir():
            init_path = item / "__init__.py"
            module_name = f"custom_nodes.{item.name}"
            if init_path.is_file():
                try:
                    print(f"  -> Importing module: {module_name}")
                    module = importlib.import_module(module_name)

                    if hasattr(module, "NODE_CLASS_MAPPINGS") and isinstance(module.NODE_CLASS_MAPPINGS, dict):
                        LOADED_NODE_CLASS_MAPPINGS.update(module.NODE_CLASS_MAPPINGS)
                        print(f"     - Added {len(module.NODE_CLASS_MAPPINGS)} class mappings from {item.name}")

                    if hasattr(module, "NODE_DISPLAY_NAME_MAPPINGS") and isinstance(module.NODE_DISPLAY_NAME_MAPPINGS, dict):
                        LOADED_NODE_DISPLAY_NAME_MAPPINGS.update(module.NODE_DISPLAY_NAME_MAPPINGS)
                        print(f"     - Added {len(module.NODE_DISPLAY_NAME_MAPPINGS)} display name mappings from {item.name}")

                except Exception as e:
                    print(f"  -> Failed to import or process module {module_name}: {e}")
                    # Optionally print traceback:
                    # traceback.print_exc()
        # Optionally handle single-file custom nodes directly in custom_nodes? (Less common)

    print(f"Finished loading. Total unique node classes: {len(set(LOADED_NODE_CLASS_MAPPINGS.values()))}")
    return True

# Example usage if run standalone
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="List installed ComfyUI nodes.")
    parser.add_argument("comfyui_path", help="Path to the root ComfyUI directory.")
    parser.add_argument("--include-custom", action=argparse.BooleanOptionalAction, default=True, help="Include custom nodes in the listing.")
    # Output defaults to the script's output directory
    script_dir = Path(__file__).parent
    default_output_dir = script_dir / "output"
    default_output_file = default_output_dir / "standalone_installed_nodes.json"
    parser.add_argument("-o", "--output", default=str(default_output_file), help=f"Output JSON file path. Defaults to {default_output_file}")
    args = parser.parse_args()

    print("Running node lister standalone...")

    # Ensure output directory exists
    output_file = Path(args.output)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    # Load node information first
    if load_comfyui_nodes(args.comfyui_path):
        # Then get the detailed info using the loaded mappings
        # Pass the loaded mappings explicitly to the function
        all_nodes = get_all_node_info(include_custom=args.include_custom, class_mappings=LOADED_NODE_CLASS_MAPPINGS, display_mappings=LOADED_NODE_DISPLAY_NAME_MAPPINGS)

        if all_nodes:
            print(f"Writing results to {output_file.resolve()}...")
            with output_file.open('w', encoding='utf-8') as f:
                def serialize_item(item):
                    if isinstance(item, Path):
                        return str(item)
                    try:
                        json.dumps(item)
                        return item
                    except TypeError:
                        return repr(item)

                import json # Import here as it's only needed for output
                json.dump(all_nodes, f, indent=4, default=serialize_item)
            print("Done.")
        else:
            print("No nodes found or error occurred during info gathering.")
    else:
        print("Failed to load ComfyUI node mappings.") 

./llm_query_api\__init__.py
from .llm_query_api_node import LLMQueryAPINode

NODE_CLASS_MAPPINGS = {
    "LLMQueryAPI": LLMQueryAPINode
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "LLMQueryAPI": "LLM Query API (Single)"
}
__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS'] 

./llm_query_api\llm_query_api_node.py
import requests
import json
from typing import Dict

# Default constants can be overridden by inputs
DEFAULT_LLM_API_URL = "http://localhost:12345/v1/chat/completions"
DEFAULT_LLM_MODEL = "meta-llama2-3.1-8b-instruct"
DEFAULT_MAX_RETRIES = 3
DEFAULT_TIMEOUT = 60

class LLMQueryAPINode:
    """
    Sends a single prompt to a generic LLM API (OpenAI compatible)
    and returns the raw response content string.
    Handles retries on failure. Also provides a static method for reuse.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "user_prompt": ("STRING", {"multiline": True}),
                "system_prompt": ("STRING", {"multiline": True, "default": "You are a helpful assistant."}),
            },
            "optional": {
                "api_url": ("STRING", {"default": DEFAULT_LLM_API_URL}),
                "model": ("STRING", {"default": DEFAULT_LLM_MODEL}),
                "temperature": ("FLOAT", {"default": 0.3, "min": 0.0, "max": 2.0, "step": 0.1}),
                "max_retries": ("INT", {"default": DEFAULT_MAX_RETRIES, "min": 0}),
                "timeout": ("INT", {"default": DEFAULT_TIMEOUT, "min": 5}),
                 # Optional trigger
                 "trigger": ("*", {}),
            }
        }

    RETURN_TYPES = ("STRING",) # Raw response content string (usually JSON)
    RETURN_NAMES = ("llm_response_content",)
    FUNCTION = "query_llm_api"
    CATEGORY = "utils/llm"
    OUTPUT_NODE = False

    @staticmethod
    def _static_query_llm_single(user_prompt: str, system_prompt: str, api_url: str, model: str, temperature: float, max_retries: int, timeout: int) -> str:
        """Static method containing the core LLM API query logic with retries."""
        if not user_prompt:
            print("Error (_static_query_llm_single): User prompt is empty.")
            return json.dumps({"error": "User prompt cannot be empty."})

        headers = {"Content-Type": "application/json"}
        data = {"model": model, "messages": [{"role": "system", "content": system_prompt},{"role": "user", "content": user_prompt}], "temperature": temperature}
        retry_count = 0
        response = None # Initialize response to handle potential error in JSONDecodeError block

        while retry_count <= max_retries:
            attempt = retry_count + 1
            print(f"Attempt {attempt}/{max_retries + 1} to query LLM API at {api_url}...")
            try:
                response = requests.post(api_url, headers=headers, json=data, timeout=timeout)
                response.raise_for_status()
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0 and "message" in result["choices"][0] and "content" in result["choices"][0]["message"]:
                    content = result["choices"][0]["message"]["content"]
                    print("LLM API call successful.")
                    return content
                else:
                    print("Error: Unexpected LLM API response structure.")
                    return json.dumps(result)
            except requests.exceptions.Timeout: print(f"Attempt {attempt}: Request timed out after {timeout} seconds.")
            except requests.exceptions.HTTPError as e: print(f"Attempt {attempt}: HTTP Error querying LLM API: {e}")
            except requests.exceptions.RequestException as e: print(f"Attempt {attempt}: Request Exception querying LLM API: {e}")
            except json.JSONDecodeError as e:
                 # Use response.text if available, otherwise indicate inability to decode
                 response_text = getattr(response, 'text', '<response object not available>')
                 print(f"Attempt {attempt}: Failed to decode LLM API response JSON: {e}")
                 return json.dumps({"error": f"JSONDecodeError: {e}", "response_text": response_text})
            except Exception as e: print(f"Attempt {attempt}: Unexpected error during LLM query: {e}")

            retry_count += 1
            if retry_count > max_retries: break

        error_message = f"LLM API query failed after {max_retries + 1} attempts."
        print(error_message)
        return json.dumps({"error": error_message})

    def query_llm_api(self, user_prompt: str, system_prompt: str, api_url: str = DEFAULT_LLM_API_URL, model: str = DEFAULT_LLM_MODEL, temperature: float = 0.3, max_retries: int = DEFAULT_MAX_RETRIES, timeout: int = DEFAULT_TIMEOUT, trigger=None) -> tuple[str]:
        """Node execution function: Calls the static query method."""
        result_content = self._static_query_llm_single(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            api_url=api_url,
            model=model,
            temperature=temperature,
            max_retries=max_retries,
            timeout=timeout
        )
        return (result_content,) 

./llm_query_api_batch\__init__.py
from .llm_query_api_batch_node import LLMQueryAPIBatchNode

NODE_CLASS_MAPPINGS = {
    "LLMQueryAPIBatch": LLMQueryAPIBatchNode
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "LLMQueryAPIBatch": "LLM Query API (Batch)"
}
__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS'] 

./llm_query_api_batch\llm_query_api_batch_node.py
import requests
import json
import re
from typing import List, Dict

# Default constants can be overridden by inputs
DEFAULT_LLM_API_URL = "http://localhost:12345/v1/chat/completions"
DEFAULT_LLM_MODEL = "meta-llama2-3.1-8b-instruct"
DEFAULT_MAX_RETRIES = 3
DEFAULT_TIMEOUT = 120 # Longer timeout for batch
DEFAULT_BATCH_SIZE = 4
DEFAULT_TEMPERATURE = 0.7

class LLMQueryAPIBatchNode:
    """
    Sends multiple prompts to a generic LLM API (OpenAI compatible) in batches.
    Expects the LLM to return a structured response (e.g., JSON array for filter node use case).
    Handles retries on failure. Provides a static method for reuse.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                # Takes a list of prompts (e.g., joined by a delimiter or as a JSON list string)
                "user_prompts": ("STRING", {"multiline": True, "default": '["Prompt 1", "Prompt 2"]'}),
                "system_prompt": ("STRING", {"multiline": True, "default": "You are a helpful assistant processing batch requests."}),
            },
            "optional": {
                "api_url": ("STRING", {"default": DEFAULT_LLM_API_URL}),
                "model": ("STRING", {"default": DEFAULT_LLM_MODEL}),
                "temperature": ("FLOAT", {"default": 0.3, "min": 0.0, "max": 2.0, "step": 0.1}),
                "batch_size": ("INT", {"default": DEFAULT_BATCH_SIZE, "min": 1}),
                "max_retries": ("INT", {"default": DEFAULT_MAX_RETRIES, "min": 0}),
                "timeout": ("INT", {"default": DEFAULT_TIMEOUT, "min": 10}),
                # Trigger optional
                "trigger": ("*", {}),
            }
        }

    RETURN_TYPES = ("STRING",) # JSON string representing the list of results
    RETURN_NAMES = ("batch_response_json",)
    FUNCTION = "query_llm_api_batch"
    CATEGORY = "utils/llm"
    OUTPUT_NODE = False

    # --- Static Helper Methods ---
    @staticmethod
    def _static_parse_input_prompts(prompts_input: str) -> List[str]:
        """Static: Attempts to parse the input string into a list of prompts."""
        prompts_input = prompts_input.strip()
        try:
            # Try parsing as JSON list
            if prompts_input.startswith('[') and prompts_input.endswith(']'):
                parsed = json.loads(prompts_input)
                if isinstance(parsed, list) and all(isinstance(p, str) for p in parsed):
                    return parsed
                else:
                     print("Warning: Input looks like JSON list but failed validation.")
        except json.JSONDecodeError:
            print("Warning: Input is not a valid JSON list. Trying newline splitting.")

        # Fallback: Split by newline (ignoring empty lines)
        return [line for line in prompts_input.splitlines() if line.strip()]

    @staticmethod
    def _static_format_batch_request_prompt(batch_prompts: List[str]) -> str:
         """Static: Formats a list of prompts into a single string for the batch API call."""
         # This formatting might need adjustment depending on how the target LLM handles batch requests.
         # For the filter use case, we used numbered blocks.
         formatted_prompt = "Process the following requests based on the system instructions. Provide a response for each request, ideally in a structured format like a JSON array:\n\n"
         formatted_prompt += "\n\n".join([
             f"--- Request {idx + 1} ---\n{p}\n--- End Request {idx + 1} ---"
             for idx, p in enumerate(batch_prompts)
         ])
         return formatted_prompt

    @staticmethod
    def _static_query_llm_batch(prompts_list: List[str], system_prompt: str, api_url: str, model: str, temperature: float, batch_size: int, max_retries: int, timeout: int) -> List[Dict]:
        """Static method containing the core batch query logic."""
        all_results = []
        headers = {"Content-Type": "application/json"}

        for i in range(0, len(prompts_list), batch_size):
            current_batch_prompts = prompts_list[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            print(f"Processing Batch {batch_num} ({len(current_batch_prompts)} prompts)...")

            # Format the request for the batch API call
            # IMPORTANT: This assumes the LLM endpoint can handle a single combined prompt
            # representing the batch. If the API requires separate calls per item,
            # this node should instead iterate and call the single LLMQueryAPI logic.
            # For now, we assume a combined prompt approach.
            combined_request_prompt = LLMQueryAPIBatchNode._static_format_batch_request_prompt(current_batch_prompts)

            data = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": combined_request_prompt}
                ],
                "temperature": temperature
            }

            retry_count = 0
            batch_success = False
            response = None # Initialize response

            while retry_count <= max_retries:
                attempt = retry_count + 1
                print(f"  Batch {batch_num} - Attempt {attempt}/{max_retries + 1}...")
                try:
                    response = requests.post(api_url, headers=headers, json=data, timeout=timeout)
                    response.raise_for_status()
                    response_json = response.json()
                    content = response_json["choices"][0]["message"]["content"]

                    # --- Response Parsing Logic ---
                    # This is crucial and depends heavily on the expected output format
                    # For the filter use case, we expect a JSON array string matching the batch size.
                    print(f"  Batch {batch_num} - Raw LLM content received (length: {len(content)}).")
                    content = content.strip()
                    start_idx = content.find('[')
                    end_idx = content.rfind(']')

                    if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                        json_str = content[start_idx:end_idx + 1]
                        json_str = json_str.replace("'", '"') # Basic cleaning
                        json_str = re.sub(r",\s*]", "]", json_str) # Handle trailing commas

                        try:
                            parsed_batch_results = json.loads(json_str)
                            if isinstance(parsed_batch_results, list):
                                print(f"  Batch {batch_num} - Successfully parsed {len(parsed_batch_results)} results from LLM response.")
                                # Basic validation and padding (optional, depends on strictness needed)
                                validated_results = []
                                for result in parsed_batch_results:
                                     # Add more specific validation if needed (e.g., check for 'score', 'reason')
                                     if isinstance(result, dict):
                                         validated_results.append(result)
                                     else:
                                         print(f"Warning: Invalid item format in batch {batch_num}: {result}")
                                         validated_results.append({"error": "Invalid item format", "raw": result})
                                # Pad if LLM didn't return enough items
                                while len(validated_results) < len(current_batch_prompts):
                                     print(f"Warning: Padding missing results for batch {batch_num}.")
                                     validated_results.append({"error": "Missing response for this item."})

                                all_results.extend(validated_results[:len(current_batch_prompts)]) # Ensure not too many results added
                                batch_success = True
                                break # Success for this batch
                            else:
                                print(f"  Batch {batch_num} - Attempt {attempt}: Parsed JSON is not a list.")
                                retry_count += 1 # Retry if structure is wrong

                        except json.JSONDecodeError as e:
                            print(f"  Batch {batch_num} - Attempt {attempt}: Batch JSON parsing error - {e}\nContent sample: {json_str[:200]}...")
                            retry_count += 1
                    else:
                        print(f"  Batch {batch_num} - Attempt {attempt}: No valid JSON array '[]' found in response.\nContent sample: {content[:200]}...")
                        retry_count += 1

                # --- Error Handling for Requests ---
                except requests.exceptions.Timeout:
                    print(f"  Batch {batch_num} - Attempt {attempt}: Request timed out.")
                    retry_count += 1
                except requests.exceptions.HTTPError as e:
                    print(f"  Batch {batch_num} - Attempt {attempt}: HTTP Error: {e}")
                    retry_count += 1
                except requests.exceptions.RequestException as e:
                    print(f"  Batch {batch_num} - Attempt {attempt}: Request Exception: {e}")
                    retry_count += 1
                except Exception as e:
                    print(f"  Batch {batch_num} - Attempt {attempt}: Unexpected error: {e}")
                    retry_count += 1

                if retry_count > max_retries:
                    print(f"Error: Batch {batch_num} failed after {max_retries + 1} attempts.")
                    # Add error placeholders for each prompt in the failed batch
                    all_results.extend([{"error": f"LLM query failed for this item after {max_retries + 1} retries."}] * len(current_batch_prompts))
                    break # Move to the next batch

            # Ensure loop terminates if batch fails
            if not batch_success and retry_count > max_retries:
                print(f"Moving to next batch after failure of batch {batch_num}.")

        return all_results # Return the list of dictionaries

    # --- Node Execution Methods ---
    def query_llm_api_batch(self, user_prompts: str, system_prompt: str, api_url: str = DEFAULT_LLM_API_URL, model: str = DEFAULT_LLM_MODEL, temperature: float = 0.3, batch_size: int = DEFAULT_BATCH_SIZE, max_retries: int = DEFAULT_MAX_RETRIES, timeout: int = DEFAULT_TIMEOUT, trigger=None) -> tuple[str]:
        """Node execution function: Parses input, calls static batch query, returns JSON string."""
        prompts_list = self._static_parse_input_prompts(user_prompts)
        if not prompts_list:
            print("Error: No valid prompts found in input.")
            return (json.dumps({"error": "No valid prompts provided."}),)

        print(f"Processing {len(prompts_list)} prompts in batches of {batch_size}...")
        results_list = self._static_query_llm_batch(
            prompts_list=prompts_list,
            system_prompt=system_prompt,
            api_url=api_url,
            model=model,
            temperature=temperature,
            batch_size=batch_size,
            max_retries=max_retries,
            timeout=timeout
        )

        # Return all collected results as a single JSON string array
        try:
             final_json_output = json.dumps(results_list, indent=2) # Pretty print optional
             return (final_json_output,)
        except TypeError as e:
             print(f"Error serializing final batch results: {e}")
             # Attempt to return partial results if serialization fails
             try:
                 partial_json = json.dumps({"error": f"Failed to serialize final results: {e}", "partial_results": results_list[:10]}, default=str)
                 return (partial_json,)
             except: # Final fallback
                 return ('{"error": "Failed to serialize final results and partial results."}',) 

./llm_utils\__init__.py
# ComfyUI-LOKI/llm_utils/__init__.py
# This file can be empty or used for shared constants/initialization later. 

./llm_utils\client.py
import requests
import json
import re
from typing import List, Dict

# --- Constants ---
LLM_API_URL = "http://localhost:12345/v1/chat/completions" # Or your LLM API endpoint
LLM_MODEL = "meta-llama2-3.1-8b-instruct" # Or your desired model
LLM_HEADERS = {"Content-Type": "application/json"}
LLM_TEMPERATURE = 0.3
MAX_RETRIES = 3
BATCH_SIZE = 4 # Default batch size for LLM calls

SYSTEM_PROMPT_FILTER = """You are an expert at analyzing ComfyUI nodes and determining their relevance to specific workflow tasks.
Your job is to evaluate each node's description and determine how useful it would be for a given workflow goal.

Please analyze the node based on:
1. Direct relevance to the workflow goal
2. Utility as a supporting node for the workflow
3. Specific features that would help achieve the goal

Rate the node's applicability from 0-100 where:
0-20: Not relevant
21-40: Marginally relevant
41-60: Moderately useful
61-80: Very useful
81-100: Essential for this workflow

Return only a JSON response in this format:
{
    "score": <0-100>,
    "reason": "<brief 1-2 sentence explanation>"
}"""

SYSTEM_PROMPT_FILTER_BATCH = f"""{SYSTEM_PROMPT_FILTER}

You will be given multiple nodes to analyze in one request.
You must respond with only a valid JSON array containing one JSON object (with 'score' and 'reason') for each node provided in the prompt, in the same order. Format: [{{'score': number, 'reason': 'string'}}, ...]"""


# --- Functions ---

def query_llm(prompt: str, system_prompt: str = SYSTEM_PROMPT_FILTER) -> Dict:
    """Query the LLM API with a single prompt."""
    data = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        "temperature": LLM_TEMPERATURE
    }

    retry_count = 0
    while retry_count < MAX_RETRIES:
        try:
            response = requests.post(LLM_API_URL, headers=LLM_HEADERS, json=data, timeout=60) # Added timeout
            response.raise_for_status()
            result = response.json()
            content = result["choices"][0]["message"]["content"]

            try:
                # Attempt to parse the JSON response
                parsed_result = json.loads(content)
                if isinstance(parsed_result, dict) and "score" in parsed_result and "reason" in parsed_result:
                    return parsed_result
                raise ValueError("Invalid JSON format in LLM response content")
            except (json.JSONDecodeError, ValueError) as parse_error:
                print(f"Attempt {retry_count + 1}: Invalid JSON response: {parse_error}\nContent: {content}")
                retry_count += 1
                continue

        except requests.exceptions.RequestException as e:
            print(f"Attempt {retry_count + 1}: Error querying LLM API: {e}")
            retry_count += 1
            if retry_count == MAX_RETRIES:
                return {"score": 0, "reason": f"Error processing node: API request failed after {MAX_RETRIES} retries."}
            continue
        except Exception as e:
            print(f"Attempt {retry_count + 1}: Unexpected error during LLM query: {e}")
            retry_count += 1
            if retry_count == MAX_RETRIES:
                 return {"score": 0, "reason": f"Error processing node: Unexpected error after {MAX_RETRIES} retries."}
            continue # Reraise unexpected errors or handle differently

    return {"score": 0, "reason": f"Error processing node: Failed after {MAX_RETRIES} retries."}


def query_llm_batch(prompts: List[str], system_prompt: str = SYSTEM_PROMPT_FILTER_BATCH, batch_size: int = BATCH_SIZE) -> List[Dict]:
    """Query the LLM API with a batch of prompts."""
    results = []

    for i in range(0, len(prompts), batch_size):
        batch_prompts_data = prompts[i:i + batch_size]

        # Create a structured prompt for batch processing
        combined_prompt = "Analyze these nodes based on the criteria provided in the system message. Return a JSON array:\n\n"
        combined_prompt += "\n\n".join([
            f"--- Node {idx + 1} ---\n{p}\n--- End Node {idx + 1} ---"
            for idx, p in enumerate(batch_prompts_data)
        ])

        data = {
            "model": LLM_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": combined_prompt}
            ],
            "temperature": LLM_TEMPERATURE
        }

        retry_count = 0
        while retry_count < MAX_RETRIES:
            try:
                response = requests.post(LLM_API_URL, headers=LLM_HEADERS, json=data, timeout=120) # Longer timeout for batch
                response.raise_for_status()
                content = response.json()["choices"][0]["message"]["content"]
                content = content.strip()

                # Attempt to extract the JSON array
                start_idx = content.find('[')
                end_idx = content.rfind(']')

                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = content[start_idx:end_idx + 1]
                    # Basic cleaning for common LLM formatting issues
                    json_str = json_str.replace("'", '"')
                    # Handle potential trailing commas before ']'
                    json_str = re.sub(r",\s*]", "]", json_str)
                    # Handle potential comments if any (simple case)
                    json_str = re.sub(r'//.*?\n', '\n', json_str)

                    try:
                        parsed_batch_results = json.loads(json_str)
                        if isinstance(parsed_batch_results, list):
                             # Validate and pad results
                            validated_results = []
                            for result in parsed_batch_results:
                                if isinstance(result, dict) and "score" in result and "reason" in result:
                                     validated_results.append(result)
                                else:
                                     print(f"Warning: Invalid item in batch response: {result}")
                                     validated_results.append({"score": 0, "reason": "Invalid response format for this item."})

                            # Ensure the number of results matches the number of prompts sent
                            while len(validated_results) < len(batch_prompts_data):
                                validated_results.append({"score": 0, "reason": "Missing response for this item."})

                            results.extend(validated_results[:len(batch_prompts_data)]) # Add validated results for this batch
                            break # Success for this batch, move to next

                    except json.JSONDecodeError as e:
                        print(f"Attempt {retry_count + 1}: Batch JSON parsing error - {e}\nContent: {json_str[:500]}...")
                        retry_count += 1
                else:
                    print(f"Attempt {retry_count + 1}: No valid JSON array found in batch response.\nContent: {content[:500]}...")
                    retry_count += 1

            except requests.exceptions.RequestException as e:
                 print(f"Attempt {retry_count + 1}: Error querying LLM API for batch: {e}")
                 retry_count += 1
                 if retry_count == MAX_RETRIES:
                     results.extend([{"score": 0, "reason": f"Failed to process node: API error after {MAX_RETRIES} retries."} for _ in batch_prompts_data])
                 continue # Continue to next retry or fail the batch
            except Exception as e:
                print(f"Attempt {retry_count + 1}: Unexpected error during batch LLM query: {e}")
                retry_count += 1
                if retry_count == MAX_RETRIES:
                    results.extend([{"score": 0, "reason": f"Failed to process node: Unexpected error after {MAX_RETRIES} retries."} for _ in batch_prompts_data])
                continue

        # If loop finishes without breaking (i.e., all retries failed)
        if retry_count == MAX_RETRIES:
             print(f"Batch failed after {MAX_RETRIES} retries.")
             # Ensure placeholders are added if not already done
             if len(results) < i + len(batch_prompts_data):
                results.extend([{"score": 0, "reason": f"Failed to process node after {MAX_RETRIES} retries."} for _ in range(len(batch_prompts_data) - (len(results) - i))])


    return results 

./twitter_scraper\twitter_scraper.md
# twitter_scraper Node Requirements

## DESCRIPTION

Scrapes data from a specific X.com (Twitter) tweet URL.

## INPUTS

- `tweet_url` (STRING): The URL of the tweet to scrape (e.g., "https://x.com/Scrapfly_dev/status/1664267318053179398").

## OUTPUTS

- `tweet_data` (DICT): A dictionary containing parsed data from the tweet.

## NODE_STRATEGY

This node uses Playwright to launch a headless browser, navigate to the tweet URL, intercept background network requests (XHR) containing tweet data, and parse the relevant information using Jmespath. 

./twitter_scraper\twitter_scraper_name.md
TOKEN="🐦📜ScrapeTweet"
EMOJI_ICON="🐦"
RECIPE="🐦🔗 => 🐦📜"
X2Y_FORMAT="C2D_tweet_url_to_json"
ALIASES=["scrape_twitter_post", "get_tweet_details"]
TAGS=["twitter", "x.com", "scrape", "web", "social media", "tweet"]
METAPHORS=["A digital birdwatcher capturing specific tweet songs", "A news correspondent fetching a specific public announcement from the town square"] 

./twitter_scraper\twitter_scraper.py
import json
import jmespath
from playwright.sync_api import sync_playwright
import argparse
from typing import Dict, List

# Jmespath parsing functions adapted from how_to.txt
# Note: User parsing is included as parse_tweet calls it, but it's not strictly needed
# if only scraping a single tweet's direct data.
def parse_user(data: Dict) -> Dict:
    if data.get("__typename") == "User":
        legacy = data.get("legacy", {})
        result = jmespath.search(
            """{
            created_at: created_at,
            default_profile: default_profile,
            description: description,
            entities: entities,
            fast_followers_count: fast_followers_count,
            favourites_count: favourites_count,
            followers_count: followers_count,
            friends_count: friends_count,
            has_custom_timelines: has_custom_timelines,
            is_translator: is_translator,
            listed_count: listed_count,
            location: location,
            media_count: media_count,
            name: name,
            normal_followers_count: normal_followers_count,
            pinned_tweet_ids_str: pinned_tweet_ids_str,
            possibly_sensitive: possibly_sensitive,
            profile_banner_url: profile_banner_url,
            profile_image_url_https: profile_image_url_https,
            screen_name: screen_name,
            statuses_count: statuses_count,
            translator_type: translator_type,
            verified: verified,
            want_retweets: want_retweets,
            withheld_in_countries: withheld_in_countries
        }""",
            legacy,
        )
        result["id"] = data.get("id")
        result["rest_id"] = data.get("rest_id")
        result["is_blue_verified"] = data.get("is_blue_verified")
    else:
        # Handle cases where user data might be missing or in a different format
        # For now, returning an empty dict or logging a warning might be appropriate
        print(f"Warning: Unexpected user data structure: {data.get('__typename')}")
        result = {}
    return result

def parse_tweet(data: Dict) -> Dict:
    """Parse Twitter tweet JSON dataset for the most important fields"""
    # print(f"DEBUG: Parsing tweet data: {json.dumps(data, indent=2)}") # DEBUG
    legacy = data.get("legacy", {})
    core = data.get("core", {})
    card = data.get("card", {})
    views = data.get("views", {})

    result = jmespath.search(
        """{
        created_at: legacy.created_at,
        attached_urls: legacy.entities.urls[].expanded_url,
        attached_urls2: legacy.entities.url.urls[].expanded_url,
        attached_media: legacy.entities.media[].media_url_https,
        tagged_users: legacy.entities.user_mentions[].screen_name,
        tagged_hashtags: legacy.entities.hashtags[].text,
        favorite_count: legacy.favorite_count,
        bookmark_count: legacy.bookmark_count,
        quote_count: legacy.quote_count,
        reply_count: legacy.reply_count,
        retweet_count: legacy.retweet_count,
        text: legacy.full_text,
        is_quote: legacy.is_quote_status,
        is_retweet: legacy.retweeted,
        language: legacy.lang,
        user_id: legacy.user_id_str,
        id: legacy.id_str,
        conversation_id: legacy.conversation_id_str,
        source: source,
        views: views.count
    }""",
        data, # Note: the original code passed `data`, not `legacy` here. Corrected based on structure.
    )

    # Ensure keys exist even if values are null/missing from jmespath search
    expected_keys = [
        "created_at", "attached_urls", "attached_urls2", "attached_media",
        "tagged_users", "tagged_hashtags", "favorite_count", "bookmark_count",
        "quote_count", "reply_count", "retweet_count", "text", "is_quote",
        "is_retweet", "language", "user_id", "id", "conversation_id", "source", "views"
    ]
    if result is None:
        result = {}
    for key in expected_keys:
        if key not in result:
            result[key] = None # Or appropriate default (e.g., [] for lists, 0 for counts)

    result["poll"] = {}
    poll_data = jmespath.search("card.legacy.binding_values", data) or []
    for poll_entry in poll_data:
        key, value_dict = poll_entry.get("key"), poll_entry.get("value")
        if not key or not value_dict:
            continue

        if "choice" in key and "string_value" in value_dict:
            result["poll"][key] = value_dict["string_value"]
        elif "end_datetime" in key and "string_value" in value_dict:
            result["poll"]["end"] = value_dict["string_value"]
        elif "last_updated_datetime" in key and "string_value" in value_dict:
            result["poll"]["updated"] = value_dict["string_value"]
        elif "counts_are_final" in key and "boolean_value" in value_dict:
            result["poll"]["ended"] = value_dict["boolean_value"]
        elif "duration_minutes" in key and "string_value" in value_dict:
            result["poll"]["duration"] = value_dict["string_value"]

    # Extract user details
    user_results = data.get("core", {}).get("user_results", {}).get("result", {})
    if user_results:
        result["user"] = parse_user(user_results)
    else:
        result["user"] = {}

    # Clean up potentially null list entries from jmespath
    for key in ["attached_urls", "attached_urls2", "attached_media", "tagged_users", "tagged_hashtags"]:
        if result.get(key) is None:
            result[key] = []

    return result

def scrape_tweet_data(url: str, headless: bool = True) -> Dict | None:
    """
    Scrape a single tweet page for Tweet thread details.
    Returns parsed parent tweet data.
    """
    _xhr_calls = []
    tweet_data = None

    def intercept_response(response):
        """capture all background requests and save them"""
        try:
            if response.request.resource_type == "xhr" and "TweetResultByRestId" in response.url:
                # print(f"DEBUG: Intercepted XHR: {response.url}") # DEBUG
                _xhr_calls.append(response)
        except Exception as e:
            print(f"Error intercepting response: {e}")
        return response # Should return response according to Playwright docs

    try:
        with sync_playwright() as pw:
            browser = pw.chromium.launch(headless=headless)
            context = browser.new_context(viewport={"width": 1920, "height": 1080})
            page = context.new_page()

            page.on("response", intercept_response)

            print(f"Navigating to {url}...")
            page.goto(url, wait_until="domcontentloaded", timeout=60000) # Increased timeout

            print("Waiting for tweet selector...")
            # Wait for the main tweet content area or a known tweet element
            try:
                 page.wait_for_selector("div[data-testid='primaryColumn']", timeout=30000)
                 page.wait_for_timeout(5000) # Wait for XHR calls
                 print("Tweet container likely loaded. Processing XHR calls.")
            except Exception as e:
                 print(f"Timeout or error waiting for tweet selector: {e}. Proceeding with captured XHR calls if any.")

            # --- Process XHR calls INSIDE the context manager --- START
            if not _xhr_calls:
                print("No relevant XHR calls (TweetResultByRestId) were intercepted.")
            else:
                # Find the main tweet data
                for xhr in reversed(_xhr_calls):
                    try:
                        data = xhr.json()
                        if 'data' in data and 'tweetResult' in data['data'] and 'result' in data['data']['tweetResult']:
                             tweet_payload = data['data']['tweetResult']['result']
                             if tweet_payload.get("__typename") == "Tweet":
                                 print(f"Found main tweet data in {xhr.url}")
                                 tweet_data = tweet_payload
                                 break
                        elif 'data' in data and 'threaded_conversation_with_injections_v2' in data['data']:
                             instructions = data['data']['threaded_conversation_with_injections_v2'].get('instructions', [])
                             for instruction in instructions:
                                  if instruction.get('type') == 'TimelineAddEntries':
                                       entries = instruction.get('entries', [])
                                       for entry in entries:
                                            content = entry.get('content')
                                            if content and content.get('entryType') == 'TimelineTimelineItem':
                                                 item_content = content.get('itemContent')
                                                 if item_content and item_content.get('itemType') == 'TimelineTweet':
                                                      tweet_results = item_content.get('tweet_results', {})
                                                      result = tweet_results.get('result')
                                                      if result and result.get('__typename') == 'Tweet':
                                                          legacy_data = result.get('legacy')
                                                          tweet_id_from_url = url.split('/')[-1].split('?')[0]
                                                          if legacy_data and legacy_data.get('id_str') == tweet_id_from_url:
                                                              print(f"Found main tweet data via TimelineAddEntries in {xhr.url}")
                                                              tweet_data = result
                                                              break
                                       if tweet_data: break
                             if tweet_data: break

                    except Exception as e:
                        # Catch JSON decode errors or other issues for specific XHR
                        print(f"Error processing XHR call from {xhr.url}: {e}")
            # --- Process XHR calls INSIDE the context manager --- END

            browser.close()

    except Exception as e:
        print(f"Playwright error during navigation or setup: {e}")
        # Ensure browser is closed if an error occurs mid-process
        try:
            # Check if browser variable exists and has a close method
            if 'browser' in locals() and hasattr(browser, 'close'):
                browser.close()
        except Exception as close_err:
             print(f"Error trying to close browser after main error: {close_err}")
        return None # Indicate failure

    if tweet_data:
        print("Parsing extracted tweet data...")
        return parse_tweet(tweet_data)
    else:
        print("Could not find valid tweet data in any intercepted XHR call after browser context.")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Scrape data from a single X.com (Twitter) tweet URL.')
    parser.add_argument('url', type=str, help='The URL of the tweet to scrape.')
    parser.add_argument('--output', type=str, default='tweet_output.json', help='File path to save the output JSON.')
    parser.add_argument('--visible', action='store_true', help='Run the browser in visible mode (not headless).')
    args = parser.parse_args()

    print(f"Attempting to scrape: {args.url}")
    scraped_data = scrape_tweet_data(args.url, headless=not args.visible)

    if scraped_data:
        print(f"Successfully scraped and parsed data. Saving to {args.output}")
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(scraped_data, f, ensure_ascii=False, indent=4)
            print("Output saved.")
        except Exception as e:
            print(f"Error saving output to {args.output}: {e}")
    else:
        print("Scraping failed.") 

./twitter_scraper\__init__.py
import subprocess
import sys
import os
from .twitter_scraper_node import TwitterScraper

did_playwright_install_check = False

def ensure_playwright_chromium():
    global did_playwright_install_check
    if did_playwright_install_check:
        return
    try:
        # Check if playwright is installed first before trying to run its command
        import playwright
        print("--- [Twitter Scraper] Ensuring Playwright Chromium is installed ---")
        subprocess.run([sys.executable, "-m", "playwright", "install", "chromium"], check=True, capture_output=True)
        print("--- [Twitter Scraper] Playwright Chromium check complete ---")
    except ImportError:
        # This should ideally not happen if root requirements are processed first,
        # but good to have a check.
        print("\033[91mERROR: Playwright module not found in twitter_scraper init. Cannot check/install browser.[0m")
    except FileNotFoundError:
         print("\033[91mERROR: Could not execute playwright command (playwright not found in PATH?). Cannot check/install browser.[0m")
    except subprocess.CalledProcessError as e:
        # Log the error, but don't necessarily stop ComfyUI loading
        # Use triple quotes for the f-string to allow the newline from stderr
        print(f"""[93mWARNING: Playwright install command failed (might be okay if browser already exists):
{e.stderr.decode()} [0m""")
    except Exception as e:
         print(f"\033[91mERROR: An unexpected error occurred during Playwright install check: {e} [0m")
    finally:
         did_playwright_install_check = True

# Run the check when this sub-module is loaded
ensure_playwright_chromium()

# Placeholder for ComfyUI node registration
NODE_CLASS_MAPPINGS = {
    "TwitterScraper": TwitterScraper
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "TwitterScraper": "🐦📜 Scrape Tweet (LOKI)"
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']

print("\033[34mLOKI Nodes: \033[93mLoaded Twitter Scraper node logic\033[0m") # Optional: Add a print statement 

./twitter_scraper\requirements.txt
playwright>=1.30.0 # Or a recent version
jmespath>=1.0.0 

./twitter_scraper\twitter_scraper_node.py
import json
from .twitter_scraper import scrape_tweet_data # Import from the same directory

class TwitterScraper:
    """ 
    LOKI Node: Scrapes data from a specific X.com (Twitter) tweet URL using Playwright.
    Input: tweet_url (STRING)
    Output: tweet_data (STRING - JSON formatted)
    """
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "tweet_url": ("STRING", {"multiline": False, "default": "https://x.com/Scrapfly_dev/status/1664267318053179398"}),
            },
            "optional": {
                 "run_headless": ("BOOLEAN", {"default": True}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("tweet_data_json",)
    FUNCTION = "scrape_tweet"
    CATEGORY = "LOKI 🦊/Web"
    OUTPUT_NODE = True # Useful for nodes that primarily produce final output data

    def scrape_tweet(self, tweet_url: str, run_headless: bool = True):
        print(f"[LokiTweetScraper] Attempting to scrape: {tweet_url}")
        scraped_data_dict = scrape_tweet_data(tweet_url, headless=run_headless)

        # --- ADDED DEBUGGING --- 
        print(f"[LokiTweetScraper] Raw data returned from scrape_tweet_data: {type(scraped_data_dict)}")
        # Avoid printing potentially huge dicts directly, check if it's None or empty first
        if isinstance(scraped_data_dict, dict):
            print(f"[LokiTweetScraper] Keys in returned data: {list(scraped_data_dict.keys())}")
        else:
            print(f"[LokiTweetScraper] Scraped data is not a dictionary (or is None).")
        # --- END DEBUGGING ---

        if scraped_data_dict:
            print(f"[LokiTweetScraper] Successfully scraped and parsed data. Attempting JSON conversion.")
            # ComfyUI primarily works with basic types; return JSON string
            try:
                json_output = json.dumps(scraped_data_dict, indent=4)
                print(f"[LokiTweetScraper] JSON conversion successful. Output length: {len(json_output)}") # DEBUG
                return (json_output,)
            except Exception as e:
                print(f"[LokiTweetScraper] Error converting scraped data to JSON: {e}")
                # Return an empty JSON object string or similar indicator of error
                return ("{}",)
        else:
            print("[LokiTweetScraper] Scraping failed (scrape_tweet_data returned None or empty).")
            # Return an empty JSON object string or similar indicator of failure
            return ("{}",) 

# Dictionary that ComfyUI uses to map node names to classes
# NODE_CLASS_MAPPINGS = {
#     "LokiTweetScraper": LokiTweetScraper
# }

# # A dictionary that contains the friendly/humanly readable titles for the nodes
# NODE_DISPLAY_NAME_MAPPINGS = {
#     "LokiTweetScraper": "🐦📜 Scrape Tweet (LOKI)"
# } 

./twitter_scraper\tweet_output.json
{
    "created_at": "Sun May 04 20:50:10 +0000 2025",
    "attached_urls": [],
    "attached_urls2": [],
    "attached_media": [
        "https://pbs.twimg.com/tweet_video_thumb/GqIgy_oW0AAKs9g.jpg"
    ],
    "tagged_users": [],
    "tagged_hashtags": [],
    "favorite_count": 22,
    "bookmark_count": 11,
    "quote_count": 2,
    "reply_count": 5,
    "retweet_count": 5,
    "text": "(1/2) The Fed’s Strategic Pause | Why Markets May Have Misread the Post-Tariff Terrain\n\nOver the last 72 hours, markets were dealt a sharp reality check. Following weeks of dovish positioning and escalating expectations of a near-term Fed rate cut, the release of April’s ISM and https://t.co/bxoW48hiDs",
    "is_quote": true,
    "is_retweet": false,
    "language": "en",
    "user_id": "1448432122881101826",
    "id": "1919132465605607788",
    "conversation_id": "1919132465605607788",
    "source": "<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>",
    "views": "5846",
    "poll": {},
    "user": {
        "created_at": "Wed Oct 13 23:35:47 +0000 2021",
        "default_profile": true,
        "description": "Macro strategy | Systemic risk & policy intel | Powered by AI + human insight | Featured by RealClearPolitics & https://t.co/d5MEzDJBQE",
        "entities": {
            "description": {
                "urls": [
                    {
                        "display_url": "InvestX.fr",
                        "expanded_url": "http://InvestX.fr",
                        "url": "https://t.co/d5MEzDJBQE",
                        "indices": [
                            112,
                            135
                        ]
                    }
                ]
            }
        },
        "fast_followers_count": 0,
        "favourites_count": 13711,
        "followers_count": 26882,
        "friends_count": 689,
        "has_custom_timelines": true,
        "is_translator": false,
        "listed_count": 585,
        "location": "",
        "media_count": 824,
        "name": "EndGame Macro",
        "normal_followers_count": 26882,
        "pinned_tweet_ids_str": [
            "1912562154848346353"
        ],
        "possibly_sensitive": false,
        "profile_banner_url": null,
        "profile_image_url_https": "https://pbs.twimg.com/profile_images/1554632419592003584/lRSfOnxJ_normal.jpg",
        "screen_name": "onechancefreedm",
        "statuses_count": 3871,
        "translator_type": "none",
        "verified": false,
        "want_retweets": null,
        "withheld_in_countries": [],
        "id": "VXNlcjoxNDQ4NDMyMTIyODgxMTAxODI2",
        "rest_id": "1448432122881101826",
        "is_blue_verified": true
    }
}

./twitter_scraper\context\how_to.txt
https://scrapfly.io/blog/how-to-scrape-twitter/

The Best Web Scraping API
Products 
Pricing
Why Us?
Docs
Blog
Knowledgebase
Login
How to Scrape X.com (Twitter) using Python (2025 Update)
by Bernardas Ališauskas
Jan 21, 2025
scrapeguide
Python
How to Scrape X.com (Twitter) using Python (2024 Update)
As Twitter.com became X.com it closed its public API though web scraping is here to the rescue!

In this X.com web scraping tutorial, we'll take a look at scraping X.com posts and profiles using Python and Playwright.

We'll be using Python to retrieve X.com data such as:

X.com post (tweet) information.
X.com user profile information.
Unfortunately, the rest of the data points are not possible to scrape without login however we'll mention some potential workarounds and suggestions.

So, we'll be scraping X.com without login or any complex tricks using headless browsers and capturing background requests making this a very simple and powerful scraper.

For our headless browser environment, we'll be using Scrapfly SDK with Javascript Rendering feature. Alternatively, for non-scrapfly users, we'll also show how to achieve similar results using Playwright.

Latest X.com Scraper Code
Why Scrape X.com?
Project Setup
How Do X.com Pages Work?
Scraping Tweets (Posts)
Parsing Tweet Dataset
Scraping X.com User Profiles
Bypass X.com Blocking with ScrapFly
Scraping X.com Search, Replies and Timelines
FAQ
X.com Scraping Summary
Get monthly web scraping insights 👆

Learn at ScrapFly Academy
Latest X.com Scraper Code
https://github.com/scrapfly/scrapfly-scrapers/
Legal Disclaimer and Precautions
This tutorial covers popular web scraping techniques for education. Interacting with public servers requires diligence and respect and here's a good summary of what not to do:

Do not scrape at rates that could damage the website.
Do not scrape data that's not available publicly.
Do not store PII of EU citizens who are protected by GDPR.
Do not repurpose the entire public datasets which can be illegal in some countries.
Scrapfly does not offer legal advice but these are good general rules to follow in web scraping
and for more you should consult a lawyer.

Why Scrape X.com?
X.com (formerly Twitter.com) is a major announcement hub where people and companies publish their news. This is a great opportunity to use X.com to follow and understand industry trends. For example, stock market or crypto market targets could be scraped to predict the future price of a stock or crypto.

X is also a great source of data for sentiment analysis. You can use X.com to find out what people think about a certain topic or brand. This is useful for market research, product development, and brand awareness.

So, if we can scrape X.com data with Python we can have access to this valuable public information for free!

Project Setup
In this tutorial, we'll cover X/Twitter scraping using Python and scrapfly-sdk or Playwright.

To parse the scraped X.com datasets we'll be using Jmespath JSON parsing library which allows to parse and reshape JSON data.

All of these libraries are available for free and can be installed via pip install terminal command:

$ pip install playwright jmespath scrapfly-sdk
Web Scraping with Playwright and Python
For an introduction to web scraping with Playwright see this beginner's guide which covers common functionality and an example project.

Web Scraping with Playwright and Python
How Do X.com Pages Work?
Before we start scraping, let's take a quick look at how X.com website works through a bit of basic reverse engineering. This will help us to develop our Twitter scraper.

To start, X.com is a javascript web application that uses a lot of background requests (XHR) to display the page data. In short, it works by loading the initial HTML, then starting a JS app and then using XHR requests loads the tweet data:

Twitter page load process
Twitter page load process
So, scraping it without a headless browser such as Playwright or Scrapfly SDK would be very difficult as we'd have to reverse engineer the entire X.com API and application process.

To add, X.com page HTML is dynamic and complex making parsing scraped content very difficult. So, the best approach to scrape Twitter is to use a headless browser and capture background requests that download the Tweet and user data.

To summarize, our best bet is to:

Start a headless web browser.
Enable background request capture.
Load X.com page.
Select captured background requests that contain post or profile data.
For example, if we take a look at a Twitter profile page in Browser Developer Tools we can see the requests Twitter performs in the background to load the page data:



0:00
/0:18

1×


X.com (Twitter) backend making a background request to retrieve data
Next, let's start by scraping X.com posts (tweets).

Scraping Tweets (Posts)
To scrape individual X.com post pages we'll be loading the page using a headless browser and capturing the background requests that retrieve tweet details. This request can be identified by TweetResultByRestId in the URL.

This background request returns a JSON response that contains post and author information.

So, to scrape this using Python we can either use Playwright or Scrapfly SDK:

Python ScrapFly
from playwright.sync_api import sync_playwright


def scrape_tweet(url: str) -> dict:
    """
    Scrape a single tweet page for Tweet thread e.g.:
    https://twitter.com/Scrapfly_dev/status/1667013143904567296
    Return parent tweet, reply tweets and recommended tweets
    """
    _xhr_calls = []

    def intercept_response(response):
        """capture all background requests and save them"""
        # we can extract details from background requests
        if response.request.resource_type == "xhr":
            _xhr_calls.append(response)
        return response

    with sync_playwright() as pw:
        browser = pw.chromium.launch(headless=False)
        context = browser.new_context(viewport={"width": 1920, "height": 1080})
        page = context.new_page()

        # enable background request intercepting:
        page.on("response", intercept_response)
        # go to url and wait for the page to load
        page.goto(url)
        page.wait_for_selector("[data-testid='tweet']")

        # find all tweet background requests:
        tweet_calls = [f for f in _xhr_calls if "TweetResultByRestId" in f.url]
        for xhr in tweet_calls:
            data = xhr.json()
            return data['data']['tweetResult']['result']



if __name__ == "__main__":
    print(scrape_tweet("https://twitter.com/Scrapfly_dev/status/1664267318053179398"))
Example Output
Here, we loaded the Tweet page using a headless browser and captured all of the background requests. Then, we filtered out the ones that contained the Tweet data.

One important note here is that we need to wait for the page to load which is indicated by tweets appearing on the page HTML otherwise we'll return our scrape before the background requests have finished.

This resulted in a massive JSON dataset that can be difficult to work with. So, let's take a look at how to reduce it with a bit of JSON parsing next.

Parsing Tweet Dataset
The Tweet dataset we scraped contains a lot of complex data so let's reduce it to something more clean and simple using Jmespath JSON parsing library.

For this, we'll be using jmespath's JSON reshaping feature which allows us to rename keys and flatten nested objects:

from typing import Dict

def parse_tweet(data: Dict) -> Dict:
    """Parse Twitter tweet JSON dataset for the most important fields"""
    result = jmespath.search(
        """{
        created_at: legacy.created_at,
        attached_urls: legacy.entities.urls[].expanded_url,
        attached_urls2: legacy.entities.url.urls[].expanded_url,
        attached_media: legacy.entities.media[].media_url_https,
        tagged_users: legacy.entities.user_mentions[].screen_name,
        tagged_hashtags: legacy.entities.hashtags[].text,
        favorite_count: legacy.favorite_count,
        bookmark_count: legacy.bookmark_count,
        quote_count: legacy.quote_count,
        reply_count: legacy.reply_count,
        retweet_count: legacy.retweet_count,
        quote_count: legacy.quote_count,
        text: legacy.full_text,
        is_quote: legacy.is_quote_status,
        is_retweet: legacy.retweeted,
        language: legacy.lang,
        user_id: legacy.user_id_str,
        id: legacy.id_str,
        conversation_id: legacy.conversation_id_str,
        source: source,
        views: views.count
    }""",
        data,
    )
    result["poll"] = {}
    poll_data = jmespath.search("card.legacy.binding_values", data) or []
    for poll_entry in poll_data:
        key, value = poll_entry["key"], poll_entry["value"]
        if "choice" in key:
            result["poll"][key] = value["string_value"]
        elif "end_datetime" in key:
            result["poll"]["end"] = value["string_value"]
        elif "last_updated_datetime" in key:
            result["poll"]["updated"] = value["string_value"]
        elif "counts_are_final" in key:
            result["poll"]["ended"] = value["boolean_value"]
        elif "duration_minutes" in key:
            result["poll"]["duration"] = value["string_value"]
    user_data = jmespath.search("core.user_results.result", data)
    if user_data:
        result["user"] = parse_user(user_data)
    return result
Above we're using jmespath to reshape the giant, nested dataset we scraped from Twitter's graphql backend into a flat dictionary containing only the most important fields.

Scraping X.com User Profiles
To scrape X.com profile pages we'll be using the same background request capturing approach though this time we'll be capturing UserBy endpoints.

We'll be using the same technique we used to scrape X posts - launch a headless browser, enable background request capture, load the page and get the data requests:

Python ScrapFly
import asyncio
from playwright.sync_api import sync_playwright


def scrape_profile(url: str) -> dict:
    """
    Scrape a X.com profile details e.g.: https://x.com/Scrapfly_dev
    """
    _xhr_calls = []

    def intercept_response(response):
        """capture all background requests and save them"""
        # we can extract details from background requests
        if response.request.resource_type == "xhr":
            _xhr_calls.append(response)
        return response

    with sync_playwright() as pw:
        browser = pw.chromium.launch(headless=False)
        context = browser.new_context(viewport={"width": 1920, "height": 1080})
        page = context.new_page()

        # enable background request intercepting:
        page.on("response", intercept_response)
        # go to url and wait for the page to load
        page.goto(url)
        page.wait_for_selector("[data-testid='primaryColumn']")

        # find all tweet background requests:
        tweet_calls = [f for f in _xhr_calls if "UserBy" in f.url]
        for xhr in tweet_calls:
            data = xhr.json()
            return data['data']['user']['result']



if __name__ == "__main__":
    print(asyncio.run(scrape_profile("https://x.com/Scrapfly_dev")))
Example Output
Bypass X.com Blocking with ScrapFly
If we start scraping X.com at scale we start to quickly run into blocking as X.com doesn't allow automated requests and will block scraper IP addresses after a few scraping requests.

To get around this Scrapfly can lend a hand!

scrapfly middleware
ScrapFly provides web scraping, screenshot, and extraction APIs for data collection at scale.

Anti-bot protection bypass - scrape web pages without blocking!
Rotating residential proxies - prevent IP address and geographic blocks.
JavaScript rendering - scrape dynamic web pages through cloud browsers.
Full browser automation - control browsers to scroll, input and click on objects.
Format conversion - scrape as HTML, JSON, Text, or Markdown.
Python and Typescript SDKs, as well as Scrapy and no-code tool integrations.
For example, to use ScrapFly with Python we can take advantage of Python SDK:

from scrapfly import ScrapflyClient, ScrapeConfig
scrapfly = ScrapflyClient(key="YOUR SCRAPFLY KEY")

result = scrapfly.scrape(ScrapeConfig(
    "https://twitter.com/Scrapfly_dev",
    # we can enable features like:
    # cloud headless browser use
    render_js=True,  
    # anti scraping protection bypass
    asp=True, 
    # screenshot taking
    screenshots={"all": "fullpage"},
    # proxy country selection
    country="US",
))
For more on using ScrapFly to scrape Twitter see the Full Scraper Code section.

Scraping X.com Search, Replies and Timelines
In this tutorial, we've covered how to scrape X.com posts and profiles that are publicly available for everyone.

The other areas like search and timelines pages are not publicly available and require a login to access which can lead to account suspension.

X.com does offer a public guest preview access for timelines and tweet search but only for Android devices. This is the only way to scrape X.com timelines, tweet replies and search without login.

The most reliable up-to-date source for this is Nitter.net which is an alternative, open source Twitter front-end.
For more on that, we recommend following Nitter Guest Account Branch on Github.

FAQ
To wrap up this Python Twitter scraper let's take a look at some frequently asked questions regarding web scraping Twitter:

Is it legal to scrape X.com?
Yes, all of the data on X.com is available publically so it's perfectly legal to scrape. However, note that some tweets can contain copyrighted material like images or videos and using this data commercially can be illegal.

How to scrape X.com without getting blocked?
X.com is a complex javascript-heavy website and is hostile to web scraping so it's easy to get blocked. To avoid this you can use ScrapFly which provides anti-scraping technology bypass and proxy rotation. Alternatively, see our article on how to avoid web scraper blocking.

Is it legal to scrape X.com while logged in?
The legality of scraping X.com while logged in is a bit of a grey area. Generally, logging in legally binds the user to the website's terms of service which in Twitter's case forbids automated scraping. This allows X to suspend your account or even take legal action. It's best to avoid web scraping X.com while logged in if possible.

How to reduce bandwidth use and speed up X scraping?
If you're using browser automation tools like Playwright (used in this article) then you can block images and unnecessary resources to save bandwidth and speed up scraping.

Latest X.com Scraper Code
https://github.com/scrapfly/scrapfly-scrapers/
X.com Scraping Summary
In this tutorial, we made a short Twitter scraper (now known as X.com) using Python headless browsers through Playwright or Scrapfly SDK.

To start, we've taken a look at how X.com works and identified where the data is located. We found that X is using background requests to populate post and profile data.

To capture and scrape these background requests we used the intercept function of Playwright or Scrapfly-SDK and parsed the raw datasets to nice clean data JSONs using jmespath.

Finally, to avoid blocking we've taken a look at ScrapFly web scraping API which provides a simple way to scrape Twitter at scale using proxies and anti scraping technology bypassing. Try out ScrapFly for free!

Check out ScrapFly Python SDK
Try ScrapFly for FREE!
scrapeguide Python
Related Posts
How to Scrape Reddit Posts, Subreddits and Profiles
Apr 22, 2024
How to Scrape Reddit Posts, Subreddits and Profiles
In this article, we'll explore how to scrape Reddit. We'll extract various social data types from subreddits, posts, and user pages. All of which through plain HTTP requests without headless browser usage.

Python scrapeguide
How to Scrape LinkedIn in 2025
Apr 05, 2024
How to Scrape LinkedIn in 2025
In this scrape guide we'll be taking a look at one of the most popular web scraping targets - LinkedIn.com. We'll be scraping people profiles, company profiles as well as job listings and search.

Python scrapeguide
How to Scrape SimilarWeb Website Traffic Analytics
Mar 25, 2024
How to Scrape SimilarWeb Website Traffic Analytics
In this guide, we'll explain how to scrape SimilarWeb through a step-by-step guide. We'll scrape comprehensive website traffic insights, websites comparing data, sitemaps, and trending industry domains.

scrapeguide SEO search-engine Python
Company
Careers
Terms of service
Privacy Policy
Data Processing Agreement
KYC Compliance
Status
Integrations
Zapier
Make
N8n
LlamaIndex
LangChain
Social
Tools
Convert cURL commands to Python code
JA3/TLS Fingerprint
HTTP2 Fingerprint
Xpath/CSS Selector Tester
Resources
API Documentation
Web Scraping Academy
Is Web Scraping Legal?
Web Scraping Tools
FAQ
Learn Web Scraping
Web Scraping with Python
Web Scraping with PHP
Web Scraping with Ruby
Web Scraping with R
Web Scraping with NodeJS
Web Scraping with Python Scrapy
How to Scrape without getting blocked tutorial
Web Scraping with Python and BeautifulSoup
Web Scraping with Nodejs and Puppeteer
How To Scrape Graphql
Best Proxies for Web Scraping
Top 5 Best Residential Proxies
Usage
What is Web Scraping used for?
Web Scraping for AI Training
Web Scraping for Compliance
Web Scraping for eCommerce
Web Scraping for Finance
Web Scraping for Fraud Detection
Web Scraping for Jobs
Web Scraping for Lead Generation
Web Scraping for News & Media
Web Scraping for Real Estate
Web Scraping for SERP & SEO
Web Scraping for Social Media
Web Scraping for Travel
© 2025 Scrapfly - The Best Web Scraping API For Developers

./twitter_user_scraper\twitter_user_scraper.md
# twitter_user_scraper Node Requirements

## DESCRIPTION

Scrapes public profile data AND the most recent ~150 tweets for a specific X.com (Twitter) user using their profile URL. Operates without requiring login.

## INPUTS

- `profile_url` (STRING): The full URL of the target user's profile page (e.g., "https://x.com/someuser").
- `max_tweets` (INT): The maximum number of tweets to attempt to retrieve (default: 50). Note that the actual number returned is often limited by Twitter (~150 for public scraping).
- `run_visible` (BOOLEAN): If true, runs the Playwright browser in non-headless (visible) mode, which can sometimes help bypass anti-bot measures (default: false).

## OUTPUTS

- `user_data` (DICT): A dictionary containing:
    - `profile`: (Dict) Parsed public profile data (name, bio, counts, etc.) or an empty dict if not found.
    - `tweets`: (List[Dict]) A list of dictionaries, each representing a parsed tweet, sorted newest first, up to `max_tweets` or the public limit.

## NODE_STRATEGY

1.  Launches a Playwright browser instance (headless or visible).
2.  Navigates to the provided `profile_url`.
3.  Intercepts background network requests (XHR) specifically listening for:
    - Profile data endpoints (`UserByScreenName`, `UserByRestId`).
    - Tweet data endpoints (`UserTweets`).
4.  Waits for the main profile content to load.
5.  Attempts to scroll down the page (`window.scrollTo(0, document.body.scrollHeight)`) up to 10 times, waiting briefly after each scroll.
6.  Processes intercepted XHR responses after each scroll attempt, parsing profile data (once) and newly found tweets using Jmespath and helper functions (`parse_user`, `parse_tweet`, `find_objects_by_typename`).
7.  Stops scrolling early if the `max_tweets` limit is reached OR if 2 consecutive scroll attempts yield no new tweets.
8.  Performs final processing of any remaining intercepted XHR calls.
9.  Closes the browser.
10. Sorts the collected tweets by `created_at` (descending).
11. Trims the tweet list to `max_tweets`.
12. Returns the combined `profile` and `tweets` data in a dictionary.

## LIMITATIONS

- **No Full Timeline/Replies:** Cannot scrape the user's full tweet history or replies without login.
- **Public Data Only:** Limited to information visible to non-logged-in users.
- **Tweet Limit:** Non-logged-in scraping seems strictly limited to retrieving only the ~150 most recent tweets visible on the profile page, regardless of scrolling attempts beyond the initial load.
- **Website Changes:** Highly dependent on current X.com website structure and private API endpoints, which can break the scraper without notice.
- **Blocking/Rate Limiting:** Susceptible to Cloudflare challenges and rate limiting, especially with frequent/automated use. Running non-headless (`run_visible=True`) may improve reliability sometimes.

## Usage (Standalone Script)

```bash
python nodes/twitter_user_scraper/twitter_user_scraper.py <profile_url> [--max_tweets N] [--output file.json] [--visible]
```

*   `<profile_url>`: Full URL of the Twitter profile.
*   `--max_tweets N` (optional): Max tweets to return (default: 20 for script, 50 for node). Actual number may be lower.
*   `--output file.json` (optional): File to save JSON output (default: `user_profile_output.json`).
*   `--visible` (optional): Runs the browser visibly (non-headless). 

./twitter_user_scraper\twitter_user_scraper_name.md
# Scrape Twitter User Profile & Tweets

## TOKEN

`🐦👤UserProfile`

## EMOJI_ICON

`👤`

## RECIPE

`🐦🔗 => 🐦👤`

## X2Y_FORMAT

`C2D+_twitter_user_url_to_json`

## ALIASES

- Twitter Profile Scraper
- X User Scraper
- Twitter User Info
- Get User Tweets

## TAGS

- twitter
- x.com
- scrape
- user
- profile
- tweets
- social media
- playwright
- web scraping
- public data

## METAPHORS

- **Digital Detective's Dossier:** Gathers public information and recent activity for a specific online persona.
- **Social Media Snapshot:** Captures a user's profile details and their latest posts like a quick snapshot.
- **Online Persona Card:** Presents a user's key info and recent updates, similar to a digital business card. 

./twitter_user_scraper\twitter_user_scraper.py
import json
import jmespath
from playwright.sync_api import sync_playwright
import argparse
from typing import Dict, Optional, List, Tuple, Any

# --- Reusing parsing function from twitter_scraper --- 
# (Ideally, this would be in a shared utility file)
def parse_user(data: Dict) -> Dict:
    """Parses the user data from the GraphQL response."""
    if data.get("__typename") == "User":
        legacy = data.get("legacy", {})
        result = jmespath.search(
            """{
            created_at: created_at, default_profile: default_profile, description: description,
            entities: entities, fast_followers_count: fast_followers_count, favourites_count: favourites_count,
            followers_count: followers_count, friends_count: friends_count, has_custom_timelines: has_custom_timelines,
            is_translator: is_translator, listed_count: listed_count, location: location,
            media_count: media_count, name: name, normal_followers_count: normal_followers_count,
            pinned_tweet_ids_str: pinned_tweet_ids_str, possibly_sensitive: possibly_sensitive,
            profile_banner_url: profile_banner_url, profile_image_url_https: profile_image_url_https,
            screen_name: screen_name, statuses_count: statuses_count, translator_type: translator_type,
            verified: verified, want_retweets: want_retweets, withheld_in_countries: withheld_in_countries
        }""",
            legacy,
        )
        result = result if isinstance(result, dict) else {}
        result["id"] = data.get("id") # GraphQL ID
        result["rest_id"] = data.get("rest_id") # Numeric ID
        result["is_blue_verified"] = data.get("is_blue_verified")
        # Add potentially missing professional details
        professional = data.get("professional", {})
        result["professional_type"] = professional.get("professional_type")
        result["category"] = professional.get("category", []) 
        
    elif data.get("__typename") == "UserUnavailable":
         print("User unavailable (suspended, deleted, etc.)")
         result = {"error": "UserUnavailable", "message": data.get("reason")}
    else:
        print(f"Warning: Unexpected user data type: {data.get('__typename')}")
        result = {"error": "UnknownDataType", "type": data.get('__typename')}
    return result

def parse_tweet(data: Dict) -> Dict:
    """Parse Twitter tweet JSON dataset for the most important fields"""
    legacy = data.get("legacy", {})
    core = data.get("core", {})
    card = data.get("card", {})
    views = data.get("views", {})

    result = jmespath.search(
        """{
        created_at: legacy.created_at,
        attached_urls: legacy.entities.urls[].expanded_url,
        attached_urls2: legacy.entities.url.urls[].expanded_url,
        attached_media: legacy.entities.media[].media_url_https,
        tagged_users: legacy.entities.user_mentions[].screen_name,
        tagged_hashtags: legacy.entities.hashtags[].text,
        favorite_count: legacy.favorite_count,
        bookmark_count: legacy.bookmark_count,
        quote_count: legacy.quote_count,
        reply_count: legacy.reply_count,
        retweet_count: legacy.retweet_count,
        text: note_tweet.note_tweet_results.result.text || legacy.full_text,
        is_quote: legacy.is_quote_status,
        is_retweet: legacy.retweeted,
        language: legacy.lang,
        user_id: legacy.user_id_str,
        id: legacy.id_str,
        conversation_id: legacy.conversation_id_str,
        source: source,
        views: views.count
    }""",
        data,
    )

    expected_keys = [
        "created_at", "attached_urls", "attached_urls2", "attached_media",
        "tagged_users", "tagged_hashtags", "favorite_count", "bookmark_count",
        "quote_count", "reply_count", "retweet_count", "text", "is_quote",
        "is_retweet", "language", "user_id", "id", "conversation_id", "source", "views"
    ]
    if result is None: result = {}
    for key in expected_keys:
        if key not in result:
            result[key] = None # Default for missing values

    result["poll"] = {}
    poll_data = jmespath.search("card.legacy.binding_values", data) or []
    for poll_entry in poll_data:
        key, value_dict = poll_entry.get("key"), poll_entry.get("value")
        if not key or not value_dict: continue
        if "choice" in key and "string_value" in value_dict: result["poll"][key] = value_dict["string_value"]
        elif "end_datetime" in key and "string_value" in value_dict: result["poll"]["end"] = value_dict["string_value"]
        elif "last_updated_datetime" in key and "string_value" in value_dict: result["poll"]["updated"] = value_dict["string_value"]
        elif "counts_are_final" in key and "boolean_value" in value_dict: result["poll"]["ended"] = value_dict["boolean_value"]
        elif "duration_minutes" in key and "string_value" in value_dict: result["poll"]["duration"] = value_dict["string_value"]

    # Use the existing parse_user, assuming it might be nested within tweet data sometimes
    user_results = core.get("user_results", {}).get("result", {})
    result["user"] = parse_user(user_results) if user_results else {}

    for key in ["attached_urls", "attached_urls2", "attached_media", "tagged_users", "tagged_hashtags"]:
        if result.get(key) is None: result[key] = [] # Ensure lists are lists

    return result
# --- End of reused parsing functions ---

# --- NEW: Helper function for recursive search --- 
def find_objects_by_typename(data: Any, target_typename: str) -> List[Dict]:
    """Recursively search dicts/lists for objects with a specific __typename."""
    found_objects = []
    if isinstance(data, dict):
        if data.get("__typename") == target_typename:
            # Basic check to see if it looks like a tweet payload
            if target_typename == "Tweet" and (data.get("legacy") or data.get("core")):
                 found_objects.append(data)
            # Add checks for other typenames if needed later
            # elif target_typename == "SomeOtherType" and ...:
            #     found_objects.append(data)
            
        for key, value in data.items():
            found_objects.extend(find_objects_by_typename(value, target_typename))
            
    elif isinstance(data, list):
        for item in data:
            found_objects.extend(find_objects_by_typename(item, target_typename))
            
    return found_objects
# --- END Helper function ---

def scrape_user_profile_and_tweets(profile_url: str, max_tweets: int = 20, headless: bool = True) -> Optional[Dict]:
    """
    Scrapes public user profile data AND tweets from a user's profile page,
    simulating scrolling to potentially capture more than the initial batch.
    Returns a combined dictionary: {'profile': {...}, 'tweets': [...]}
    """
    profile_xhr_calls = []
    tweets_xhr_calls = []
    
    profile_data = None
    initial_tweets = []

    def intercept_response(response):
        nonlocal profile_xhr_calls, tweets_xhr_calls
        try:
            if response.request.resource_type == "xhr" and ("UserByScreenName" in response.url or "UserByRestId" in response.url):
                # print(f"[Scraper Info] Intercepted Profile XHR: {response.url}") # Optional Info
                profile_xhr_calls.append(response)
            elif response.request.resource_type == "xhr" and "/UserTweets" in response.url:
                 # print(f"[Scraper Info] Intercepted Tweets XHR: {response.url}") # Optional Info
                 tweets_xhr_calls.append(response)
        except Exception as e:
            print(f"Error intercepting response: {e}")
        return response

    try:
        with sync_playwright() as pw:
            browser = pw.chromium.launch(headless=headless)
            context = browser.new_context(viewport={"width": 1920, "height": 1080})
            page = context.new_page()
            page.on("response", intercept_response)

            print(f"Navigating to profile: {profile_url}...")
            page.goto(profile_url, wait_until="domcontentloaded", timeout=60000)
            print("Waiting for profile content selector...")
            try:
                 page.wait_for_selector("div[data-testid='primaryColumn']", timeout=30000)
                 print("Initial profile container loaded. Waiting briefly...")
                 page.wait_for_timeout(5000) # Initial wait 

                 # --- RE-ADD SCROLLING --- 
                 scroll_attempts = 10 
                 print(f"[Scraper Debug] Will attempt up to {scroll_attempts} scrolls to find up to {max_tweets} tweets.")
                 consecutive_scrolls_with_no_new_tweets = 0 # Counter for early stopping
                 
                 for i in range(scroll_attempts):
                     # --- Process currently intercepted XHRs --- 
                     current_tweet_count = len(initial_tweets)
                     # --- Process Profile XHRs (Subset - only find profile once) ---
                     if not profile_data and profile_xhr_calls: 
                         # ... (Same profile processing logic as before, only run once) ...
                         for xhr in reversed(profile_xhr_calls): 
                             try: 
                                 data = xhr.json() 
                                 user_payload = jmespath.search("data.user.result", data) 
                                 if user_payload: 
                                     parsed_profile = parse_user(user_payload) 
                                     if parsed_profile and not parsed_profile.get("error"): 
                                         profile_data = parsed_profile 
                                         print("Successfully parsed valid user profile data.") 
                                         break 
                             except Exception as e: print(f"Error processing Profile XHR {xhr.url}: {e}") 
                         profile_xhr_calls = [] # Clear processed profile calls
                         
                     # --- Process Tweets XHRs (Incremental) --- 
                     if tweets_xhr_calls: 
                         print(f"[Scraper Debug] Processing {len(tweets_xhr_calls)} new Tweets XHRs before scroll {i+1}...")
                         new_tweets_in_batch = 0
                         for xhr in reversed(tweets_xhr_calls): 
                             if len(initial_tweets) >= max_tweets: break 
                             try: 
                                 data = xhr.json() 
                                 payloads = find_objects_by_typename(data, "Tweet") 
                                 if payloads: 
                                     for p in payloads: 
                                         if len(initial_tweets) >= max_tweets: break 
                                         parsed = parse_tweet(p) 
                                         if parsed and parsed.get('id') and not any(t['id'] == parsed['id'] for t in initial_tweets): 
                                             initial_tweets.append(parsed) 
                                             new_tweets_in_batch += 1
                             except Exception as e: print(f"Error processing Tweets XHR {xhr.url}: {e}") 
                         if new_tweets_in_batch > 0: 
                             print(f"[Scraper Debug] Added {new_tweets_in_batch} tweets from recent XHRs. Total now: {len(initial_tweets)}.")
                         tweets_xhr_calls = [] # Clear processed tweet calls for this iteration

                     tweets_added_this_iteration = len(initial_tweets) - current_tweet_count
                     
                     # --- Early stopping logic --- 
                     if tweets_added_this_iteration == 0:
                         consecutive_scrolls_with_no_new_tweets += 1
                         print(f"[Scraper Debug] No new tweets found in XHRs after scroll {i}. ({consecutive_scrolls_with_no_new_tweets} consecutive)")
                     else:
                         consecutive_scrolls_with_no_new_tweets = 0 # Reset counter if new tweets found

                     if consecutive_scrolls_with_no_new_tweets >= 2:
                         print("[Scraper Debug] Stopping scroll early: No new tweets found in last 2 attempts.")
                         break
                     # --- End Early stopping --- 
                         
                     # --- Now check max_tweets limit and scroll --- 
                     if len(initial_tweets) >= max_tweets:
                         print(f"[Scraper Debug] Reached max_tweets ({max_tweets}). Stopping scroll loop.") # Changed log slightly
                         break 
                         
                     print(f"[Scraper Debug] Scrolling down attempt {i+1}/{scroll_attempts} (currently have {len(initial_tweets)} tweets)...")
                     page.evaluate("window.scrollTo(0, document.body.scrollHeight)") # Scroll to bottom
                     print(f"[Scraper Debug] Waiting after scroll {i+1}...")
                     page.wait_for_timeout(5000) # Longer wait after scroll
                     
                 if len(initial_tweets) < max_tweets and consecutive_scrolls_with_no_new_tweets < 2:
                    print("[Scraper Debug] Finished scroll attempts loop. Processing any remaining XHRs...")
                 # --- END SCROLLING --- 

                 print(f"Finished scroll/load phase. Processing final XHRs...") # Updated log
            except Exception as e:
                 print(f"Timeout or error waiting for selector/scroll: {e}. Proceeding with captured XHRs.")

            # --- Final XHR Processing (ensure everything caught is processed) --- 
            # Process profile one last time if missed
            if not profile_data and profile_xhr_calls:
                # ... (Same profile processing logic as above) ...
                 for xhr in reversed(profile_xhr_calls): 
                     try: 
                         data = xhr.json() 
                         user_payload = jmespath.search("data.user.result", data) 
                         if user_payload: 
                             parsed_profile = parse_user(user_payload) 
                             if parsed_profile and not parsed_profile.get("error"): 
                                 profile_data = parsed_profile 
                                 print("Successfully parsed valid user profile data (final check).") 
                                 break 
                     except Exception as e: print(f"Error processing Profile XHR {xhr.url} (final check): {e}") 

            # Process tweets one last time
            if tweets_xhr_calls:
                print(f"[Scraper Debug] Final processing of {len(tweets_xhr_calls)} remaining Tweets XHRs...")
                new_tweets_in_batch = 0
                for xhr in reversed(tweets_xhr_calls):
                     if len(initial_tweets) >= max_tweets: break
                     try: 
                         data = xhr.json() 
                         payloads = find_objects_by_typename(data, "Tweet") 
                         if payloads: 
                             for p in payloads: 
                                 if len(initial_tweets) >= max_tweets: break 
                                 parsed = parse_tweet(p) 
                                 if parsed and parsed.get('id') and not any(t['id'] == parsed['id'] for t in initial_tweets): 
                                     initial_tweets.append(parsed) 
                                     new_tweets_in_batch +=1
                     except Exception as e: print(f"Error processing Tweets XHR {xhr.url} (final check): {e}") 
                if new_tweets_in_batch > 0:
                    print(f"[Scraper Debug] Added {new_tweets_in_batch} tweets from final XHR processing. Final total: {len(initial_tweets)}.")
            # --- End Final XHR Processing ---

            browser.close()

    except Exception as e:
        print(f"Playwright error during navigation or setup: {e}")
        try:
            if 'browser' in locals() and hasattr(browser, 'close'): browser.close()
        except Exception as close_err: print(f"Error closing browser: {close_err}")
        return None # Indicate failure

    # --- Combine Results (Sort and Trim) --- 
    if profile_data or initial_tweets:
        # Sort tweets by date (newest first) before trimming
        try:
            initial_tweets.sort(key=lambda t: t.get('created_at', ''), reverse=True)
        except Exception as sort_e:
             print(f"[Scraper Warning] Could not sort tweets by date: {sort_e}")
             
        # Trim to max_tweets
        trimmed_tweets = initial_tweets[:max_tweets]
        
        combined_result = {
            "profile": profile_data or {},
            "tweets": trimmed_tweets 
        }
        print(f"Successfully scraped. Profile found: {bool(profile_data)}, Tweets returned: {len(trimmed_tweets)} (max requested: {max_tweets})")
        return combined_result
    else:
        print(f"Could not find profile or initial tweets.")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Scrape public profile data and recent tweets from an X.com user URL.')
    parser.add_argument('url', type=str, help='The URL of the user profile to scrape.')
    parser.add_argument('--max_tweets', type=int, default=20, help='Maximum number of tweets to attempt to scrape.')
    parser.add_argument('--output', type=str, default='user_profile_output.json', help='File path to save the output JSON.')
    parser.add_argument('--visible', action='store_true', help='Run the browser in visible mode.')
    args = parser.parse_args()

    print(f"Attempting to scrape profile and up to {args.max_tweets} tweets: {args.url}")
    scraped_data = scrape_user_profile_and_tweets(args.url, max_tweets=args.max_tweets, headless=not args.visible)

    if scraped_data:
        print(f"Successfully scraped data. Saving to {args.output}")
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(scraped_data, f, ensure_ascii=False, indent=4)
            print("Output saved.")
        except Exception as e:
            print(f"Error saving output to {args.output}: {e}")
    else:
        print("Profile and initial tweets scraping failed.") 

./twitter_user_scraper\twitter_user_scraper_node.py
import json
from .twitter_user_scraper import scrape_user_profile_and_tweets # Import from the user scraper script
from typing import Optional, Dict

class TwitterUserScraper:
    """ 
    LOKI Node: Scrapes public profile data AND initial tweets for a specific X.com user.
    Input: profile_url (STRING)
    Output: user_data_json (STRING - JSON formatted dict: {'profile':{...}, 'tweets':[...]})
    """
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "profile_url": ("STRING", {"multiline": False, "default": "https://x.com/Scrapfly_dev"}),
                "max_tweets": ("INT", {"default": 20, "min": 1, "max": 200}),
            },
             "optional": {
                 "run_headless": ("BOOLEAN", {"default": True}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("user_data_json",)
    FUNCTION = "scrape_user_and_tweets"
    CATEGORY = "LOKI 🦊/Web"
    OUTPUT_NODE = True

    def scrape_user_and_tweets(self, profile_url: str, max_tweets: int, run_headless: bool = True):
        print(f"[LokiUserScraper] Attempting to scrape profile & up to {max_tweets} tweets: {profile_url}")
        scraped_combined_dict: Optional[Dict] = scrape_user_profile_and_tweets(
            profile_url, 
            max_tweets=max_tweets,
            headless=run_headless
        )

        # --- Basic Debugging --- 
        print(f"[LokiUserScraper] Raw data returned from scrape_user_profile_and_tweets: {type(scraped_combined_dict)}")
        # --- End Debugging ---

        if scraped_combined_dict:
            profile_found = bool(scraped_combined_dict.get("profile"))
            tweets_found = len(scraped_combined_dict.get("tweets", []))
            print(f"[LokiUserScraper] Successfully scraped data (Profile: {profile_found}, Tweets: {tweets_found}). Attempting JSON conversion.")
            try:
                json_output = json.dumps(scraped_combined_dict, indent=4)
                print(f"[LokiUserScraper] JSON conversion successful. Output length: {len(json_output)}")
                return (json_output,)
            except Exception as e:
                print(f"[LokiUserScraper] Error converting scraped data to JSON: {e}")
                return ("{}",) # Return empty JSON object string on error
        else:
            print("[LokiUserScraper] Profile and initial tweets scraping failed.")
            return ("{}",) # Return empty JSON object string on failure 

./twitter_user_scraper\__init__.py
import subprocess
import sys
import os
from .twitter_user_scraper_node import TwitterUserScraper # Import the new node class

# --- Playwright check (Keep as is, maybe make shared later) --- 
# We might not strictly need this for the user scraper if it doesn't use playwright yet,
# but keeping it doesn't hurt for now.
did_playwright_install_check = False
def ensure_playwright_chromium():
    global did_playwright_install_check
    # ... (existing playwright check function placeholder) ...
    pass 

# ensure_playwright_chromium()

# --- Node Registration --- 
NODE_CLASS_MAPPINGS = {
    "TwitterUserScraper": TwitterUserScraper
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "TwitterUserScraper": "🐦👤 Scrape User Profile (LOKI)"
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']

print("\033[34mLOKI Nodes: \033[93mLoaded Twitter User Scraper node logic\033[0m") 

./twitter_user_scraper\requirements.txt
playwright>=1.30.0 # Or a recent version
jmespath>=1.0.0 

./twitter_user_scraper\STATUS.md
# Node Status: twitter_user_scraper

## System Prerequisites (Manual Installation Required!)

**IMPORTANT:** Playwright requires specific system-level browser dependencies to function. These **cannot** be installed automatically by this node or pip.

- **Linux/WSL:** You MUST install these dependencies manually using your system's package manager. The recommended command is:
  ```bash
  sudo playwright install-deps
  ```
- **Windows:** Running `playwright install` in your command prompt *should* typically handle necessary browser components, but ensure your OS is up-to-date.
- **Docker:** If running ComfyUI in Docker, these system dependencies **must be added to your Dockerfile** during the image build process. Simply running the command inside a running container will not persist across container restarts. You'll need to modify the Dockerfile to include the installation steps (e.g., add `RUN sudo playwright install-deps && sudo rm -rf /var/lib/apt/lists/*` or the equivalent `apt-get install` commands before installing Python requirements).

Failure to install these system dependencies will result in errors like "Host system is missing dependencies" when the node tries to launch a browser.

---

## Current State

- **Functionality:** Successfully scrapes public user profile information and the most recent ~150 tweets from a user's profile page using Playwright without login (requires system dependencies to be installed manually).
- **Layers Implemented:** ID, REQUIREMENTS, NAME, FUNCTIONALITY (PYTHON), INITIALIZATION, FUNCTIONALITY (COMFY-NODE).
- **Output:** Saves combined profile and tweet data to a JSON file (script mode) or outputs a dictionary (ComfyUI node).
- **Scrolling:** Implements scrolling to attempt loading more tweets, with early stopping if no new tweets are found after 2 attempts.

## TODOs / Future Work

- **[Medium] Refactor Shared Code:** Move `parse_user`, `parse_tweet`, and `find_objects_by_typename` to a shared utility module (e.g., `nodes/utils/twitter_parsing.py`) to be used by `twitter_scraper`, `twitter_thread_scraper`, and `twitter_user_scraper`.
- **[Low] Enhance Error Handling:** Implement more specific exception handling for different scraping failures (e.g., network errors, page structure changes, Cloudflare blocks).
- **[High] Add Login Support (Optional/Risky):** Investigate adding optional browser state injection or cookie loading to allow scraping while logged in. This would bypass public limits but increases complexity and risk of account suspension. Requires careful consideration of security and ToS.
- **[Medium] Implement TEST Layer:** Add `twitter_user_scraper_test.py` with unit tests for parsing functions and integration tests for the scraping process (using mock data or controlled test accounts if possible).
- **[Low] Implement VISUAL Layer:** Design an icon (`icon.png`), card (`card.png`), and potentially a basic `glamour.json` UI.
- **[Low] Improve Configuration:** Explore moving script arguments (`--output`, `--max_tweets`) to a `CONFIG` layer (`twitter_user_scraper.json`) if shared configuration becomes necessary beyond node inputs.

## Known Issues / Limitations

- **Requires Manual System Dependencies:** Playwright browser dependencies must be installed manually on the host system or within the Docker image (see System Prerequisites section).
- Reliant on current X.com website structure and internal API endpoints, which are subject to change without notice.
- Public scraping limit (~150 tweets) cannot be bypassed without login.
- Potential for Cloudflare blocks or rate limiting, especially with frequent use or from server IPs. Headless mode can be less reliable.

## Priority

**Medium.** The node is functional for its core public scraping task but could benefit from refactoring, testing, and potentially more advanced (but riskier) features like login support. Ensuring users understand the manual dependency installation is crucial. 

./twitter_user_scraper\CONTEXT.md
# Context: twitter_user_scraper

## Internal Context (ComfyUI-LOKI)

- **Relation to `twitter_scraper`:** This node (`twitter_user_scraper`) focuses on scraping a *user's profile page* to retrieve their profile details and their most recent publicly visible tweets (~150). In contrast, the `twitter_scraper` node targets a *specific tweet URL* to scrape that single tweet and potentially its replies/thread.
- **Relation to `twitter_thread_scraper`:** The `twitter_thread_scraper` builds upon `twitter_scraper` to specifically follow conversation IDs and reconstruct entire threads, whereas `twitter_user_scraper` only gets the latest tweets *from the user's profile view*.
- **Shared Logic:** All Twitter scraping nodes currently share similar parsing logic (`parse_user`, `parse_tweet`) implemented within each script. A refactoring TODO exists (see `STATUS.md`) to move this into a shared utility module (`nodes/utils/twitter_parsing.py`).
- **Dependencies:** Relies on Playwright for browser automation and Jmespath for JSON parsing. These dependencies are declared in the node's `requirements.txt`.

## External Context

- **Twitter/X.com Scraping Challenges:** Scraping X.com is notoriously difficult due to:
    - Frequent changes to the website's structure and internal GraphQL API endpoints.
    - Aggressive anti-bot measures (Cloudflare, login walls for extended access).
    - Rate limiting based on IP address or account activity.
- **Public vs. Logged-in:** This node performs *public* scraping, which does not violate Twitter's ToS regarding automation but is limited in the data accessible (e.g., tweet count limit, no access to full timelines, replies, or private accounts). Logged-in scraping provides more data but requires handling authentication and carries the risk of account suspension.
- **Alternative Approaches:** 
    - **Official API:** Twitter's official API has become heavily restricted and expensive for most use cases.
    - **Nitter:** Open-source alternative front-ends like Nitter previously offered easier scraping but are often unreliable or blocked.
    - **Commercial Scraping Services:** Services like ScrapFly (see `nodes/twitter_scraper/context/how_to.txt`), Bright Data, etc., offer robust scraping infrastructure (proxy rotation, CAPTCHA solving, browser farms) but come at a cost.
- **Justification:** Implementing a basic public scraper within LOKI provides a free, integrated way to fetch recent user activity, useful for downstream analysis or generation tasks, despite its inherent limitations. 

./twitter_thread_scraper\twitter_thread_scraper.md
# twitter_thread_scraper Node Requirements

## DESCRIPTION

Attempts to scrape tweet data from an X.com (Twitter) thread, starting from a given tweet URL. Prioritizes following the main author's thread replies.

**Note:** Due to X.com restrictions for non-logged-in users, this node can typically only retrieve the **starting tweet** and potentially 1-2 immediate replies if they load initially. Full thread scraping requires login, which is not implemented due to Terms of Service violations and account suspension risks.

## INPUTS

- `thread_start_url` (STRING): The URL of the first tweet in the thread.
- `max_tweets` (INT): The maximum number of tweets to *attempt* to return (effective limit may be much lower due to login restrictions).

## OUTPUTS

- `thread_data_list` (LIST[DICT]): A list containing parsed data for the tweets successfully retrieved (often only the first tweet).

## NODE_STRATEGY

Uses Playwright (without login) to load the initial tweet page, intercepts background network requests (XHR), and parses available tweet data. It tries to follow the author's sequence based on the limited data available in the initial page load XHRs.

## LIMITATIONS

- **Requires Login for Full Threads:** X.com requires users to be logged in to view full threads and replies beyond the first few. This scraper operates without login to avoid ToS violations and account risks.
- **Limited Reply Depth:** Consequently, this node usually only retrieves the starting tweet. It cannot reliably access the full conversation history visible to logged-in users.
- **No Scrolling for More Replies:** Even simulated scrolling does not reliably trigger the loading of further replies for non-logged-in sessions based on current testing. 

./twitter_thread_scraper\twitter_thread_scraper_name.md
TOKEN="🐦🧵ScrapeThread"
EMOJI_ICON="🧵"
RECIPE="🐦🔗 => 🐦🧵"
X2Y_FORMAT="C2D+_tweet_to_json+"
ALIASES=["scrape_twitter_thread", "get_tweet_thread", "get_conversation"]
TAGS=["twitter", "x.com", "scrape", "web", "social media", "thread", "conversation"]
METAPHORS=["A digital librarian archiving an entire tweet conversation", "A reporter transcribing a sequence of public statements"] 

./twitter_thread_scraper\twitter_thread_scraper.py
import json
import jmespath
from playwright.sync_api import sync_playwright
import argparse
from typing import Dict, List, Optional

# --- Reusing parsing functions from twitter_scraper ---
# (Ideally, these would be in a shared utility file)
def parse_user(data: Dict) -> Dict:
    if data.get("__typename") == "User":
        legacy = data.get("legacy", {})
        result = jmespath.search(
            """{
            created_at: created_at, default_profile: default_profile, description: description,
            entities: entities, fast_followers_count: fast_followers_count, favourites_count: favourites_count,
            followers_count: followers_count, friends_count: friends_count, has_custom_timelines: has_custom_timelines,
            is_translator: is_translator, listed_count: listed_count, location: location,
            media_count: media_count, name: name, normal_followers_count: normal_followers_count,
            pinned_tweet_ids_str: pinned_tweet_ids_str, possibly_sensitive: possibly_sensitive,
            profile_banner_url: profile_banner_url, profile_image_url_https: profile_image_url_https,
            screen_name: screen_name, statuses_count: statuses_count, translator_type: translator_type,
            verified: verified, want_retweets: want_retweets, withheld_in_countries: withheld_in_countries
        }""",
            legacy,
        )
        # Ensure result is a dict even if search fails
        result = result if isinstance(result, dict) else {}
        result["id"] = data.get("id")
        result["rest_id"] = data.get("rest_id")
        result["is_blue_verified"] = data.get("is_blue_verified")
    else:
        print(f"Warning: Unexpected user data structure: {data.get('__typename')}")
        result = {}
    return result

def parse_tweet(data: Dict) -> Dict:
    """Parse Twitter tweet JSON dataset for the most important fields"""
    legacy = data.get("legacy", {})
    core = data.get("core", {})
    card = data.get("card", {})
    views = data.get("views", {})

    result = jmespath.search(
        """{
        created_at: legacy.created_at,
        attached_urls: legacy.entities.urls[].expanded_url,
        attached_urls2: legacy.entities.url.urls[].expanded_url,
        attached_media: legacy.entities.media[].media_url_https,
        tagged_users: legacy.entities.user_mentions[].screen_name,
        tagged_hashtags: legacy.entities.hashtags[].text,
        favorite_count: legacy.favorite_count,
        bookmark_count: legacy.bookmark_count,
        quote_count: legacy.quote_count,
        reply_count: legacy.reply_count,
        retweet_count: legacy.retweet_count,
        text: legacy.full_text,
        is_quote: legacy.is_quote_status,
        is_retweet: legacy.retweeted,
        language: legacy.lang,
        user_id: legacy.user_id_str,
        id: legacy.id_str,
        conversation_id: legacy.conversation_id_str,
        source: source,
        views: views.count
    }""",
        data,
    )

    expected_keys = [
        "created_at", "attached_urls", "attached_urls2", "attached_media",
        "tagged_users", "tagged_hashtags", "favorite_count", "bookmark_count",
        "quote_count", "reply_count", "retweet_count", "text", "is_quote",
        "is_retweet", "language", "user_id", "id", "conversation_id", "source", "views"
    ]
    if result is None: result = {}
    for key in expected_keys:
        if key not in result:
            result[key] = None # Default for missing values

    result["poll"] = {}
    poll_data = jmespath.search("card.legacy.binding_values", data) or []
    for poll_entry in poll_data:
        key, value_dict = poll_entry.get("key"), poll_entry.get("value")
        if not key or not value_dict: continue
        if "choice" in key and "string_value" in value_dict: result["poll"][key] = value_dict["string_value"]
        elif "end_datetime" in key and "string_value" in value_dict: result["poll"]["end"] = value_dict["string_value"]
        elif "last_updated_datetime" in key and "string_value" in value_dict: result["poll"]["updated"] = value_dict["string_value"]
        elif "counts_are_final" in key and "boolean_value" in value_dict: result["poll"]["ended"] = value_dict["boolean_value"]
        elif "duration_minutes" in key and "string_value" in value_dict: result["poll"]["duration"] = value_dict["string_value"]

    user_results = core.get("user_results", {}).get("result", {})
    result["user"] = parse_user(user_results) if user_results else {}

    for key in ["attached_urls", "attached_urls2", "attached_media", "tagged_users", "tagged_hashtags"]:
        if result.get(key) is None: result[key] = [] # Ensure lists are lists

    return result
# --- End of reused parsing functions ---


def scrape_thread_data(url: str, max_tweets: int = 10, headless: bool = True) -> Optional[List[Dict]]:
    """
    Scrape a Twitter thread starting from the given URL.
    Returns a list of parsed tweet dictionaries.
    """
    _xhr_calls = []
    thread_tweets = None
    start_tweet_id = url.split('/')[-1].split('?')[0] # Extract tweet ID from URL
    original_author_id = None

    def intercept_response(response):
        try:
            # --- DEBUG: Log all XHR --- 
            if response.request.resource_type == "xhr":
                print(f"[Scraper Debug] Intercepted XHR: {response.url}") 
            # --- END DEBUG ---

            # Target the endpoint known to contain tweet details / conversation
            if response.request.resource_type == "xhr" and ("TweetResultByRestId" in response.url or "TweetDetail" in response.url):
                # print(f"DEBUG: Adding relevant XHR: {response.url}") # Keep this commented unless needed
                _xhr_calls.append(response)
        except Exception as e:
            print(f"Error intercepting response: {e}")
        return response

    try:
        with sync_playwright() as pw:
            browser = pw.chromium.launch(headless=headless)
            context = browser.new_context(viewport={"width": 1920, "height": 1080})
            page = context.new_page()
            page.on("response", intercept_response)

            print(f"Navigating to thread start: {url}...")
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            print("Waiting for tweet selector...")
            try:
                 page.wait_for_selector("div[data-testid='primaryColumn']", timeout=30000)
                 print("Initial tweet container loaded. Waiting briefly...")
                 page.wait_for_timeout(5000) # Initial wait 

                 # --- ADD SCROLLING (Multiple) --- 
                 scroll_attempts = 3
                 for i in range(scroll_attempts):
                     print(f"[Scraper Debug] Scrolling down attempt {i+1}/{scroll_attempts}...")
                     page.evaluate("window.scrollBy(0, window.innerHeight * 1.5)") # Scroll down 1.5 viewports
                     print(f"[Scraper Debug] Waiting after scroll {i+1}...")
                     page.wait_for_timeout(3000) # Wait a bit after each scroll
                 print("[Scraper Debug] Finished scrolling attempts. Waiting a final time...")
                 page.wait_for_timeout(5000) # Longer final wait
                 # --- END SCROLLING --- 

                 print("Processing XHR calls after scrolling.")
            except Exception as e:
                 print(f"Timeout or error waiting for selector/scroll: {e}. Proceeding.")

            # Process XHR calls to find the thread data
            if not _xhr_calls:
                print("No relevant XHR calls (TweetResultByRestId/TweetDetail) intercepted.")
            else:
                print(f"[Scraper Debug] Processing {_xhr_calls} relevant XHR calls.")
                all_potential_tweet_payloads = [] # Store raw payloads from all relevant XHRs
                original_author_id = None # Reset before loop
                start_tweet_payload = None # Reset before loop

                # --- Pass 1: Find Start Tweet & Author ID, Collect all potential payloads --- 
                for i, xhr in enumerate(reversed(_xhr_calls)):
                    print(f"[Scraper Debug] --- Analyzing XHR Call #{len(_xhr_calls)-i} (URL: {xhr.url}) --- First Pass --- ")
                    try:
                        data = xhr.json()
                        # Collect *all* tweet-like objects from this XHR's known structures
                        current_xhr_payloads = jmespath.search("data.threaded_conversation_with_injections_v2.instructions[?type=='TimelineAddEntries'].entries[].content.itemContent.tweet_results.result || data.tweetResult.result || []", data)
                        # Flatten potential nested lists/results
                        if isinstance(current_xhr_payloads, list):
                            all_potential_tweet_payloads.extend([p for p in current_xhr_payloads if p and p.get("__typename") == "Tweet"])
                        elif current_xhr_payloads and current_xhr_payloads.get("__typename") == "Tweet":
                             all_potential_tweet_payloads.append(current_xhr_payloads)
                        
                        # Find the starting tweet specifically within this XHR to get author ID (only need once)
                        if not original_author_id:
                            query = f"data.threaded_conversation_with_injections_v2.instructions[?type=='TimelineAddEntries'].entries[?entryId=='tweet-{start_tweet_id}'].content.itemContent.tweet_results.result || data.tweetResult.result"
                            found_payload = jmespath.search(query, data)
                            if isinstance(found_payload, list):
                                found_payload = found_payload[0] if found_payload else None
                            
                            if found_payload and found_payload.get("__typename") == "Tweet":
                                start_tweet_payload = found_payload # Keep track of the specific start payload
                                author_info = start_tweet_payload.get('core', {}).get('user_results', {}).get('result', {})
                                original_author_id = author_info.get('rest_id')
                                print(f"[Scraper Debug] Found starting tweet in {xhr.url}. Extracted original_author_id: {original_author_id}")

                    except json.JSONDecodeError:
                        print(f"[Scraper Debug] Could not decode JSON from XHR: {xhr.url}")
                    except Exception as e:
                        print(f"[Scraper Debug] Error processing XHR call (Pass 1) from {xhr.url}: {e}")
                # --- End Pass 1 --- 

                # --- Pass 2: Parse all collected payloads and reconstruct thread --- 
                if original_author_id and all_potential_tweet_payloads:
                     print(f"[Scraper Debug] --- Reconstructing Thread --- Found {len(all_potential_tweet_payloads)} potential tweet payloads across all XHRs. Author ID: {original_author_id}")
                     parsed_tweets = {} # {tweet_id: parsed_data}
                     for tweet_payload in all_potential_tweet_payloads:
                          parsed = parse_tweet(tweet_payload)
                          if parsed and parsed.get('id') and parsed['id'] not in parsed_tweets: # Avoid duplicates
                               parsed_tweets[parsed['id']] = parsed
                     
                     # Reconstruct logic (simplified - assuming parsed_tweets now holds all candidates)
                     thread_result = []
                     current_tweet_id = start_tweet_id
                     processed_ids = set()

                     while current_tweet_id in parsed_tweets and len(thread_result) < max_tweets and current_tweet_id not in processed_ids:
                          current_tweet_data = parsed_tweets[current_tweet_id]
                          thread_result.append(current_tweet_data)
                          processed_ids.add(current_tweet_id)

                          # Heuristic: Find the *first* subsequent tweet in our parsed list by the same author
                          # This relies on the assumption that timeline entries often preserve some order
                          found_current = False
                          next_tweet_id = None
                          for tweet_id, tweet_data in parsed_tweets.items():
                               if tweet_id == current_tweet_id:
                                    found_current = True
                                    continue
                               if found_current and tweet_data.get('user',{}).get('rest_id') == original_author_id and tweet_id not in processed_ids:
                                    next_tweet_id = tweet_id
                                    break # Take the first one found after the current one
                          
                          if next_tweet_id:
                               current_tweet_id = next_tweet_id
                          else:
                               break # No further tweet by author found

                     thread_tweets = thread_result # Assign the final reconstructed list
                     if thread_tweets:
                         print(f"[Scraper Debug] Successfully reconstructed thread with {len(thread_tweets)} tweets.")
                     else:
                          print("[Scraper Debug] Failed to reconstruct thread from parsed tweets.")

                elif not original_author_id:
                     print("[Scraper Debug] Could not find original author ID in any relevant XHR call.")
                elif not all_potential_tweet_payloads:
                     print("[Scraper Debug] No tweet payloads were collected from relevant XHR calls.")
                # --- End Pass 2 --- 

            browser.close()

    except Exception as e:
        print(f"Playwright error during navigation or setup: {e}")
        try:
            if 'browser' in locals() and hasattr(browser, 'close'): browser.close()
        except Exception as close_err: print(f"Error closing browser: {close_err}")
        return None

    if thread_tweets:
        print(f"Returning thread with {len(thread_tweets)} tweets (max requested: {max_tweets}).")
        return thread_tweets
    else:
        print("Could not find valid thread data in any intercepted XHR call.")
        return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Scrape a Twitter thread starting from a specific tweet URL.')
    parser.add_argument('url', type=str, help='The URL of the first tweet in the thread.')
    parser.add_argument('--max', type=int, default=10, help='Maximum number of tweets to scrape from the thread.')
    parser.add_argument('--output', type=str, default='thread_output.json', help='File path to save the output JSON list.')
    parser.add_argument('--visible', action='store_true', help='Run the browser in visible mode.')
    args = parser.parse_args()

    print(f"Attempting to scrape thread from: {args.url} (max: {args.max})")
    scraped_thread = scrape_thread_data(args.url, max_tweets=args.max, headless=not args.visible)

    if scraped_thread:
        print(f"Successfully scraped thread. Saving {len(scraped_thread)} tweets to {args.output}")
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(scraped_thread, f, ensure_ascii=False, indent=4)
            print("Output saved.")
        except Exception as e:
            print(f"Error saving output to {args.output}: {e}")
    else:
        print("Thread scraping failed.") 

./twitter_thread_scraper\twitter_thread_scraper_node.py
import json
from .twitter_thread_scraper import scrape_thread_data # Import from the thread scraper script
from typing import List, Dict

class TwitterThreadScraper:
    """ 
    LOKI Node: Scrapes tweet data from an entire X.com (Twitter) thread using Playwright.
    Input: thread_start_url (STRING), max_tweets (INT)
    Output: thread_data_json (STRING - JSON formatted list)
    """
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "thread_start_url": ("STRING", {"multiline": False, "default": ""}),
                "max_tweets": ("INT", {"default": 10, "min": 1, "max": 100}), # Add reasonable limits
            },
            "optional": {
                 "run_headless": ("BOOLEAN", {"default": True}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("thread_data_json",)
    FUNCTION = "scrape_thread"
    CATEGORY = "LOKI 🦊/Web"
    OUTPUT_NODE = True

    def scrape_thread(self, thread_start_url: str, max_tweets: int, run_headless: bool = True):
        print(f"[LokiThreadScraper] Attempting to scrape thread: {thread_start_url} (max: {max_tweets})")
        scraped_thread_list: Optional[List[Dict]] = scrape_thread_data(
            thread_start_url, 
            max_tweets=max_tweets, 
            headless=run_headless
        )

        # --- Basic Debugging --- 
        print(f"[LokiThreadScraper] Raw data returned from scrape_thread_data: {type(scraped_thread_list)}")
        if isinstance(scraped_thread_list, list):
            print(f"[LokiThreadScraper] Number of tweets returned: {len(scraped_thread_list)}")
        # --- End Debugging ---

        if scraped_thread_list:
            print(f"[LokiThreadScraper] Successfully scraped thread data. Attempting JSON conversion.")
            try:
                # Output the list of tweet dicts as a JSON string
                json_output = json.dumps(scraped_thread_list, indent=4)
                print(f"[LokiThreadScraper] JSON conversion successful. Output length: {len(json_output)}")
                return (json_output,)
            except Exception as e:
                print(f"[LokiThreadScraper] Error converting scraped thread data to JSON: {e}")
                return ("[]",) # Return empty JSON list string on error
        else:
            print("[LokiThreadScraper] Thread scraping failed (scrape_thread_data returned None or empty list).")
            return ("[]",) # Return empty JSON list string on failure 

./twitter_thread_scraper\__init__.py
import subprocess
import sys
import os
from .twitter_thread_scraper_node import TwitterThreadScraper # Import the new node class

# --- Playwright check (Keep as is, maybe make shared later) --- 
did_playwright_install_check = False
def ensure_playwright_chromium():
    global did_playwright_install_check
    # ... (keep existing playwright check function) ...
    pass # Placeholder, keep the existing function body

# Run the check when this sub-module is loaded
# ensure_playwright_chromium() # Maybe only run if needed by this node?

# --- Node Registration --- 
NODE_CLASS_MAPPINGS = {
    "TwitterThreadScraper": TwitterThreadScraper
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "TwitterThreadScraper": "🐦🧵 Scrape Thread (LOKI)"
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']

print("\033[34mLOKI Nodes: \033[93mLoaded Twitter Thread Scraper node logic\033[0m") 

./twitter_thread_scraper\requirements.txt
playwright>=1.30.0 # Or a recent version
jmespath>=1.0.0 

./twitter_thread_scraper\thread_output.json
[
    {
        "created_at": "Sun May 04 20:50:10 +0000 2025",
        "attached_urls": [],
        "attached_urls2": [],
        "attached_media": [
            "https://pbs.twimg.com/tweet_video_thumb/GqIgy_oW0AAKs9g.jpg"
        ],
        "tagged_users": [],
        "tagged_hashtags": [],
        "favorite_count": 22,
        "bookmark_count": 11,
        "quote_count": 2,
        "reply_count": 5,
        "retweet_count": 5,
        "text": "(1/2) The Fed’s Strategic Pause | Why Markets May Have Misread the Post-Tariff Terrain\n\nOver the last 72 hours, markets were dealt a sharp reality check. Following weeks of dovish positioning and escalating expectations of a near-term Fed rate cut, the release of April’s ISM and https://t.co/bxoW48hiDs",
        "is_quote": true,
        "is_retweet": false,
        "language": "en",
        "user_id": "1448432122881101826",
        "id": "1919132465605607788",
        "conversation_id": "1919132465605607788",
        "source": "<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>",
        "views": "6046",
        "poll": {},
        "user": {
            "created_at": "Wed Oct 13 23:35:47 +0000 2021",
            "default_profile": true,
            "description": "Macro strategy | Systemic risk & policy intel | Powered by AI + human insight | Featured by RealClearPolitics & https://t.co/d5MEzDJBQE",
            "entities": {
                "description": {
                    "urls": [
                        {
                            "display_url": "InvestX.fr",
                            "expanded_url": "http://InvestX.fr",
                            "url": "https://t.co/d5MEzDJBQE",
                            "indices": [
                                112,
                                135
                            ]
                        }
                    ]
                }
            },
            "fast_followers_count": 0,
            "favourites_count": 13713,
            "followers_count": 26882,
            "friends_count": 689,
            "has_custom_timelines": true,
            "is_translator": false,
            "listed_count": 585,
            "location": "",
            "media_count": 824,
            "name": "EndGame Macro",
            "normal_followers_count": 26882,
            "pinned_tweet_ids_str": [
                "1912562154848346353"
            ],
            "possibly_sensitive": false,
            "profile_banner_url": null,
            "profile_image_url_https": "https://pbs.twimg.com/profile_images/1554632419592003584/lRSfOnxJ_normal.jpg",
            "screen_name": "onechancefreedm",
            "statuses_count": 3871,
            "translator_type": "none",
            "verified": false,
            "want_retweets": null,
            "withheld_in_countries": [],
            "id": "VXNlcjoxNDQ4NDMyMTIyODgxMTAxODI2",
            "rest_id": "1448432122881101826",
            "is_blue_verified": true
        }
    }
]

